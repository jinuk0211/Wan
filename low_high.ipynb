{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.14.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.20.0)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.5)\n",
      "Collecting tqdm (from gdown)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.10.5)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, PySocks, gdown\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [gdown]\n",
      "\u001b[1A\u001b[2KSuccessfully installed PySocks-1.7.1 gdown-5.2.0 tqdm-4.67.1\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1rxJbCRKen1-8bOMH0P1ke850aXSgIdSD\n",
      "From (redirected): https://drive.google.com/uc?id=1rxJbCRKen1-8bOMH0P1ke850aXSgIdSD&confirm=t&uuid=ffd28e87-778b-4485-829c-8306100a375f\n",
      "To: /workspace/flash_attn-2.8.3+cu12torch2.8cxx11abiTRUE-cp312-cp312-linux_x86_64.whl\n",
      "100%|█████████████████████████████████████████| 256M/256M [00:01<00:00, 170MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1bLngg6cZPCV5weaf0kpF9P2XAxkv-EVE\n",
      "From (redirected): https://drive.google.com/uc?id=1bLngg6cZPCV5weaf0kpF9P2XAxkv-EVE&confirm=t&uuid=421cede1-980e-40e2-8f12-8b56c6f4d054\n",
      "To: /workspace/goldenboylora-22-LOW-V2-e106.safetensors\n",
      "100%|████████████████████████████████████████| 307M/307M [00:07<00:00, 41.4MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=16is64ePTcBvg_ZFSG9wK-1t9Hq0xVoZC\n",
      "From (redirected): https://drive.google.com/uc?id=16is64ePTcBvg_ZFSG9wK-1t9Hq0xVoZC&confirm=t&uuid=df215415-94b1-4f1d-9ac6-0fb848c2cc67\n",
      "To: /workspace/goldenboylora-22-HIGH-e52.safetensors\n",
      "100%|████████████████████████████████████████| 307M/307M [00:11<00:00, 25.8MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=19lqCjnnGpEzakfK89IK7xFRodrVELh55\n",
      "From (redirected): https://drive.google.com/uc?id=19lqCjnnGpEzakfK89IK7xFRodrVELh55&confirm=t&uuid=0e4b11ce-74e3-40b4-8589-9252a1b6db86\n",
      "To: /workspace/yoshiaki-kawajiri-low-v3_e92.safetensors\n",
      "100%|████████████████████████████████████████| 307M/307M [00:06<00:00, 49.5MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1oEywspshRQxLdZpOLWmviMOOSbZZ0BR8\n",
      "From (redirected): https://drive.google.com/uc?id=1oEywspshRQxLdZpOLWmviMOOSbZZ0BR8&confirm=t&uuid=a975d7b3-7648-4cd4-a311-df5f27d4c92d\n",
      "To: /workspace/CassHamadaWan2.2LowNoise.safetensors\n",
      "100%|████████████████████████████████████████| 307M/307M [00:07<00:00, 43.5MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1nkBhg4sMR1uVdHiR5VnpsGT3s6d4KwlD\n",
      "From (redirected): https://drive.google.com/uc?id=1nkBhg4sMR1uVdHiR5VnpsGT3s6d4KwlD&confirm=t&uuid=29dcd48a-522e-4370-a7c3-0048776dc508\n",
      "To: /workspace/CassHamadaWan2.2HighNoise.safetensors\n",
      "100%|████████████████████████████████████████| 307M/307M [00:12<00:00, 24.9MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1x2TB8puNPW3Y_kmSjtPB8cpZnE5VhMIL\n",
      "From (redirected): https://drive.google.com/uc?id=1x2TB8puNPW3Y_kmSjtPB8cpZnE5VhMIL&confirm=t&uuid=e30e26dd-4906-4136-bd46-74c4c3d8e105\n",
      "To: /workspace/[WAN2.2]Offseasonsanta_Redmond_low_noise.safetensors\n",
      "100%|████████████████████████████████████████| 307M/307M [00:07<00:00, 38.5MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1JnLKvVqHA2-83xjPyCQaIS_Tyw8XjpHK\n",
      "From (redirected): https://drive.google.com/uc?id=1JnLKvVqHA2-83xjPyCQaIS_Tyw8XjpHK&confirm=t&uuid=ab10d3e6-1909-4f33-9832-394053c6ed7d\n",
      "To: /workspace/[WAN2.2]Offseasonsanta_Redmond_high_noise.safetensors\n",
      "100%|████████████████████████████████████████| 307M/307M [00:08<00:00, 36.9MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1GiJO12IBs4-Xzin3ihA74S5o2u9_lJmU\n",
      "From (redirected): https://drive.google.com/uc?id=1GiJO12IBs4-Xzin3ihA74S5o2u9_lJmU&confirm=t&uuid=bbb612e4-68ea-45e1-a00f-9a6015ac9d99\n",
      "To: /workspace/wan22-x1nyu3-20epoc-low-k3nk.safetensors\n",
      "100%|████████████████████████████████████████| 307M/307M [00:09<00:00, 32.6MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1GJrD_UsutFIhRO76q3cHBvGk7mdlSsxM\n",
      "From (redirected): https://drive.google.com/uc?id=1GJrD_UsutFIhRO76q3cHBvGk7mdlSsxM&confirm=t&uuid=d76dccb2-c218-434e-a26d-6a37970c611f\n",
      "To: /workspace/985347-wan22_14B-low-Nfj1nx-e65.safetensors\n",
      "100%|████████████████████████████████████████| 307M/307M [00:09<00:00, 32.9MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1AZ5VoEomwBnBMSKp73WcxW8ljPnjQRIE\n",
      "From (redirected): https://drive.google.com/uc?id=1AZ5VoEomwBnBMSKp73WcxW8ljPnjQRIE&confirm=t&uuid=c1c0b7cf-10a4-4125-9228-d914724590c2\n",
      "To: /workspace/985347-wan22_14B-high-Nfj1nx-e71.safetensors\n",
      "100%|████████████████████████████████████████| 307M/307M [00:03<00:00, 81.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "# !gdown https://drive.google.com/uc?id=12ftWJEKe3lgcFh4bjsB2RMOCKPHZtS3L #wan22_14b_i2v_orbit_low_noise.safetensors #i2v\n",
    "# !gdown https://drive.google.com/uc?id=1px9OPyDSW7qBDK1BOemMn1Ku4Nmb8dum #wan22_14b_i2v_orbit_high_noise.safetensors #i2v\n",
    "# !gdown https://drive.google.com/uc?id=1ptru9D-qZrFWu8G6MAEyasTyNN_HQD0J #wan2.2_wugong_i2v_low.safetensors #\n",
    "# !gdown https://drive.google.com/uc?id=1eGLEWlKaSmpav_3BbjfQ8yn4bP14HJuR #wan2.2_wugong_i2v_high.safetensors\n",
    "\n",
    "!pip install gdown\n",
    "\n",
    "!gdown https://drive.google.com/uc?id=1rxJbCRKen1-8bOMH0P1ke850aXSgIdSD #flash attn\n",
    "# !gdown https://drive.google.com/uc?id=1NM51m5S0PkQSvyfAtg8sh-zj3f1Noi8D #FastWan_T2V_14B_480p_lora_rank_128_bf16\n",
    "\n",
    "# style\n",
    "\n",
    "# goldenboylora-22-LOW-V2-e106, TRIGGER = 'GoldenBoyStyle'\n",
    "!gdown https://drive.google.com/uc?id=1bLngg6cZPCV5weaf0kpF9P2XAxkv-EVE\n",
    "!gdown https://drive.google.com/uc?id=16is64ePTcBvg_ZFSG9wK-1t9Hq0xVoZC\n",
    "\n",
    "# maripol-wan2_2_HIGH-v1-000080, TRIGGER = 'a polaroid photo of' 'in the style of maripol'\n",
    "# !gdown https://drive.google.com/uc?id=1koCEYeMtHPT3aZv8gkja3YDiXI7OiLoV\n",
    "# !gdown https://drive.google.com/uc?id=1NcihKMqVMnZvXxh5jfurG1KtIZxAzxFE\n",
    "\n",
    "# # yoshiaki-kawajiri-low-v3_e92.safetensors v3 하나임\n",
    "!gdown https://drive.google.com/uc?id=19lqCjnnGpEzakfK89IK7xFRodrVELh55\n",
    "\n",
    "# ghibli Studio Ghibli Dark Fairytale 예시 Studio Ghibli Dark Fairytale, A digital paint ~~ 이미지 lora\n",
    "# !gdown https://drive.google.com/uc?id=1Qom0I6HAktel3hQ_TdVl9Ii4rArXg1bO\n",
    " \n",
    "# pure color anime style trigger = YL简洁无线条的动漫风格 - 한개밖에 없음\n",
    "# !gdown https://drive.google.com/uc?id=1fvDP9zoF-JU7s3ojSNyBXNDuR8wiaV_q\n",
    " \n",
    "#unipixel trigger = nv-p1xel4rt\n",
    "# !gdown https://drive.google.com/uc?id=1tiE7bZuVb3eTGCWw7K_fgqY6Psi0DVx2\n",
    "# !gdown https://drive.google.com/uc?id=1H2Ae-bn9ugDi8rB8nLIYFHfQS-UPvK32\n",
    "\n",
    "# # content\n",
    "# emma watson trigger word = wanemma\n",
    "# !gdown https://drive.google.com/uc?id=1G80Pvx_FFwcynPeDDGZgD9UHvaLVtw96\n",
    "\n",
    "# #Latina Women T2V - Wan 2.2 Video Lora, trigget word = 'gu4d4lup3'\n",
    "# !gdown https://drive.google.com/uc?id=1sfSD6ErTayf7uInNENEcfMtmfYZx4o35\n",
    "\n",
    "# latin\n",
    "# !gdown https://drive.google.com/uc?id=1oWV_lCncRYOTA15ODAD1QEBE5qHb_ZLU\n",
    "\n",
    "# # wan22-J1n4ri-84epoc-low-k3nk trigget word = 'j1nar1'\n",
    "# !gdown https://drive.google.com/uc?id=1zgsIx5LcU_HCSGBWkTeTwf7fFhXKTSsR\n",
    "\n",
    "# # Maid outfit, short hair, white socks, cute, TRIGGER = 'NSFDFNP'\n",
    "# !gdown https://drive.google.com/uc?id=1WXh6v-7fMOhvTi3IMrr-rpu6uvSRsUBu\n",
    "\n",
    "# # CassHamadaWan2.2LowNoise, TRIGGER = 'Cass Hamada' '3D Cartoon Style'\n",
    "!gdown https://drive.google.com/uc?id=1oEywspshRQxLdZpOLWmviMOOSbZZ0BR8\n",
    "!gdown https://drive.google.com/uc?id=1nkBhg4sMR1uVdHiR5VnpsGT3s6d4KwlD\n",
    "\n",
    "# 산타  trigger phrase is Off-Season Santa.For best results, start your prompt with “Off-Season Santa\n",
    "!gdown https://drive.google.com/uc?id=1x2TB8puNPW3Y_kmSjtPB8cpZnE5VhMIL\n",
    "!gdown https://drive.google.com/uc?id=1JnLKvVqHA2-83xjPyCQaIS_Tyw8XjpHK\n",
    "\n",
    "#Asian Women T2V trigger = x1nyu3,\n",
    "!gdown https://drive.google.com/uc?id=1GiJO12IBs4-Xzin3ihA74S5o2u9_lJmU\n",
    "\n",
    "#trigger 징크스 Trigger words: Nfj1nx, blue hair\n",
    "!gdown https://drive.google.com/uc?id=1GJrD_UsutFIhRO76q3cHBvGk7mdlSsxM\n",
    "!gdown https://drive.google.com/uc?id=1AZ5VoEomwBnBMSKp73WcxW8ljPnjQRIE\n",
    "\n",
    "# # motion\n",
    "\n",
    "# # #wan_2.2_t2v_highnoise_broken_v1.0 trigger word = 'She breaks down in tears' 'She is terrified' 'she cries out in pain' 'strengt 0.5'\n",
    "# !gdown https://drive.google.com/uc?id=1wQ_rRijvb4jgkRe_VnULnHhJLXk11FaL\n",
    "\n",
    "# # camera\n",
    "\n",
    "# # trigger = FollowCam.\n",
    "# !gdown https://drive.google.com/uc?id=1mBKBwKbR95J0ljiDKYBJirgSZ3_TTS6i\n",
    "\n",
    "# # trigger = VERTIGO, Dolly zoom\n",
    "# !gdown https://drive.google.com/uc?id=1TFwwLDapBfjZMeiJv_Y1x8GIZexROmkc\n",
    "\n",
    "# # trigger = AxialView.\n",
    "# !gdown https://drive.google.com/uc?id=1tEkEhQ|np6vMgRew2EUoSub6vH_ZHYDjG\n",
    "# !gdown https://drive.google.com/uc?id=1s29Xzb4zkgK3Y16Cf0BWwANTCx-OzEEv\n",
    "\n",
    "# # trigger = Window Seat\n",
    "# !gdown https://drive.google.com/uc?id=1aLEWAWpEFNLJfQ7i1Dnaor2QrvW4COZs\n",
    "# !gdown https://drive.google.com/uc?id=1E0f5JUlSVQRZqONjw01nhokzGv6wL6Gj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleared\n"
     ]
    }
   ],
   "source": [
    "import torch, gc\n",
    "\n",
    "def clear_gpu():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "clear_gpu()\n",
    "print('cleared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nm5vOwz1R4Cq",
    "outputId": "6f849e61-7f8f-4765-ce41-2e22df3abf68",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install diffusers easydict ftfy transformers accelerate einops -q\n",
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "!pip install '/workspace/flash_attn-2.8.3+cu12torch2.8cxx11abiTRUE-cp312-cp312-linux_x86_64.whl' -q\n",
    "!pip install hf_transfer -q\n",
    "!export HF_HUB_ENABLE_HF_TRANSFER=1\n",
    "!export HF_TOKEN='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'\n",
    "!pip install huggingface_hub -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tvenw0TIHeqb",
    "outputId": "c56222ac-dba3-4f68-e816-473d7d9d8e95",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[cli] in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (1.2.0)\n",
      "Collecting InquirerPy==0.3.4 (from huggingface_hub[cli])\n",
      "  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface_hub[cli])\n",
      "  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.52)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.14)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub[cli]) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub[cli]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub[cli]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub[cli]) (2025.10.5)\n",
      "Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
      "Downloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
      "Installing collected packages: pfzy, InquirerPy\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [InquirerPy]\n",
      "\u001b[1A\u001b[2KSuccessfully installed InquirerPy-0.3.4 pfzy-0.3.4\n",
      "\u001b[33m⚠️  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
      "Downloading 'low_noise_model/config.json' to 'Wan2.2-T2V-A14B/.cache/huggingface/download/low_noise_model/8_PA_wEVGiVa2goH2H4KQOQpvVY=.33aa2246fbda19e9320740fae567e0833a188a48.incomplete'\n",
      "config.json: 100%|█████████████████████████████| 250/250 [00:00<00:00, 1.50MB/s]\n",
      "Download complete. Moving file to Wan2.2-T2V-A14B/low_noise_model/config.json\n",
      "Downloading 'low_noise_model/diffusion_pytorch_model-00001-of-00006.safetensors' to 'Wan2.2-T2V-A14B/.cache/huggingface/download/low_noise_model/Y9iCDe5rRRU4fPBiTVYUc_mA-R0=.f2a6df8981742260b3f2cd8b51a276eb0ffa0332c9bbbadc3d625f269096f01c.incomplete'\n",
      "low_noise_model/diffusion_pytorch_model-(…): 100%|█| 9.99G/9.99G [00:12<00:00, 7\n",
      "Download complete. Moving file to Wan2.2-T2V-A14B/low_noise_model/diffusion_pytorch_model-00001-of-00006.safetensors\n",
      "Downloading 'low_noise_model/diffusion_pytorch_model-00002-of-00006.safetensors' to 'Wan2.2-T2V-A14B/.cache/huggingface/download/low_noise_model/Kn8G95r2Em00Nl63-0zVOKjNfPc=.ea3998b0b394ede1df6c8cb9ea44f1bed0d3d7018bd8113baca9ff74e100b5d9.incomplete'\n",
      "low_noise_model/diffusion_pytorch_model-(…): 100%|█| 9.94G/9.94G [00:12<00:00, 8\n",
      "Download complete. Moving file to Wan2.2-T2V-A14B/low_noise_model/diffusion_pytorch_model-00002-of-00006.safetensors\n",
      "Downloading 'low_noise_model/diffusion_pytorch_model-00003-of-00006.safetensors' to 'Wan2.2-T2V-A14B/.cache/huggingface/download/low_noise_model/lFGeO7bGlU3m3RgYSJm1QFmmR7E=.e1222162ecdc5a8bb4815f78ae42951cdc3e50a3fe41d8f592462d28f8638681.incomplete'\n",
      "low_noise_model/diffusion_pytorch_model-(…): 100%|█| 9.94G/9.94G [00:12<00:00, 8\n",
      "Download complete. Moving file to Wan2.2-T2V-A14B/low_noise_model/diffusion_pytorch_model-00003-of-00006.safetensors\n",
      "Downloading 'low_noise_model/diffusion_pytorch_model-00004-of-00006.safetensors' to 'Wan2.2-T2V-A14B/.cache/huggingface/download/low_noise_model/x-VRNyWTgP4y3SFm1Da2Wja3TUE=.9fdd1165fe0c5efd87036b3722e69f1d2004bd27396905a997afb7ddf7f22edf.incomplete'\n",
      "low_noise_model/diffusion_pytorch_model-(…): 100%|█| 9.84G/9.84G [00:13<00:00, 7\n",
      "Download complete. Moving file to Wan2.2-T2V-A14B/low_noise_model/diffusion_pytorch_model-00004-of-00006.safetensors\n",
      "Downloading 'low_noise_model/diffusion_pytorch_model-00005-of-00006.safetensors' to 'Wan2.2-T2V-A14B/.cache/huggingface/download/low_noise_model/upxoVy5Njri4Gs1hY6atjpwUIRQ=.04909e9fa1f15de79851230c7f223a0c96fcfd629bdd914b21c9344cec63c78a.incomplete'\n",
      "low_noise_model/diffusion_pytorch_model-(…): 100%|█| 9.84G/9.84G [00:12<00:00, 8\n",
      "Download complete. Moving file to Wan2.2-T2V-A14B/low_noise_model/diffusion_pytorch_model-00005-of-00006.safetensors\n",
      "Downloading 'low_noise_model/diffusion_pytorch_model-00006-of-00006.safetensors' to 'Wan2.2-T2V-A14B/.cache/huggingface/download/low_noise_model/7owkydFacB_uwG4OcO9V4yD-SUk=.7a481d5adcc53ed3ae8b3222b4c4ee63bac6703dae9ff6c62a7d52e546c88809.incomplete'\n",
      "low_noise_model/diffusion_pytorch_model-(…): 100%|█| 7.60G/7.60G [00:09<00:00, 8\n",
      "Download complete. Moving file to Wan2.2-T2V-A14B/low_noise_model/diffusion_pytorch_model-00006-of-00006.safetensors\n",
      "Downloading 'low_noise_model/diffusion_pytorch_model.safetensors.index.json' to 'Wan2.2-T2V-A14B/.cache/huggingface/download/low_noise_model/VpHHFEQcyqWBKjkjOBMoMr6uTZU=.41fcf67b4865048a4859004640117accd8e5ca09.incomplete'\n",
      "(…)ion_pytorch_model.safetensors.index.json: 96.8kB [00:00, 148MB/s]\n",
      "Download complete. Moving file to Wan2.2-T2V-A14B/low_noise_model/diffusion_pytorch_model.safetensors.index.json\n",
      "/workspace/Wan2.2-T2V-A14B\n",
      "\u001b[33m⚠️  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
      "Downloading 'Wan2.1_VAE.pth' to 'Wan2.2-T2V-A14B/.cache/huggingface/download/OZTShqo6Di7rh0SEswnfwSjKa9w=.38071ab59bd94681c686fa51d75a1968f64e470262043be31f7a094e442fd981.incomplete'\n",
      "Wan2.1_VAE.pth: 100%|█████████████████████████| 508M/508M [00:01<00:00, 266MB/s]\n",
      "Download complete. Moving file to Wan2.2-T2V-A14B/Wan2.1_VAE.pth\n",
      "/workspace/Wan2.2-T2V-A14B\n",
      "\u001b[33m⚠️  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
      "Downloading 'configuration.json' to 'Wan2.2-T2V-A14B/.cache/huggingface/download/pOL2LEBeNWU9Y3bTUNO_khibHs8=.dc867bf9a56dc745fb98b8cdfee9e69a3ec53926.incomplete'\n",
      "configuration.json: 100%|█████████████████████| 47.0/47.0 [00:00<00:00, 297kB/s]\n",
      "Download complete. Moving file to Wan2.2-T2V-A14B/configuration.json\n",
      "/workspace/Wan2.2-T2V-A14B\n",
      "\u001b[33m⚠️  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
      "Downloading 'high_noise_model/config.json' to 'Wan2.2-T2V-A14B/.cache/huggingface/download/high_noise_model/8_PA_wEVGiVa2goH2H4KQOQpvVY=.33aa2246fbda19e9320740fae567e0833a188a48.incomplete'\n",
      "config.json: 100%|█████████████████████████████| 250/250 [00:00<00:00, 1.53MB/s]\n",
      "Download complete. Moving file to Wan2.2-T2V-A14B/high_noise_model/config.json\n",
      "Downloading 'high_noise_model/diffusion_pytorch_model-00001-of-00006.safetensors' to 'Wan2.2-T2V-A14B/.cache/huggingface/download/high_noise_model/Y9iCDe5rRRU4fPBiTVYUc_mA-R0=.cb58f36df7e604c6e7b0df80a3f6f9b72c77695e581e19985fc3a0e6732ba1e2.incomplete'\n",
      "high_noise_model/diffusion_pytorch_model(…): 100%|█| 9.99G/9.99G [00:12<00:00, 8\n",
      "Download complete. Moving file to Wan2.2-T2V-A14B/high_noise_model/diffusion_pytorch_model-00001-of-00006.safetensors\n",
      "Downloading 'high_noise_model/diffusion_pytorch_model-00002-of-00006.safetensors' to 'Wan2.2-T2V-A14B/.cache/huggingface/download/high_noise_model/Kn8G95r2Em00Nl63-0zVOKjNfPc=.6719620a0feb4553d0306296ba4fad7d6d3b70b666fcbd02ad41fd21df3ab6af.incomplete'\n",
      "high_noise_model/diffusion_pytorch_model(…): 100%|█| 9.94G/9.94G [00:11<00:00, 8\n",
      "Download complete. Moving file to Wan2.2-T2V-A14B/high_noise_model/diffusion_pytorch_model-00002-of-00006.safetensors\n",
      "Downloading 'high_noise_model/diffusion_pytorch_model-00003-of-00006.safetensors' to 'Wan2.2-T2V-A14B/.cache/huggingface/download/high_noise_model/lFGeO7bGlU3m3RgYSJm1QFmmR7E=.0eb7864481499e65698ab94ddfad00e1289960b9f1a45aed6530ba6701859513.incomplete'\n",
      "high_noise_model/diffusion_pytorch_model(…): 100%|█| 9.94G/9.94G [00:11<00:00, 8\n",
      "Download complete. Moving file to Wan2.2-T2V-A14B/high_noise_model/diffusion_pytorch_model-00003-of-00006.safetensors\n",
      "Downloading 'high_noise_model/diffusion_pytorch_model-00004-of-00006.safetensors' to 'Wan2.2-T2V-A14B/.cache/huggingface/download/high_noise_model/x-VRNyWTgP4y3SFm1Da2Wja3TUE=.8646c9af94dbb2917380dbb6c2ba7fa34332566f2f9f0f508750e377fdad8e0a.incomplete'\n",
      "high_noise_model/diffusion_pytorch_model(…): 100%|█| 9.84G/9.84G [00:11<00:00, 8\n",
      "Download complete. Moving file to Wan2.2-T2V-A14B/high_noise_model/diffusion_pytorch_model-00004-of-00006.safetensors\n",
      "Downloading 'high_noise_model/diffusion_pytorch_model-00005-of-00006.safetensors' to 'Wan2.2-T2V-A14B/.cache/huggingface/download/high_noise_model/upxoVy5Njri4Gs1hY6atjpwUIRQ=.743ca2839d8339eeac1419de9bcfb6f02d4eab4264b01db6d4970047ae9385d0.incomplete'\n",
      "high_noise_model/diffusion_pytorch_model(…): 100%|█| 9.84G/9.84G [00:13<00:00, 7\n",
      "Download complete. Moving file to Wan2.2-T2V-A14B/high_noise_model/diffusion_pytorch_model-00005-of-00006.safetensors\n",
      "Downloading 'high_noise_model/diffusion_pytorch_model-00006-of-00006.safetensors' to 'Wan2.2-T2V-A14B/.cache/huggingface/download/high_noise_model/7owkydFacB_uwG4OcO9V4yD-SUk=.78e445e6ca232fb2b8669673b8f7e2f607f84b12e0489429577e8aea624ad7c1.incomplete'\n",
      "high_noise_model/diffusion_pytorch_model(…): 100%|█| 7.60G/7.60G [00:09<00:00, 7\n",
      "Download complete. Moving file to Wan2.2-T2V-A14B/high_noise_model/diffusion_pytorch_model-00006-of-00006.safetensors\n",
      "Downloading 'high_noise_model/diffusion_pytorch_model.safetensors.index.json' to 'Wan2.2-T2V-A14B/.cache/huggingface/download/high_noise_model/VpHHFEQcyqWBKjkjOBMoMr6uTZU=.41fcf67b4865048a4859004640117accd8e5ca09.incomplete'\n",
      "(…)ion_pytorch_model.safetensors.index.json: 96.8kB [00:00, 341MB/s]\n",
      "Download complete. Moving file to Wan2.2-T2V-A14B/high_noise_model/diffusion_pytorch_model.safetensors.index.json\n",
      "/workspace/Wan2.2-T2V-A14B\n"
     ]
    }
   ],
   "source": [
    "!pip install \"huggingface_hub[cli]\"\n",
    "# !huggingface-cli download Wan-AI/Wan2.2-T2V-A14B --local-dir ./Wan2.2-T2V-A14B --exclude \"high_noise_model/*\"\n",
    "!huggingface-cli download Wan-AI/Wan2.2-T2V-A14B \\\n",
    "  --local-dir ./Wan2.2-T2V-A14B \\\n",
    "  --include \"low_noise_model/*\"\n",
    "!huggingface-cli download Wan-AI/Wan2.2-T2V-A14B \\\n",
    "  --local-dir ./Wan2.2-T2V-A14B \\\n",
    "  --include \"Wan2.1_VAE.pth\"\n",
    "!huggingface-cli download Wan-AI/Wan2.2-T2V-A14B \\\n",
    "  --local-dir ./Wan2.2-T2V-A14B \\\n",
    "  --include \"configuration.json\"\n",
    "!huggingface-cli download Wan-AI/Wan2.2-T2V-A14B \\\n",
    "  --local-dir ./Wan2.2-T2V-A14B \\\n",
    "  --include \"high_noise_model/*\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPCshBEaQX23",
    "outputId": "344b91c3-2abe-4fd0-b36f-18d4d8f7d6a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    import flash_attn_interface\n",
    "    FLASH_ATTN_3_AVAILABLE = True\n",
    "except ModuleNotFoundError:\n",
    "    FLASH_ATTN_3_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import flash_attn\n",
    "    FLASH_ATTN_2_AVAILABLE = True\n",
    "except ModuleNotFoundError:\n",
    "    FLASH_ATTN_2_AVAILABLE = False\n",
    "\n",
    "import warnings\n",
    "\n",
    "__all__ = [\n",
    "    'flash_attention',\n",
    "    'attention',\n",
    "]\n",
    "\n",
    "\n",
    "def flash_attention(\n",
    "    q,\n",
    "    k,\n",
    "    v,\n",
    "    q_lens=None,\n",
    "    k_lens=None,\n",
    "    dropout_p=0.,\n",
    "    softmax_scale=None,\n",
    "    q_scale=None,\n",
    "    causal=False,\n",
    "    window_size=(-1, -1),\n",
    "    deterministic=False,\n",
    "    dtype=torch.bfloat16,\n",
    "    version=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    q:              [B, Lq, Nq, C1].\n",
    "    k:              [B, Lk, Nk, C1].\n",
    "    v:              [B, Lk, Nk, C2]. Nq must be divisible by Nk.\n",
    "    q_lens:         [B].\n",
    "    k_lens:         [B].\n",
    "    dropout_p:      float. Dropout probability.\n",
    "    softmax_scale:  float. The scaling of QK^T before applying softmax.\n",
    "    causal:         bool. Whether to apply causal attention mask.\n",
    "    window_size:    (left right). If not (-1, -1), apply sliding window local attention.\n",
    "    deterministic:  bool. If True, slightly slower and uses more memory.\n",
    "    dtype:          torch.dtype. Apply when dtype of q/k/v is not float16/bfloat16.\n",
    "    \"\"\"\n",
    "    half_dtypes = (torch.float16, torch.bfloat16)\n",
    "    assert dtype in half_dtypes\n",
    "    assert q.device.type == 'cuda' and q.size(-1) <= 256\n",
    "\n",
    "    # params\n",
    "    b, lq, lk, out_dtype = q.size(0), q.size(1), k.size(1), q.dtype\n",
    "\n",
    "    def half(x):\n",
    "        return x if x.dtype in half_dtypes else x.to(dtype)\n",
    "\n",
    "    # preprocess query\n",
    "    if q_lens is None:\n",
    "        q = half(q.flatten(0, 1))\n",
    "        q_lens = torch.tensor(\n",
    "            [lq] * b, dtype=torch.int32).to(\n",
    "                device=q.device, non_blocking=True)\n",
    "    else:\n",
    "        q = half(torch.cat([u[:v] for u, v in zip(q, q_lens)]))\n",
    "\n",
    "    # preprocess key, value\n",
    "    if k_lens is None:\n",
    "        k = half(k.flatten(0, 1))\n",
    "        v = half(v.flatten(0, 1))\n",
    "        k_lens = torch.tensor(\n",
    "            [lk] * b, dtype=torch.int32).to(\n",
    "                device=k.device, non_blocking=True)\n",
    "    else:\n",
    "        k = half(torch.cat([u[:v] for u, v in zip(k, k_lens)]))\n",
    "        v = half(torch.cat([u[:v] for u, v in zip(v, k_lens)]))\n",
    "\n",
    "    q = q.to(v.dtype)\n",
    "    k = k.to(v.dtype)\n",
    "\n",
    "    if q_scale is not None:\n",
    "        q = q * q_scale\n",
    "\n",
    "    if version is not None and version == 3 and not FLASH_ATTN_3_AVAILABLE:\n",
    "        warnings.warn(\n",
    "            'Flash attention 3 is not available, use flash attention 2 instead.'\n",
    "        )\n",
    "\n",
    "    # apply attention\n",
    "    if (version is None or version == 3) and FLASH_ATTN_3_AVAILABLE:\n",
    "        # Note: dropout_p, window_size are not supported in FA3 now.\n",
    "        x = flash_attn_interface.flash_attn_varlen_func(\n",
    "            q=q,\n",
    "            k=k,\n",
    "            v=v,\n",
    "            cu_seqlens_q=torch.cat([q_lens.new_zeros([1]), q_lens]).cumsum(\n",
    "                0, dtype=torch.int32).to(q.device, non_blocking=True),\n",
    "            cu_seqlens_k=torch.cat([k_lens.new_zeros([1]), k_lens]).cumsum(\n",
    "                0, dtype=torch.int32).to(q.device, non_blocking=True),\n",
    "            seqused_q=None,\n",
    "            seqused_k=None,\n",
    "            max_seqlen_q=lq,\n",
    "            max_seqlen_k=lk,\n",
    "            softmax_scale=softmax_scale,\n",
    "            causal=causal,\n",
    "            deterministic=deterministic)[0].unflatten(0, (b, lq))\n",
    "    else:\n",
    "        assert FLASH_ATTN_2_AVAILABLE\n",
    "        x = flash_attn.flash_attn_varlen_func(\n",
    "            q=q,\n",
    "            k=k,\n",
    "            v=v,\n",
    "            cu_seqlens_q=torch.cat([q_lens.new_zeros([1]), q_lens]).cumsum(\n",
    "                0, dtype=torch.int32).to(q.device, non_blocking=True),\n",
    "            cu_seqlens_k=torch.cat([k_lens.new_zeros([1]), k_lens]).cumsum(\n",
    "                0, dtype=torch.int32).to(q.device, non_blocking=True),\n",
    "            max_seqlen_q=lq,\n",
    "            max_seqlen_k=lk,\n",
    "            dropout_p=dropout_p,\n",
    "            softmax_scale=softmax_scale,\n",
    "            causal=causal,\n",
    "            window_size=window_size,\n",
    "            deterministic=deterministic).unflatten(0, (b, lq))\n",
    "\n",
    "    # output\n",
    "    return x.type(out_dtype)\n",
    "\n",
    "\n",
    "def attention(\n",
    "    q,\n",
    "    k,\n",
    "    v,\n",
    "    q_lens=None,\n",
    "    k_lens=None,\n",
    "    dropout_p=0.,\n",
    "    softmax_scale=None,\n",
    "    q_scale=None,\n",
    "    causal=False,\n",
    "    window_size=(-1, -1),\n",
    "    deterministic=False,\n",
    "    dtype=torch.bfloat16,\n",
    "    fa_version=None,\n",
    "):\n",
    "    if FLASH_ATTN_2_AVAILABLE or FLASH_ATTN_3_AVAILABLE:\n",
    "        return flash_attention(\n",
    "            q=q,\n",
    "            k=k,\n",
    "            v=v,\n",
    "            q_lens=q_lens,\n",
    "            k_lens=k_lens,\n",
    "            dropout_p=dropout_p,\n",
    "            softmax_scale=softmax_scale,\n",
    "            q_scale=q_scale,\n",
    "            causal=causal,\n",
    "            window_size=window_size,\n",
    "            deterministic=deterministic,\n",
    "            dtype=dtype,\n",
    "            version=fa_version,\n",
    "        )\n",
    "    else:\n",
    "        if q_lens is not None or k_lens is not None:\n",
    "            warnings.warn(\n",
    "                'Padding mask is disabled when using scaled_dot_product_attention. It can have a significant impact on performance.'\n",
    "            )\n",
    "        attn_mask = None\n",
    "\n",
    "        q = q.transpose(1, 2).to(dtype)\n",
    "        k = k.transpose(1, 2).to(dtype)\n",
    "        v = v.transpose(1, 2).to(dtype)\n",
    "\n",
    "        out = torch.nn.functional.scaled_dot_product_attention(\n",
    "            q, k, v, attn_mask=attn_mask, is_causal=causal, dropout_p=dropout_p)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous()\n",
    "        return out\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "haqbECdvQhnt",
    "outputId": "1a3b540a-fb85-4533-ce6d-339a7562ea5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from diffusers.configuration_utils import ConfigMixin, register_to_config\n",
    "from diffusers.models.modeling_utils import ModelMixin\n",
    "\n",
    "\n",
    "__all__ = ['WanModel']\n",
    "\n",
    "\n",
    "def sinusoidal_embedding_1d(dim, position):\n",
    "    # preprocess\n",
    "    assert dim % 2 == 0\n",
    "    half = dim // 2\n",
    "    position = position.type(torch.float64)\n",
    "\n",
    "    # calculation\n",
    "    sinusoid = torch.outer(\n",
    "        position, torch.pow(10000, -torch.arange(half).to(position).div(half)))\n",
    "    x = torch.cat([torch.cos(sinusoid), torch.sin(sinusoid)], dim=1)\n",
    "    return x\n",
    "\n",
    "\n",
    "@torch.amp.autocast('cuda', enabled=False)\n",
    "def rope_params(max_seq_len, dim, theta=10000):\n",
    "    assert dim % 2 == 0\n",
    "    freqs = torch.outer(\n",
    "        torch.arange(max_seq_len),\n",
    "        1.0 / torch.pow(theta,\n",
    "                        torch.arange(0, dim, 2).to(torch.float64).div(dim)))\n",
    "    freqs = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs\n",
    "\n",
    "\n",
    "@torch.amp.autocast('cuda', enabled=False)\n",
    "def rope_apply(x, grid_sizes, freqs):\n",
    "    n, c = x.size(2), x.size(3) // 2\n",
    "\n",
    "    # split freqs\n",
    "    freqs = freqs.split([c - 2 * (c // 3), c // 3, c // 3], dim=1)\n",
    "\n",
    "    # loop over samples\n",
    "    output = []\n",
    "    for i, (f, h, w) in enumerate(grid_sizes.tolist()):\n",
    "        seq_len = f * h * w\n",
    "\n",
    "        # precompute multipliers\n",
    "        x_i = torch.view_as_complex(x[i, :seq_len].to(torch.float64).reshape(\n",
    "            seq_len, n, -1, 2))\n",
    "        freqs_i = torch.cat([\n",
    "            freqs[0][:f].view(f, 1, 1, -1).expand(f, h, w, -1),\n",
    "            freqs[1][:h].view(1, h, 1, -1).expand(f, h, w, -1),\n",
    "            freqs[2][:w].view(1, 1, w, -1).expand(f, h, w, -1)\n",
    "        ],\n",
    "                            dim=-1).reshape(seq_len, 1, -1)\n",
    "\n",
    "        # apply rotary embedding\n",
    "        x_i = torch.view_as_real(x_i * freqs_i).flatten(2)\n",
    "        x_i = torch.cat([x_i, x[i, seq_len:]])\n",
    "\n",
    "        # append to collection\n",
    "        output.append(x_i)\n",
    "    return torch.stack(output).float()\n",
    "\n",
    "\n",
    "class WanRMSNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            x(Tensor): Shape [B, L, C]\n",
    "        \"\"\"\n",
    "        return self._norm(x.float()).type_as(x) * self.weight\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
    "\n",
    "\n",
    "class WanLayerNorm(nn.LayerNorm):\n",
    "\n",
    "    def __init__(self, dim, eps=1e-6, elementwise_affine=False):\n",
    "        super().__init__(dim, elementwise_affine=elementwise_affine, eps=eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            x(Tensor): Shape [B, L, C]\n",
    "        \"\"\"\n",
    "        return super().forward(x.float()).type_as(x)\n",
    "\n",
    "\n",
    "class WanSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 window_size=(-1, -1),\n",
    "                 qk_norm=True,\n",
    "                 eps=1e-6):\n",
    "        assert dim % num_heads == 0\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.window_size = window_size\n",
    "        self.qk_norm = qk_norm\n",
    "        self.eps = eps\n",
    "\n",
    "        # layers\n",
    "        self.q = nn.Linear(dim, dim)\n",
    "        self.k = nn.Linear(dim, dim)\n",
    "        self.v = nn.Linear(dim, dim)\n",
    "        self.o = nn.Linear(dim, dim)\n",
    "        self.norm_q = WanRMSNorm(dim, eps=eps) if qk_norm else nn.Identity()\n",
    "        self.norm_k = WanRMSNorm(dim, eps=eps) if qk_norm else nn.Identity()\n",
    "\n",
    "    def forward(self, x, seq_lens, grid_sizes, freqs):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            x(Tensor): Shape [B, L, num_heads, C / num_heads]\n",
    "            seq_lens(Tensor): Shape [B]\n",
    "            grid_sizes(Tensor): Shape [B, 3], the second dimension contains (F, H, W)\n",
    "            freqs(Tensor): Rope freqs, shape [1024, C / num_heads / 2]\n",
    "        \"\"\"\n",
    "        b, s, n, d = *x.shape[:2], self.num_heads, self.head_dim\n",
    "\n",
    "        # query, key, value function\n",
    "        def qkv_fn(x):\n",
    "            q = self.norm_q(self.q(x)).view(b, s, n, d)\n",
    "            k = self.norm_k(self.k(x)).view(b, s, n, d)\n",
    "            v = self.v(x).view(b, s, n, d)\n",
    "            return q, k, v\n",
    "\n",
    "        q, k, v = qkv_fn(x)\n",
    "#---------------------------------        \n",
    "        q = rope_apply(q, grid_sizes, freqs)\n",
    "        k = rope_apply(k, grid_sizes, freqs)\n",
    "        \n",
    "        # ensure q/k are tensors and on same device as v (usually cuda)\n",
    "        q = q.to(v.device) if not isinstance(q, (list, tuple)) else q  # 안전장치\n",
    "        k = k.to(v.device) if not isinstance(k, (list, tuple)) else k\n",
    "        \n",
    "#---------------------------------\n",
    "        x = flash_attention(\n",
    "            q=q,\n",
    "            k=k,\n",
    "            v=v,\n",
    "            k_lens=seq_lens,\n",
    "            window_size=self.window_size)\n",
    "\n",
    "        # after attention, offload q/k to CPU to save GPU memory\n",
    "        try:\n",
    "            # detach to avoid keeping graph and move to cpu\n",
    "            self.rope_q = q.detach().cpu() if isinstance(q, torch.Tensor) else tuple(t.detach().cpu() for t in q)\n",
    "            self.rope_k = k.detach().cpu() if isinstance(k, torch.Tensor) else tuple(t.detach().cpu() for t in k)\n",
    "            # delete GPU copies if you won't use q/k on GPU anymore\n",
    "            del q, k\n",
    "            torch.cuda.empty_cache()\n",
    "        except Exception as e:\n",
    "            # 안전하게 실패해도 실행 흐름은 유지\n",
    "            print(\"offload failed:\", e)\n",
    "            \n",
    "        # output\n",
    "        x = x.flatten(2)\n",
    "        x = self.o(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class WanCrossAttention(WanSelfAttention):\n",
    "\n",
    "    def forward(self, x, context, context_lens):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            x(Tensor): Shape [B, L1, C]\n",
    "            context(Tensor): Shape [B, L2, C]\n",
    "            context_lens(Tensor): Shape [B]\n",
    "        \"\"\"\n",
    "        b, n, d = x.size(0), self.num_heads, self.head_dim\n",
    "\n",
    "        # compute query, key, value\n",
    "        q = self.norm_q(self.q(x)).view(b, -1, n, d)\n",
    "        k = self.norm_k(self.k(context)).view(b, -1, n, d)\n",
    "        v = self.v(context).view(b, -1, n, d)\n",
    "\n",
    "        # compute attention\n",
    "        x = flash_attention(q, k, v, k_lens=context_lens)\n",
    "\n",
    "        # output\n",
    "        x = x.flatten(2)\n",
    "        x = self.o(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class WanAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 ffn_dim,\n",
    "                 num_heads,\n",
    "                 window_size=(-1, -1),\n",
    "                 qk_norm=True,\n",
    "                 cross_attn_norm=False,\n",
    "                 eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.qk_norm = qk_norm\n",
    "        self.cross_attn_norm = cross_attn_norm\n",
    "        self.eps = eps\n",
    "\n",
    "        # layers\n",
    "        self.norm1 = WanLayerNorm(dim, eps)\n",
    "        self.self_attn = WanSelfAttention(dim, num_heads, window_size, qk_norm,\n",
    "                                          eps)\n",
    "        self.norm3 = WanLayerNorm(\n",
    "            dim, eps,\n",
    "            elementwise_affine=True) if cross_attn_norm else nn.Identity()\n",
    "        self.cross_attn = WanCrossAttention(dim, num_heads, (-1, -1), qk_norm,\n",
    "                                            eps)\n",
    "        self.norm2 = WanLayerNorm(dim, eps)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, ffn_dim), nn.GELU(approximate='tanh'),\n",
    "            nn.Linear(ffn_dim, dim))\n",
    "\n",
    "        # modulation\n",
    "        self.modulation = nn.Parameter(torch.randn(1, 6, dim) / dim**0.5)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        e,\n",
    "        seq_lens,\n",
    "        grid_sizes,\n",
    "        freqs,\n",
    "        context,\n",
    "        context_lens,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            x(Tensor): Shape [B, L, C]\n",
    "            e(Tensor): Shape [B, L1, 6, C]\n",
    "            seq_lens(Tensor): Shape [B], length of each sequence in batch\n",
    "            grid_sizes(Tensor): Shape [B, 3], the second dimension contains (F, H, W)\n",
    "            freqs(Tensor): Rope freqs, shape [1024, C / num_heads / 2]\n",
    "        \"\"\"\n",
    "        assert e.dtype == torch.float32\n",
    "        with torch.amp.autocast('cuda', dtype=torch.float32):\n",
    "            e = (self.modulation.unsqueeze(0) + e).chunk(6, dim=2)\n",
    "        assert e[0].dtype == torch.float32\n",
    "\n",
    "        # self-attention\n",
    "        y = self.self_attn(\n",
    "            self.norm1(x).float() * (1 + e[1].squeeze(2)) + e[0].squeeze(2),\n",
    "            seq_lens, grid_sizes, freqs)\n",
    "        with torch.amp.autocast('cuda', dtype=torch.float32):\n",
    "            x = x + y * e[2].squeeze(2)\n",
    "\n",
    "        # cross-attention & ffn function\n",
    "        def cross_attn_ffn(x, context, context_lens, e):\n",
    "            x = x + self.cross_attn(self.norm3(x), context, context_lens)\n",
    "            y = self.ffn(\n",
    "                self.norm2(x).float() * (1 + e[4].squeeze(2)) + e[3].squeeze(2))\n",
    "            with torch.amp.autocast('cuda', dtype=torch.float32):\n",
    "                x = x + y * e[5].squeeze(2)\n",
    "            return x\n",
    "\n",
    "        x = cross_attn_ffn(x, context, context_lens, e)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, out_dim, patch_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.out_dim = out_dim\n",
    "        self.patch_size = patch_size\n",
    "        self.eps = eps\n",
    "\n",
    "        # layers\n",
    "        out_dim = math.prod(patch_size) * out_dim\n",
    "        self.norm = WanLayerNorm(dim, eps)\n",
    "        self.head = nn.Linear(dim, out_dim)\n",
    "\n",
    "        # modulation\n",
    "        self.modulation = nn.Parameter(torch.randn(1, 2, dim) / dim**0.5)\n",
    "\n",
    "    def forward(self, x, e):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            x(Tensor): Shape [B, L1, C]\n",
    "            e(Tensor): Shape [B, L1, C]\n",
    "        \"\"\"\n",
    "        assert e.dtype == torch.float32\n",
    "        with torch.amp.autocast('cuda', dtype=torch.float32):\n",
    "            e = (self.modulation.unsqueeze(0) + e.unsqueeze(2)).chunk(2, dim=2)\n",
    "            x = (\n",
    "                self.head(\n",
    "                    self.norm(x) * (1 + e[1].squeeze(2)) + e[0].squeeze(2)))\n",
    "        return x\n",
    "\n",
    "#diffuesr의 클래스는 대부분\n",
    "# from diffusers import ModelMixin, ConfigMixin\n",
    "# class UNet2DConditionModel(ModelMixin, ConfigMixin):\n",
    "# class ConfigMixin(FromPretrainedMixin): 내부적으로 frompretrainedmixin을 상속받고 이 class가 from_pretrained를 가짐\n",
    "# self.low_noise_model = WanModel.from_pretrained(\n",
    "#     checkpoint_dir, subfolder=config.low_noise_checkpoint)\n",
    "class WanModel(ModelMixin, ConfigMixin):\n",
    "    r\"\"\"\n",
    "    Wan diffusion backbone supporting both text-to-video and image-to-video.\n",
    "    \"\"\"\n",
    "\n",
    "    ignore_for_config = [\n",
    "        'patch_size', 'cross_attn_norm', 'qk_norm', 'text_dim', 'window_size'\n",
    "    ]\n",
    "    _no_split_modules = ['WanAttentionBlock']\n",
    "\n",
    "    @register_to_config\n",
    "    def __init__(self,\n",
    "                 model_type='t2v',\n",
    "                 patch_size=(1, 2, 2),\n",
    "                 text_len=512,\n",
    "                 in_dim=16,\n",
    "                 dim=2048,\n",
    "                 ffn_dim=8192,\n",
    "                 freq_dim=256,\n",
    "                 text_dim=4096,\n",
    "                 out_dim=16,\n",
    "                 num_heads=16,\n",
    "                 num_layers=32,\n",
    "                 window_size=(-1, -1),\n",
    "                 qk_norm=True,\n",
    "                 cross_attn_norm=True,\n",
    "                 eps=1e-6):\n",
    "        r\"\"\"\n",
    "\n",
    "        Initialize the diffusion model backbone.\n",
    "\n",
    "        Args:\n",
    "            model_type (`str`, *optional*, defaults to 't2v'):\n",
    "                Model variant - 't2v' (text-to-video) or 'i2v' (image-to-video)\n",
    "            patch_size (`tuple`, *optional*, defaults to (1, 2, 2)):\n",
    "                3D patch dimensions for video embedding (t_patch, h_patch, w_patch)\n",
    "            text_len (`int`, *optional*, defaults to 512):\n",
    "                Fixed length for text embeddings\n",
    "            in_dim (`int`, *optional*, defaults to 16):\n",
    "                Input video channels (C_in)\n",
    "            dim (`int`, *optional*, defaults to 2048):\n",
    "                Hidden dimension of the transformer\n",
    "            ffn_dim (`int`, *optional*, defaults to 8192):\n",
    "                Intermediate dimension in feed-forward network\n",
    "            freq_dim (`int`, *optional*, defaults to 256):\n",
    "                Dimension for sinusoidal time embeddings\n",
    "            text_dim (`int`, *optional*, defaults to 4096):\n",
    "                Input dimension for text embeddings\n",
    "            out_dim (`int`, *optional*, defaults to 16):\n",
    "                Output video channels (C_out)\n",
    "            num_heads (`int`, *optional*, defaults to 16):\n",
    "                Number of attention heads\n",
    "            num_layers (`int`, *optional*, defaults to 32):\n",
    "                Number of transformer blocks\n",
    "            window_size (`tuple`, *optional*, defaults to (-1, -1)):\n",
    "                Window size for local attention (-1 indicates global attention)\n",
    "            qk_norm (`bool`, *optional*, defaults to True):\n",
    "                Enable query/key normalization\n",
    "            cross_attn_norm (`bool`, *optional*, defaults to False):\n",
    "                Enable cross-attention normalization\n",
    "            eps (`float`, *optional*, defaults to 1e-6):\n",
    "                Epsilon value for normalization layers\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        assert model_type in ['t2v', 'i2v', 'ti2v', 's2v']\n",
    "        self.model_type = model_type\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.text_len = text_len\n",
    "        self.in_dim = in_dim\n",
    "        self.dim = dim\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.freq_dim = freq_dim\n",
    "        self.text_dim = text_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.window_size = window_size\n",
    "        self.qk_norm = qk_norm\n",
    "        self.cross_attn_norm = cross_attn_norm\n",
    "        self.eps = eps\n",
    "\n",
    "        # embeddings\n",
    "        self.patch_embedding = nn.Conv3d(\n",
    "            in_dim, dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.text_embedding = nn.Sequential(\n",
    "            nn.Linear(text_dim, dim), nn.GELU(approximate='tanh'),\n",
    "            nn.Linear(dim, dim))\n",
    "\n",
    "        self.time_embedding = nn.Sequential(\n",
    "            nn.Linear(freq_dim, dim), nn.SiLU(), nn.Linear(dim, dim))\n",
    "        self.time_projection = nn.Sequential(nn.SiLU(), nn.Linear(dim, dim * 6))\n",
    "\n",
    "        # blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            WanAttentionBlock(dim, ffn_dim, num_heads, window_size, qk_norm,\n",
    "                              cross_attn_norm, eps) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # head\n",
    "        self.head = Head(dim, out_dim, patch_size, eps)\n",
    "\n",
    "        # buffers (don't use register_buffer otherwise dtype will be changed in to())\n",
    "        assert (dim % num_heads) == 0 and (dim // num_heads) % 2 == 0\n",
    "        d = dim // num_heads\n",
    "        self.freqs = torch.cat([\n",
    "            rope_params(1024, d - 4 * (d // 6)),\n",
    "            rope_params(1024, 2 * (d // 6)),\n",
    "            rope_params(1024, 2 * (d // 6))\n",
    "        ],\n",
    "                               dim=1)\n",
    "\n",
    "        # initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        t,\n",
    "        context,\n",
    "        seq_len,\n",
    "        y=None,\n",
    "    ):\n",
    "        r\"\"\" arg_c = { 'context': [context[0]], 'seq_len': max_seq_len, 'y': [y],}\n",
    "        noise_pred_cond = model(\n",
    "            latent_model_input, t=timestep, **arg_c)[0]\n",
    "        if offload_model:\n",
    "            torch.cuda.empty_cache()\n",
    "        noise_pred_uncond = model(\n",
    "            latent_model_input, t=timestep, **arg_null)[0]\n",
    "        if offload_model:\n",
    "            torch.cuda.empty_cache()\n",
    "        noise_pred = noise_pred_uncond + sample_guide_scale * (\n",
    "            noise_pred_cond - noise_pred_uncond)\n",
    "\n",
    "        Args:\n",
    "            x (List[Tensor]):\n",
    "                List of input video tensors, each with shape [C_in, F, H, W]\n",
    "            t (Tensor):\n",
    "                Diffusion timesteps tensor of shape [B]\n",
    "            context (List[Tensor]):\n",
    "                List of text embeddings each with shape [L, C]\n",
    "            seq_len (`int`):\n",
    "                Maximum sequence length for positional encoding\n",
    "            y (List[Tensor], *optional*):\n",
    "                Conditional video inputs for image-to-video mode, same shape as x\n",
    "\n",
    "        Returns:\n",
    "            List[Tensor]:\n",
    "                List of denoised video tensors with original input shapes [C_out, F, H / 8, W / 8]\n",
    "        \"\"\"\n",
    "        if self.model_type == 'i2v':\n",
    "            assert y is not None\n",
    "        # params\n",
    "        device = self.patch_embedding.weight.device\n",
    "        if self.freqs.device != device:\n",
    "            self.freqs = self.freqs.to(device)\n",
    "\n",
    "        if y is not None:\n",
    "            x = [torch.cat([u, v], dim=0) for u, v in zip(x, y)]\n",
    "\n",
    "        # embeddings\n",
    "        x = [self.patch_embedding(u.unsqueeze(0)) for u in x]\n",
    "        grid_sizes = torch.stack(\n",
    "            [torch.tensor(u.shape[2:], dtype=torch.long) for u in x])\n",
    "        x = [u.flatten(2).transpose(1, 2) for u in x]\n",
    "        seq_lens = torch.tensor([u.size(1) for u in x], dtype=torch.long)\n",
    "        assert seq_lens.max() <= seq_len\n",
    "        x = torch.cat([\n",
    "            torch.cat([u, u.new_zeros(1, seq_len - u.size(1), u.size(2))],\n",
    "                      dim=1) for u in x\n",
    "        ])\n",
    "\n",
    "        # time embeddings\n",
    "        if t.dim() == 1:\n",
    "            t = t.expand(t.size(0), seq_len)\n",
    "        with torch.amp.autocast('cuda', dtype=torch.float32):\n",
    "            bt = t.size(0)\n",
    "            t = t.flatten()\n",
    "            e = self.time_embedding(\n",
    "                sinusoidal_embedding_1d(self.freq_dim,\n",
    "                                        t).unflatten(0, (bt, seq_len)).float())\n",
    "            e0 = self.time_projection(e).unflatten(2, (6, self.dim))\n",
    "            assert e.dtype == torch.float32 and e0.dtype == torch.float32\n",
    "\n",
    "        # context\n",
    "        context_lens = None\n",
    "        context = self.text_embedding(\n",
    "            torch.stack([\n",
    "                torch.cat(\n",
    "                    [u, u.new_zeros(self.text_len - u.size(0), u.size(1))])\n",
    "                for u in context\n",
    "            ]))\n",
    "\n",
    "        # arguments\n",
    "        kwargs = dict(\n",
    "            e=e0,\n",
    "            seq_lens=seq_lens,\n",
    "            grid_sizes=grid_sizes,\n",
    "            freqs=self.freqs,\n",
    "            context=context,\n",
    "            context_lens=context_lens)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, **kwargs)\n",
    "\n",
    "        # head\n",
    "        x = self.head(x, e)\n",
    "\n",
    "        # unpatchify\n",
    "        x = self.unpatchify(x, grid_sizes)\n",
    "        return [u.float() for u in x]\n",
    "\n",
    "    def unpatchify(self, x, grid_sizes):\n",
    "        r\"\"\"\n",
    "        Reconstruct video tensors from patch embeddings.\n",
    "\n",
    "        Args:\n",
    "            x (List[Tensor]):\n",
    "                List of patchified features, each with shape [L, C_out * prod(patch_size)]\n",
    "            grid_sizes (Tensor):\n",
    "                Original spatial-temporal grid dimensions before patching,\n",
    "                    shape [B, 3] (3 dimensions correspond to F_patches, H_patches, W_patches)\n",
    "\n",
    "        Returns:\n",
    "            List[Tensor]:\n",
    "                Reconstructed video tensors with shape [C_out, F, H / 8, W / 8]\n",
    "        \"\"\"\n",
    "\n",
    "        c = self.out_dim\n",
    "        out = []\n",
    "        for u, v in zip(x, grid_sizes.tolist()):\n",
    "            u = u[:math.prod(v)].view(*v, *self.patch_size, c)\n",
    "            u = torch.einsum('fhwpqrc->cfphqwr', u)\n",
    "            u = u.reshape(c, *[i * j for i, j in zip(v, self.patch_size)])\n",
    "            out.append(u)\n",
    "        return out\n",
    "\n",
    "    def init_weights(self):\n",
    "        r\"\"\"\n",
    "        Initialize model parameters using Xavier initialization.\n",
    "        \"\"\"\n",
    "\n",
    "        # basic init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "        # init embeddings\n",
    "        nn.init.xavier_uniform_(self.patch_embedding.weight.flatten(1))\n",
    "        for m in self.text_embedding.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, std=.02)\n",
    "        for m in self.time_embedding.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, std=.02)\n",
    "\n",
    "        # init output layer\n",
    "        nn.init.zeros_(self.head.head.weight)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dwGUj7-X2k-5"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def sort_state_dict_by_blocknum(state_dict):\n",
    "    \"\"\"\n",
    "    Sort state_dict keys by 'blocks.<num>' numeric order.\n",
    "    If key doesn't contain 'blocks.<num>', send it to the end.\n",
    "    \"\"\"\n",
    "    def extract_block_num(key):\n",
    "        match = re.search(r'blocks\\.(\\d+)', key)\n",
    "        return int(match.group(1)) if match else 9999\n",
    "\n",
    "    # 키를 블록 번호 기준으로 정렬\n",
    "    sorted_keys = sorted(state_dict.keys(), key=lambda k: extract_block_num(k))\n",
    "\n",
    "    # 정렬된 순서로 새 state_dict 구성\n",
    "    sorted_state_dict = {k: state_dict[k] for k in sorted_keys}\n",
    "    return sorted_state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GIbvjIKjWJ3q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# 그냥 lora\n",
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "def insert_lora(path1,path2, model, k=1, only_A=False, only_B=False, block = 40,device='cuda'):\n",
    "  lora1 = load_file(path1)\n",
    "  lora1 = sort_state_dict_by_blocknum(lora1)\n",
    "  lora2 = load_file(path2)\n",
    "  lora2 = sort_state_dict_by_blocknum(lora2) #순서 정렬\n",
    "  lora1 = {k.replace(\"diffusion_model.\", \"\", 1): v for k, v in lora1.items()} # diffusion_model. 제거\n",
    "  lora2 = {k.replace(\"diffusion_model.\", \"\", 1): v for k, v in lora2.items()}\n",
    "\n",
    "  lora_pairs1 = {}\n",
    "  lora_pairs2 = {}\n",
    "  timestep = block * 20 # LoRA가 총 800개 - 40개의 block이 - 10개 layer(ffn, attn) - LoRA A,B 행렬\n",
    "  # for key1,key2 in zip(lora1.keys()[:timestep],lora2.keys()[:timestep]):\n",
    "  for key1, key2 in zip(list(lora1.keys())[:timestep], list(lora2.keys())[:timestep]):\n",
    "\n",
    "      if 'lora_A.weight' in key1:\n",
    "        print(key1)\n",
    "        base_key = key1.replace('.lora_A.weight', '')\n",
    "        lora_b_key = base_key + '.lora_B.weight'\n",
    "        if lora_b_key in lora1:\n",
    "            lora_pairs1[base_key] = {\n",
    "                'A': lora1[key1],\n",
    "                'B': lora1[lora_b_key]\n",
    "            }\n",
    "      else:\n",
    "        print('lora_A 존재 x')\n",
    "      if 'lora_A.weight' in key2:\n",
    "          print(key2) #blocks.1.ffn.2.lora_A.weight\n",
    "          base_key = key2.replace('.lora_A.weight', '')\n",
    "          lora_b_key = base_key + '.lora_B.weight'\n",
    "          if lora_b_key in lora2:\n",
    "              lora_pairs2[base_key] = {\n",
    "                  'A': lora2[key2],\n",
    "                  'B': lora2[lora_b_key]\n",
    "              }\n",
    "      else:\n",
    "        print('lora_A 존재 x')\n",
    "\n",
    "  assert len(lora_pairs1.keys())==len(lora_pairs2.keys())\n",
    "  print(f'lora_pairs1 keys:{lora_pairs1.keys()}')\n",
    "  merge_lora = {}\n",
    "  for key1, key2 in zip(lora_pairs1.keys(),lora_pairs2.keys()):\n",
    "\n",
    "    if key1 == key2:\n",
    "      lora_pairs1[key1]['A'] = lora_pairs1[key1]['A'].detach().clone().float() if isinstance(lora_pairs1[key1]['A'], torch.Tensor) else torch.tensor(lora_pairs1[key1]['A'], dtype=torch.float32)\n",
    "      lora_pairs1[key1]['B'] = lora_pairs1[key1]['B'].detach().clone().float() if isinstance(lora_pairs1[key1]['B'], torch.Tensor) else torch.tensor(lora_pairs1[key1]['B'], dtype=torch.float32)\n",
    "      lora_pairs2[key2]['A'] = lora_pairs2[key2]['A'].detach().clone().float() if isinstance(lora_pairs2[key2]['A'], torch.Tensor) else torch.tensor(lora_pairs2[key2]['A'], dtype=torch.float32)\n",
    "      lora_pairs2[key2]['B'] = lora_pairs2[key2]['B'].detach().clone().float() if isinstance(lora_pairs2[key2]['B'], torch.Tensor) else torch.tensor(lora_pairs2[key2]['B'], dtype=torch.float32)\n",
    "\n",
    "      if only_A:\n",
    "        merge_key1 = key1 + '.weight'\n",
    "        merge_lora[merge_key1] = lora_pairs1[key1]\n",
    "      if only_B:\n",
    "        merge_key2 = key2 + '.weight'\n",
    "        print('second lora win')\n",
    "        merge_lora[merge_key2] = lora_pairs2[key2]\n",
    "\n",
    "    else:\n",
    "      print('key1, key2가 같지 않다')\n",
    "\n",
    "  if only_A or only_B:\n",
    "    for key, value in model.named_parameters():\n",
    "      if key in merge_lora.keys():\n",
    "        param = reduce(getattr, key.split('.'), model)\n",
    "        print(key)\n",
    "        print(param.data.shape)\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            matrix = merge_lora[key]['B'] @ merge_lora[key]['A']\n",
    "            param.data += matrix.to('cuda')\n",
    "            print(f'{key} lora 추가')\n",
    "      else:\n",
    "        print(f'{key} 파라미터 안에 lora_merge를 적용할 수 없습니다')\n",
    "        continue\n",
    "\n",
    "  else:\n",
    "    for key, value in model.named_parameters():\n",
    "      base = key.rsplit('.', 1)[0] # 마지막 토큰 제거\n",
    "      if key in [k + \".weight\" for k in lora_pairs1.keys()]:\n",
    "          print(f'base_key가 존재:{key}') # blocks.0.self_attn.q.bias도 걸러지네 이게 blocks.0.self_attn.q.weight\n",
    "          param = reduce(getattr, key.split('.'), model)\n",
    "\n",
    "          print(key) #blocks.0.self_attn.q\n",
    "          print(param.data.shape) #torch.Size([5120, 5120])\n",
    "          print(f'base:{base}')\n",
    "          with torch.amp.autocast('cuda'):\n",
    "            matrix1 = lora_pairs1[base]['B'] @ lora_pairs1[base]['A'] #'blocks.0.self_attn'\n",
    "            matrix2 = lora_pairs2[base]['B'] @ lora_pairs2[base]['A']\n",
    "            param.data += matrix1.to('cuda')\n",
    "            param.data += matrix2.to('cuda')\n",
    "            print(f'{base} lora 추가')\n",
    "  return model\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#k lora\n",
    "!pip install matplotlib -q\n",
    "import re\n",
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "def insert_klora(path1,path2, model, k=3):\n",
    "\n",
    "  lora1 = load_file(path1)\n",
    "  lora1 = sort_state_dict_by_blocknum(lora1)\n",
    "  lora2 = load_file(path2)\n",
    "  lora2 = sort_state_dict_by_blocknum(lora2)\n",
    "  lora1 = {k.replace(\"diffusion_model.\", \"\", 1): v for k, v in lora1.items()} # diffusion_model. 제거\n",
    "  lora2 = {k.replace(\"diffusion_model.\", \"\", 1): v for k, v in lora2.items()}\n",
    "    \n",
    "  lora_pairs1 = {}\n",
    "  lora_pairs2 = {}\n",
    "  for key1,key2 in zip(lora1.keys(),lora2.keys()):\n",
    "\n",
    "      if 'lora_A.weight' in key1:\n",
    "        print(key1)\n",
    "        base_key = key1.replace('.lora_A.weight', '')\n",
    "        lora_b_key = base_key + '.lora_B.weight'\n",
    "        if lora_b_key in lora1:\n",
    "            lora_pairs1[base_key] = {\n",
    "                'A': lora1[key1],\n",
    "                'B': lora1[lora_b_key]\n",
    "            }\n",
    "      else:\n",
    "        print('lora_A 존재 x')\n",
    "      if 'lora_A.weight' in key2:\n",
    "          print(key2)\n",
    "          base_key = key2.replace('.lora_A.weight', '')\n",
    "          lora_b_key = base_key + '.lora_B.weight'\n",
    "          if lora_b_key in lora2:\n",
    "              lora_pairs2[base_key] = {\n",
    "                  'A': lora2[key2],\n",
    "                  'B': lora2[lora_b_key]\n",
    "              }\n",
    "      else:\n",
    "        print('lora_A 존재 x')\n",
    "  assert len(lora_pairs1.keys())==len(lora_pairs2.keys())\n",
    "  merge_lora = {}\n",
    "  for key1, key2 in zip(lora_pairs1.keys(),lora_pairs2.keys()):\n",
    "    if key1 == key2:\n",
    "      lora_pairs1[key1]['A'] = lora_pairs1[key1]['A'].detach().clone().float() if isinstance(lora_pairs1[key1]['A'], torch.Tensor) else torch.tensor(lora_pairs1[key1]['A'], dtype=torch.float32)\n",
    "      lora_pairs1[key1]['B'] = lora_pairs1[key1]['B'].detach().clone().float() if isinstance(lora_pairs1[key1]['B'], torch.Tensor) else torch.tensor(lora_pairs1[key1]['B'], dtype=torch.float32)\n",
    "      lora_pairs2[key2]['A'] = lora_pairs2[key2]['A'].detach().clone().float() if isinstance(lora_pairs2[key2]['A'], torch.Tensor) else torch.tensor(lora_pairs2[key2]['A'], dtype=torch.float32)\n",
    "      lora_pairs2[key2]['B'] = lora_pairs2[key2]['B'].detach().clone().float() if isinstance(lora_pairs2[key2]['B'], torch.Tensor) else torch.tensor(lora_pairs2[key2]['B'], dtype=torch.float32)\n",
    "      with torch.amp.autocast('cuda'):\n",
    "        matrix1 = lora_pairs1[key1]['B'] @ lora_pairs1[key1]['A']\n",
    "        matrix2 = lora_pairs2[key2]['B'] @ lora_pairs2[key2]['A']\n",
    "      abs_matrix1 = torch.abs(matrix1)\n",
    "      abs_matrix2 = torch.abs(matrix2)\n",
    "      #top-k comparison and select one lora matrix\n",
    "      top_k_values1, _ = torch.topk(abs_matrix1.flatten(), k)\n",
    "      top_k_sum1 = top_k_values1.sum().item()\n",
    "      top_k_values2, _ = torch.topk(abs_matrix2.flatten(), k)\n",
    "      top_k_sum2 = top_k_values2.sum().item()\n",
    "\n",
    "      if top_k_sum1 > top_k_sum2:\n",
    "        print('first lora win')\n",
    "        model_key1 = key1 + '.weight'\n",
    "        merge_lora[model_key1] = lora_pairs1[key1]\n",
    "      else:\n",
    "        model_key2 = key2 + '.weight'\n",
    "        print('second lora win')\n",
    "        merge_lora[model_key2] = lora_pairs2[key2]\n",
    "    else:\n",
    "      print('key1, key2가 같지 않다')\n",
    "  for key, value in model.named_parameters(): #diffusion_model.blocks.0.cross_attn.k.weight\n",
    "    \n",
    "      if key in merge_lora.keys():\n",
    "        print(f'{key}에 klora 적용')\n",
    "        param = reduce(getattr, key.split('.'), model)\n",
    "        with torch.amp.autocast('cuda'):\n",
    "          matrix =  merge_lora[key]['B'] @ merge_lora[key]['A']\n",
    "          param.data += matrix.to('cuda')\n",
    "      else:\n",
    "        print(f'{key}파라미터 안에 lora_merge를 적용할 수 없습니다')\n",
    "        continue\n",
    "  return model\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: av in /usr/local/lib/python3.12/dist-packages (16.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# ETS-Lora\n",
    "!pip install av\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from safetensors.torch import load_file\n",
    "from functools import reduce\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "from torchvision.io import read_video\n",
    "from torchvision.transforms import functional as F\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "\n",
    "# Define paths to the video and reference image\n",
    "video_path = \"/content/video.mp4\"  # Replace with your video path\n",
    "video_path2 =  \"/content/video2.mp4\"\n",
    "\n",
    "try:\n",
    "    video, _, _ = read_video(video_path, pts_unit=\"sec\")\n",
    "    video2, _, _ = read_video(video_path2, pts_unit=\"sec\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Video not found at {video_path}\")\n",
    "    # You might want to handle this error\n",
    "    exit()\n",
    "\n",
    "dinov2_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "dinov2_model = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n",
    "\n",
    "def calculate_dinov2_score(video_frames, video_frames2, processor, model):\n",
    "    # Preprocess video frames and reference image\n",
    "    video_inputs = [processor(images=F.to_pil_image(frame.permute(2,0,1)), return_tensors=\"pt\") for frame in video_frames]\n",
    "    video_inputs2 = [processor(images=F.to_pil_image(frame.permute(2,0,1)), return_tensors=\"pt\") for frame in video_frames2]\n",
    "\n",
    "        # dinov2_scores = []\n",
    "    dinov2_scores = []\n",
    "\n",
    "    for frame_inputs1, frame_inputs2 in zip(video_inputs, video_inputs2):\n",
    "        with torch.no_grad():\n",
    "            out1 = model(**frame_inputs1)\n",
    "            feat1 = out1.last_hidden_state.mean(dim=1)\n",
    "            out2 = model(**frame_inputs2)\n",
    "            feat2 = out2.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        feat1_norm = feat1 / feat1.norm(p=2, dim=-1, keepdim=True)\n",
    "        feat2_norm = feat2 / feat2.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "        score = (feat1_norm * feat2_norm).sum(dim=-1).mean().item()\n",
    "        dinov2_scores.append(score)\n",
    "\n",
    "    dinov2_mean_score = sum(dinov2_scores) / len(dinov2_scores)\n",
    "    print(\"DINOv2 score mean:\", dinov2_mean_score)\n",
    "    return dinov2_scores\n",
    "\n",
    "\n",
    "# dinov2_scores_per_frame = calculate_dinov2_score(video, video2 , dinov2_processor, dinov2_model)\n",
    "\n",
    "# average_dinov2_score = sum(dinov2_scores_per_frame) / len(dinov2_scores_per_frame)\n",
    "\n",
    "!pip install matplotlib -q\n",
    "import re\n",
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import operator\n",
    "glo_c = 0\n",
    "global glo_c \n",
    "def insert_etslora(path1,path2, model, k=1):\n",
    "\n",
    "  lora1 = load_file(path1)\n",
    "  lora1 = sort_state_dict_by_blocknum(lora1)\n",
    "  lora2 = load_file(path2)\n",
    "  lora2 = sort_state_dict_by_blocknum(lora2)\n",
    "  lora1 = {k.replace(\"diffusion_model.\", \"\", 1): v for k, v in lora1.items()} # diffusion_model. 제거\n",
    "  lora2 = {k.replace(\"diffusion_model.\", \"\", 1): v for k, v in lora2.items()}    \n",
    "\n",
    "  lora_pairs1 = {}\n",
    "  lora_pairs2 = {}\n",
    "  for key1,key2 in zip(lora1.keys(),lora2.keys()):\n",
    "\n",
    "      if 'lora_A.weight' in key1:\n",
    "        print(key1)\n",
    "        base_key = key1.replace('.lora_A.weight', '')\n",
    "        lora_b_key = base_key + '.lora_B.weight'\n",
    "        if lora_b_key in lora1:\n",
    "            lora_pairs1[base_key] = {\n",
    "                'A': lora1[key1],\n",
    "                'B': lora1[lora_b_key]\n",
    "            }\n",
    "      else:\n",
    "        print('lora_A 존재 x')\n",
    "      if 'lora_A.weight' in key2:\n",
    "          print(key2)\n",
    "          base_key = key2.replace('.lora_A.weight', '')\n",
    "          lora_b_key = base_key + '.lora_B.weight'\n",
    "          if lora_b_key in lora2:\n",
    "              lora_pairs2[base_key] = {\n",
    "                  'A': lora2[key2],\n",
    "                  'B': lora2[lora_b_key]\n",
    "              }\n",
    "      else:\n",
    "        print('lora_A 존재 x')\n",
    "  assert len(lora_pairs1.keys())==len(lora_pairs2.keys())\n",
    "  merge_lora = {}\n",
    "  for key1, key2 in zip(lora_pairs1.keys(),lora_pairs2.keys()):\n",
    "    if key1 == key2:\n",
    "      lora_pairs1[key1]['A'] = lora_pairs1[key1]['A'].detach().clone().float() if isinstance(lora_pairs1[key1]['A'], torch.Tensor) else torch.tensor(lora_pairs1[key1]['A'], dtype=torch.float32)\n",
    "      lora_pairs1[key1]['B'] = lora_pairs1[key1]['B'].detach().clone().float() if isinstance(lora_pairs1[key1]['B'], torch.Tensor) else torch.tensor(lora_pairs1[key1]['B'], dtype=torch.float32)\n",
    "      lora_pairs2[key2]['A'] = lora_pairs2[key2]['A'].detach().clone().float() if isinstance(lora_pairs2[key2]['A'], torch.Tensor) else torch.tensor(lora_pairs2[key2]['A'], dtype=torch.float32)\n",
    "      lora_pairs2[key2]['B'] = lora_pairs2[key2]['B'].detach().clone().float() if isinstance(lora_pairs2[key2]['B'], torch.Tensor) else torch.tensor(lora_pairs2[key2]['B'], dtype=torch.float32)\n",
    "      with torch.amp.autocast('cuda'):\n",
    "        matrix1 = lora_pairs1[key1]['B'] @ lora_pairs1[key1]['A']\n",
    "        matrix2 = lora_pairs2[key2]['B'] @ lora_pairs2[key2]['A']\n",
    "      matrix1 = torch.norm(matrix1, p='fro')\n",
    "      matrix2 = torch.norm(matrix2, p='fro')\n",
    "\n",
    "\n",
    "      # 이부분 추가하기\n",
    "      timestep_sum = 800\n",
    "      glo_c += 1\n",
    "      alpha = 0.1\n",
    "      gamma = glo_c / timestep_sum * alpha\n",
    "      dino = 0.3\n",
    "      gamma = gamma + (1 - dino)\n",
    "\n",
    "        # apply scaling factor to the sum of top k values\n",
    "      matrix1 = matrix1\n",
    "      matrix2 = matrix2 * gamma #style\n",
    "\n",
    "      if matrix1 > matrix2:\n",
    "        print('first lora win')\n",
    "        key1 += '.weight'\n",
    "        merge_lora[key1] = lora_pairs1[key1]\n",
    "      else:\n",
    "        key2 += '.weight'\n",
    "        print('second lora win')\n",
    "        merge_lora[key2] = lora_pairs2[key2]\n",
    "    else:\n",
    "      print('key1, key2가 같지 않다')\n",
    "  for key, value in model.named_parameters():\n",
    "\n",
    "      if key in merge_lora.keys():\n",
    "        print(f'{key}에 klora 적용')\n",
    "        param = reduce(getattr, key.split('.'), model)\n",
    "        with torch.amp.autocast('cuda'):\n",
    "          matrix =  merge_lora[key]['B'] @ merge_lora[key]['A']\n",
    "          param.data += matrix.to('cuda')\n",
    "      else:\n",
    "        print(f'{key}파라미터 안에 lora_merge를 적용할 수 없습니다')\n",
    "        continue\n",
    "  return model\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# lion lora\n",
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "def insert_lionlora(path1,path2, model, k=1, only_A=False, only_B=False, block = 40,device='cuda'):\n",
    "  lora1 = load_file(path1)\n",
    "  lora1 = sort_state_dict_by_blocknum(lora1)\n",
    "  lora2 = load_file(path2)\n",
    "  lora2 = sort_state_dict_by_blocknum(lora2) #순서 정렬\n",
    "  lora1 = {k.replace(\"diffusion_model.\", \"\", 1): v for k, v in lora1.items()} # diffusion_model. 제거\n",
    "  lora2 = {k.replace(\"diffusion_model.\", \"\", 1): v for k, v in lora2.items()}\n",
    "\n",
    "  lora_pairs1 = {}\n",
    "  lora_pairs2 = {}\n",
    "  timestep = block * 20 # LoRA가 총 800개 - 40개의 block이 - 10개 layer(ffn, attn) - LoRA A,B 행렬\n",
    "  # for key1,key2 in zip(lora1.keys()[:timestep],lora2.keys()[:timestep]):\n",
    "  for key1, key2 in zip(list(lora1.keys())[:timestep], list(lora2.keys())[:timestep]):\n",
    "\n",
    "      if 'lora_A.weight' in key1:\n",
    "        print(key1)\n",
    "        base_key = key1.replace('.lora_A.weight', '')\n",
    "        lora_b_key = base_key + '.lora_B.weight'\n",
    "        if lora_b_key in lora1:\n",
    "            lora_pairs1[base_key] = {\n",
    "                'A': lora1[key1],\n",
    "                'B': lora1[lora_b_key]\n",
    "            }\n",
    "      else:\n",
    "        print('lora_A 존재 x')\n",
    "      if 'lora_A.weight' in key2:\n",
    "          print(key2) #blocks.1.ffn.2.lora_A.weight\n",
    "          base_key = key2.replace('.lora_A.weight', '')\n",
    "          lora_b_key = base_key + '.lora_B.weight'\n",
    "          if lora_b_key in lora2:\n",
    "              lora_pairs2[base_key] = {\n",
    "                  'A': lora2[key2],\n",
    "                  'B': lora2[lora_b_key]\n",
    "              }\n",
    "      else:\n",
    "        print('lora_A 존재 x')\n",
    "\n",
    "  assert len(lora_pairs1.keys())==len(lora_pairs2.keys())\n",
    "  print(f'lora_pairs1 keys:{lora_pairs1.keys()}')\n",
    "  merge_lora = {}\n",
    "  for key1, key2 in zip(lora_pairs1.keys(),lora_pairs2.keys()):\n",
    "\n",
    "    if key1 == key2:\n",
    "      lora_pairs1[key1]['A'] = lora_pairs1[key1]['A'].detach().clone().float() if isinstance(lora_pairs1[key1]['A'], torch.Tensor) else torch.tensor(lora_pairs1[key1]['A'], dtype=torch.float32)\n",
    "      lora_pairs1[key1]['B'] = lora_pairs1[key1]['B'].detach().clone().float() if isinstance(lora_pairs1[key1]['B'], torch.Tensor) else torch.tensor(lora_pairs1[key1]['B'], dtype=torch.float32)\n",
    "      lora_pairs2[key2]['A'] = lora_pairs2[key2]['A'].detach().clone().float() if isinstance(lora_pairs2[key2]['A'], torch.Tensor) else torch.tensor(lora_pairs2[key2]['A'], dtype=torch.float32)\n",
    "      lora_pairs2[key2]['B'] = lora_pairs2[key2]['B'].detach().clone().float() if isinstance(lora_pairs2[key2]['B'], torch.Tensor) else torch.tensor(lora_pairs2[key2]['B'], dtype=torch.float32)\n",
    "\n",
    "      if only_A:\n",
    "        merge_key1 = key1 + '.weight'\n",
    "        merge_lora[merge_key1] = lora_pairs1[key1]\n",
    "      if only_B:\n",
    "        key2 += '.weight'\n",
    "        print('second lora win')\n",
    "        merge_lora[key2] = lora_pairs2[key2]\n",
    "\n",
    "    else:\n",
    "      print('key1, key2가 같지 않다')\n",
    "\n",
    "  if only_A or only_B:\n",
    "    for key, value in model.named_parameters():\n",
    "      if key in merge_lora.keys():\n",
    "        param = reduce(getattr, key.split('.'), model)\n",
    "        print(key)\n",
    "        print(param.data.shape)\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            matrix = merge_lora[key]['B'] @ merge_lora[key]['A']\n",
    "            param.data += matrix.to('cuda')\n",
    "            print(f'{key} lora 추가')\n",
    "      else:\n",
    "        print(f'{key} 파라미터 안에 lora_merge를 적용할 수 없습니다')\n",
    "        continue\n",
    "\n",
    "  else:\n",
    "    for key, value in model.named_parameters():\n",
    "      base = key.rsplit('.', 1)[0] # 마지막 토큰 제거\n",
    "      if key in [k + \".weight\" for k in lora_pairs1.keys()]:\n",
    "          print(f'base_key가 존재:{key}') # blocks.0.self_attn.q.bias도 걸러지네 이게 blocks.0.self_attn.q.weight\n",
    "          param = reduce(getattr, key.split('.'), model)\n",
    "\n",
    "          print(key) #blocks.0.self_attn.q\n",
    "          print(param.data.shape) #torch.Size([5120, 5120])\n",
    "          print(f'base:{base}')\n",
    "          with torch.amp.autocast('cuda'):\n",
    "            matrix1 = lora_pairs1[base]['B'] @ lora_pairs1[base]['A'] #'blocks.0.self_attn'\n",
    "            matrix2 = lora_pairs2[base]['B'] @ lora_pairs2[base]['A']\n",
    "\n",
    "            matrix1_norm = torch.norm(matrix1, p='fro')\n",
    "            matrix2_norm = torch.norm(matrix2, p='fro')\n",
    "\n",
    "            alpha = (matrix1_norm.item() + matrix2_norm.item()) / 2\n",
    "            matrix1_lora_normed = (alpha / matrix1_norm.item()) * matrix1\n",
    "            matrix2_lora_normed = (alpha / matrix2_norm.item()) * matrix2\n",
    "            matrix1_lora_normed = matrix1_lora_normed.to(torch.float32)\n",
    "            matrix2_lora_normed = matrix2_lora_normed.to(torch.float32)            \n",
    "            \n",
    "            param.data += matrix1_lora_normed.to('cuda')\n",
    "            param.data += matrix2_lora_normed.to('cuda')\n",
    "            print(f'{base} lora 추가')\n",
    "  return model\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QC0bkxq3x8Od",
    "outputId": "81c3b59c-f7ee-4a9f-eaff-11e9e14faaf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace')\n",
    "from vae import Wan2_1_VAE\n",
    "from t5 import T5EncoderModel\n",
    "from solver import (\n",
    "    FlowDPMSolverMultistepScheduler,\n",
    "    get_sampling_sigmas,\n",
    "    retrieve_timesteps,\n",
    ")\n",
    "from fm_solvers_unipc import FlowUniPCMultistepScheduler\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ugtEp_C2tTc0",
    "outputId": "96db76a3-007d-440a-bc6d-5ae5d5a7f0d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.\n",
    "from easydict import EasyDict\n",
    "# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.\n",
    "import torch\n",
    "from easydict import EasyDict\n",
    "\n",
    "#------------------------ Wan shared config ------------------------#\n",
    "wan_shared_cfg = EasyDict()\n",
    "\n",
    "# t5\n",
    "wan_shared_cfg.t5_model = 'umt5_xxl'\n",
    "wan_shared_cfg.t5_dtype = torch.bfloat16\n",
    "wan_shared_cfg.text_len = 512\n",
    "\n",
    "# transformer\n",
    "wan_shared_cfg.param_dtype = torch.bfloat16\n",
    "\n",
    "# inference\n",
    "wan_shared_cfg.num_train_timesteps = 1000\n",
    "wan_shared_cfg.sample_fps = 16\n",
    "wan_shared_cfg.sample_hook_neg_prompt = '色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走'\n",
    "wan_shared_cfg.frame_num = 81\n",
    "\n",
    "#------------------------ Wan T2V A14B ------------------------#\n",
    "\n",
    "t2v_A14B = EasyDict(__name__='Config: Wan T2V A14B')\n",
    "t2v_A14B.update(wan_shared_cfg)\n",
    "\n",
    "# t5\n",
    "t2v_A14B.t5_checkpoint = 'models_t5_umt5-xxl-enc-bf16.pth'\n",
    "t2v_A14B.t5_tokenizer = 'google/umt5-xxl'\n",
    "\n",
    "# vae\n",
    "t2v_A14B.vae_checkpoint = 'Wan2.1_VAE.pth'\n",
    "t2v_A14B.vae_stride = (4, 8, 8)\n",
    "\n",
    "# transformer\n",
    "t2v_A14B.patch_size = (1, 2, 2)\n",
    "t2v_A14B.dim = 5120\n",
    "t2v_A14B.ffn_dim = 13824\n",
    "t2v_A14B.freq_dim = 256\n",
    "t2v_A14B.num_heads = 40\n",
    "t2v_A14B.num_layers = 40\n",
    "t2v_A14B.window_size = (-1, -1)\n",
    "t2v_A14B.qk_norm = True\n",
    "t2v_A14B.cross_attn_norm = True\n",
    "t2v_A14B.eps = 1e-6\n",
    "t2v_A14B.low_noise_checkpoint = 'low_noise_model'\n",
    "t2v_A14B.high_noise_checkpoint = 'high_noise_model'\n",
    "\n",
    "# inference\n",
    "t2v_A14B.sample_shift = 12.0\n",
    "t2v_A14B.sample_steps = 40\n",
    "t2v_A14B.boundary = 0.875\n",
    "t2v_A14B.sample_guide_scale = (3.0, 4.0)  # low noise, high noise\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9ul_gduisMNm",
    "outputId": "975e4a54-a0b4-413a-9fd2-b7382136810b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.\n",
    "import gc\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import types\n",
    "from contextlib import contextmanager\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from torch.distributed.fsdp import MixedPrecision, ShardingStrategy\n",
    "import torch.distributed as dist\n",
    "\n",
    "def shard_model(\n",
    "    model,\n",
    "    device_id,\n",
    "    param_dtype=torch.bfloat16,\n",
    "    reduce_dtype=torch.float32,\n",
    "    buffer_dtype=torch.float32,\n",
    "    process_group=None,\n",
    "    sharding_strategy=ShardingStrategy.FULL_SHARD,\n",
    "    sync_module_states=True,\n",
    "    use_lora=False\n",
    "):\n",
    "    model = FSDP(\n",
    "        module=model,\n",
    "        process_group=process_group,\n",
    "        sharding_strategy=sharding_strategy,\n",
    "        auto_wrap_policy=partial(\n",
    "            lambda_auto_wrap_policy, lambda_fn=lambda m: m in model.blocks),\n",
    "        mixed_precision=MixedPrecision(\n",
    "            param_dtype=param_dtype,\n",
    "            reduce_dtype=reduce_dtype,\n",
    "            buffer_dtype=buffer_dtype),\n",
    "        device_id=device_id,\n",
    "        sync_module_states=sync_module_states,\n",
    "        use_orig_params=True if use_lora else False)\n",
    "    return model\n",
    "\n",
    "\n",
    "class WanT2V:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config= t2v_A14B, #위의 easydict\n",
    "        checkpoint_dir = './',#\"./Wan2.2-T2V-A14B\", #./\n",
    "        device_id=0,\n",
    "        rank=0,\n",
    "        t5_fsdp=False,\n",
    "        dit_fsdp=False,\n",
    "        use_sp=False, #모델 나눠서 올릴지 말지\n",
    "        t5_cpu=False,\n",
    "        init_on_cpu=True,\n",
    "        convert_model_dtype=False,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Initializes the Wan text-to-video generation model components.\n",
    "\n",
    "        Args:\n",
    "            config (EasyDict):\n",
    "                Object containing model parameters initialized from config.py\n",
    "            device_id (`int`,  *optional*, defaults to 0):\n",
    "                Id of target GPU device\n",
    "            rank (`int`,  *optional*, defaults to 0):\n",
    "                Process rank for distributed training\n",
    "            t5_fsdp (`bool`, *optional*, defaults to False):\n",
    "                Enable FSDP sharding for T5 model\n",
    "            dit_fsdp (`bool`, *optional*, defaults to False):\n",
    "                Enable FSDP sharding for DiT model\n",
    "            use_sp (`bool`, *optional*, defaults to False):\n",
    "                Enable distribution strategy of sequence parallel.\n",
    "            t5_cpu (`bool`, *optional*, defaults to False):\n",
    "                Whether to place T5 model on CPU. Only works without t5_fsdp.\n",
    "            init_on_cpu (`bool`, *optional*, defaults to True):\n",
    "                Enable initializing Transformer Model on CPU. Only works without FSDP or USP.\n",
    "            convert_model_dtype (`bool`, *optional*, defaults to False):\n",
    "                Convert DiT model parameters dtype to 'config.param_dtype'.\n",
    "                Only works without FSDP.\n",
    "        \"\"\"\n",
    "        self.device = torch.device(f\"cuda:{device_id}\")\n",
    "        self.config = config\n",
    "        self.rank = rank\n",
    "        self.t5_cpu = t5_cpu\n",
    "        self.init_on_cpu = init_on_cpu\n",
    "\n",
    "        self.num_train_timesteps = config.num_train_timesteps\n",
    "        self.boundary = config.boundary\n",
    "        self.param_dtype = config.param_dtype\n",
    "\n",
    "        if t5_fsdp or dit_fsdp or use_sp:\n",
    "            self.init_on_cpu = False\n",
    "        shard_fn = partial(shard_model, device_id=device_id)\n",
    "        # self.text_encoder = T5EncoderModel(\n",
    "        self.vae_stride = config.vae_stride\n",
    "        self.patch_size = config.patch_size\n",
    "        self.vae = Wan2_1_VAE(\n",
    "            vae_pth=os.path.join(checkpoint_dir, config.vae_checkpoint),\n",
    "            device=self.device)\n",
    "        logging.info(f\"Creating WanModel from {checkpoint_dir}\")\n",
    "        self.low_noise_model = WanModel.from_pretrained(\n",
    "            checkpoint_dir, subfolder=config.low_noise_checkpoint, torch_dtype=torch.float16).to('cuda')\n",
    "        print('wan instantiated')\n",
    "        self.high_noise_model = WanModel.from_pretrained(\n",
    "            checkpoint_dir, subfolder=config.high_noise_checkpoint, torch_dtype=torch.float16).to('cuda')\n",
    "\n",
    "        # self.low_noise_model = insert_lora('/workspace/CassHamadaWan2.2LowNoise.safetensors','/workspace/5DFollowcam_Redmond_low_noise.safetensors',self.low_noise_model,only_A=True,block= 40)\n",
    "        # self.low_noise_model = insert_lora('/workspace/CassHamadaWan2.2LowNoise.safetensors','/workspace/5DFollowcam_Redmond_low_noise.safetensors',self.low_noise_model,only_B=True,block= 40)\n",
    "        # self.low_noise_model = insert_lora('/workspace/CassHamadaWan2.2LowNoise.safetensors','/workspace/5DFollowcam_Redmond_low_noise.safetensors',self.low_noise_model,only_A=False,block= 40)\n",
    "        # self.low_noise_model = insert_klora('/workspace/CassHamadaWan2.2LowNoise.safetensors','/workspace/5DFollowcam_Redmond_low_noise.safetensors',self.low_noise_model)\n",
    "        # self.low_noise_model = insert_lionlora('/workspace/CassHamadaWan2.2LowNoise.safetensors','/workspace/5DFollowcam_Redmond_low_noise.safetensors',self.low_noise_model)\n",
    "        # self.low_noise_model = insert_estlora('/workspace/CassHamadaWan2.2LowNoise.safetensors','/workspace/5DFollowcam_Redmond_low_noise.safetensors',self.low_noise_model)\n",
    "\n",
    "        #golden 징크스\n",
    "        self.low_noise_model = insert_lora('/workspace/985347-wan22_14B-low-Nfj1nx-e65.safetensors','/workspace/goldenboylora-22-LOW-V2-e106.safetensors',self.low_noise_model,only_A=False,block= 40)\n",
    "        self.high_noise_model = insert_lora('/workspace/985347-wan22_14B-high-Nfj1nx-e71.safetensors','/workspace/goldenboylora-22-HIGH-e52.safetensors',self.low_noise_model,only_A=False,block= 40)\n",
    "        # self.low_noise_model = insert_klora('/workspace/985347-wan22_14B-low-Nfj1nx-e65.safetensors','/workspace/goldenboylora-22-LOW-V2-e106.safetensors',self.low_noise_model) #k=3\n",
    "\n",
    "        #kawajiri 아시아 여자\n",
    "        # self.low_noise_model = insert_lora('/workspace/wan22-x1nyu3-20epoc-low-k3nk.safetensors','/workspace/yoshiaki-kawajiri-low-v3_e92.safetensors',self.low_noise_model,only_A=False,block= 40)\n",
    "        # self.low_noise_model = insert_klora('/workspace/wan22-x1nyu3-20epoc-low-k3nk.safetensors','/workspace/yoshiaki-kawajiri-low-v3_e92.safetensors',self.low_noise_model) #k=3\n",
    "        \n",
    "        #산타 pure_color\n",
    "        # self.low_noise_model = insert_lora('/workspace/purecoloranimestyle.safetensors','/workspace/[WAN2.2]Offseasonsanta_Redmond_low_noise.safetensors',self.low_noise_model,only_A=False,block= 40)\n",
    "        # self.low_noise_model = insert_klora('/workspace/purecoloranimestyle.safetensors','/workspace/[WAN2.2]Offseasonsanta_Redmond_low_noise.safetensors',self.low_noise_model) #k=3\n",
    "\n",
    "        #CASS HAMADA ghibli\n",
    "        # self.low_noise_model = insert_lora('/workspace/DarkGhibliWan2.2Low_low_noise.safetensors','/workspace/CassHamadaWan2.2LowNoise.safetensors',self.low_noise_model,only_A=False,block= 40)\n",
    "        # self.low_noise_model = insert_klora('/workspace/DarkGhibliWan2.2Low_low_noise.safetensors','/workspace/CassHamadaWan2.2LowNoise.safetensors',self.low_noise_model) #k=3\n",
    "        \n",
    "        self.low_noise_model = self._configure_model(model=self.low_noise_model,\n",
    "            #use_sp=use_sp, dit_fsdp=dit_fsdp, shard_fn=shard_fn 전부다 false임\n",
    "            convert_model_dtype=convert_model_dtype)\n",
    "        self.high_noise_model = self._configure_model(model=self.high_noise_model,\n",
    "            # use_sp=use_sp,dit_fsdp=dit_fsdp,shard_fn=shard_fn,\n",
    "            convert_model_dtype=convert_model_dtype)        \n",
    "\n",
    "        \n",
    "        # self.low_noise_model = self._configure_model(\n",
    "        #     model=self.low_noise_model,\n",
    "        #     use_sp=use_sp,\n",
    "        #     dit_fsdp=dit_fsdp,\n",
    "        #     shard_fn=shard_fn,\n",
    "        #     convert_model_dtype=convert_model_dtype)\n",
    "\n",
    "        if use_sp:\n",
    "            self.sp_size = get_world_size()\n",
    "        else:\n",
    "            self.sp_size = 1\n",
    "\n",
    "\n",
    "        # self.sample_neg_prompt = config.sample_neg_prompt\n",
    "\n",
    "    def _configure_model(self, model,convert_model_dtype, use_sp=False, dit_fsdp=False, #shard_fn,\n",
    "                         ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            use_sp (`bool`):\n",
    "                Enable distribution strategy of sequence parallel.\n",
    "            dit_fsdp (`bool`):\n",
    "                Enable FSDP sharding for DiT model.\n",
    "            shard_fn (callable):\n",
    "                The function to apply FSDP sharding.\n",
    "            convert_model_dtype (`bool`):\n",
    "                Convert DiT model parameters dtype to 'config.param_dtype'.\n",
    "                Only works without FSDP.\n",
    "        \"\"\"\n",
    "        model.eval().requires_grad_(False)\n",
    "\n",
    "        # if use_sp:\n",
    "        #     for block in model.blocks:\n",
    "        #         block.self_attn.forward = types.MethodType(\n",
    "        #             sp_attn_forward, block.self_attn)\n",
    "        #     model.forward = types.MethodType(sp_dit_forward, model)\n",
    "\n",
    "        # if dist.is_initialized():\n",
    "        #     dist.barrier()\n",
    "\n",
    "        if dit_fsdp:\n",
    "            model = shard_fn(model)\n",
    "        else:\n",
    "            if convert_model_dtype:\n",
    "                model.to(self.param_dtype)\n",
    "            if not self.init_on_cpu:\n",
    "                model.to(self.device)\n",
    "        print('model configured')\n",
    "        return model\n",
    "\n",
    "    def _prepare_model_for_timestep(self, t, boundary, offload_model):\n",
    "        r\"\"\"\n",
    "        Prepares and returns the required model for the current timestep.\n",
    "\n",
    "        Args:\n",
    "            t (torch.Tensor):\n",
    "                current timestep.\n",
    "            boundary (`int`):\n",
    "                The timestep threshold. If `t` is at or above this value,\n",
    "                the `high_noise_model` is considered as the required model.\n",
    "            offload_model (`bool`):\n",
    "                A flag intended to control the offloading behavior.\n",
    "\n",
    "        Returns:\n",
    "            torch.nn.Module:\n",
    "                The active model on the target device for the current timestep.\n",
    "        \"\"\"\n",
    "        if t.item() >= boundary:\n",
    "            required_model_name = 'high_noise_model'\n",
    "            offload_model_name = 'low_noise_model'\n",
    "        else:\n",
    "            required_model_name = 'low_noise_model'\n",
    "            offload_model_name = 'high_noise_model'\n",
    "        if offload_model or self.init_on_cpu:\n",
    "            if next(getattr(\n",
    "                    self,\n",
    "                    offload_model_name).parameters()).device.type == 'cuda':\n",
    "                getattr(self, offload_model_name).to('cpu')\n",
    "            if next(getattr(\n",
    "                    self,\n",
    "                    required_model_name).parameters()).device.type == 'cpu':\n",
    "                getattr(self, required_model_name).to(self.device)\n",
    "        return getattr(self, required_model_name)\n",
    "\n",
    "    def generate(self,\n",
    "                 input_prompt,\n",
    "                 size=(1280, 720),\n",
    "                 frame_num=20,#81\n",
    "                 shift=5.0,\n",
    "                 sample_solver='unipc',\n",
    "                 sampling_steps=50,\n",
    "                 guide_scale=5.0,\n",
    "                 n_prompt=\"\",\n",
    "                 seed=-1,\n",
    "                 offload_model=True,\n",
    "                 context=0, # 추가한것\n",
    "                 context_null=0):\n",
    "        r\"\"\"\n",
    "        Generates video frames from text prompt using diffusion process.\n",
    "\n",
    "        Args:\n",
    "            input_prompt (`str`):\n",
    "                Text prompt for content generation\n",
    "            size (`tuple[int]`, *optional*, defaults to (1280,720)):\n",
    "                Controls video resolution, (width,height).\n",
    "            frame_num (`int`, *optional*, defaults to 81):\n",
    "                How many frames to sample from a video. The number should be 4n+1\n",
    "            shift (`float`, *optional*, defaults to 5.0):\n",
    "                Noise schedule shift parameter. Affects temporal dynamics\n",
    "            sample_solver (`str`, *optional*, defaults to 'unipc'):\n",
    "                Solver used to sample the video.\n",
    "            sampling_steps (`int`, *optional*, defaults to 50):\n",
    "                Number of diffusion sampling steps. Higher values improve quality but slow generation\n",
    "            guide_scale (`float` or tuple[`float`], *optional*, defaults 5.0):\n",
    "                Classifier-free guidance scale. Controls prompt adherence vs. creativity.\n",
    "                If tuple, the first guide_scale will be used for low noise model and\n",
    "                the second guide_scale will be used for high noise model.\n",
    "            n_prompt (`str`, *optional*, defaults to \"\"):\n",
    "                Negative prompt for content exclusion. If not given, use `config.sample_neg_prompt`\n",
    "            seed (`int`, *optional*, defaults to -1):\n",
    "                Random seed for noise generation. If -1, use random seed.\n",
    "            offload_model (`bool`, *optional*, defaults to True):\n",
    "                If True, offloads models to CPU during generation to save VRAM\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor:\n",
    "                Generated video frames tensor. Dimensions: (C, N H, W) where:\n",
    "                - C: Color channels (3 for RGB)\n",
    "                - N: Number of frames (81)\n",
    "                - H: Frame height (from size)\n",
    "                - W: Frame width from size)\n",
    "        \"\"\"\n",
    "        # preprocess\n",
    "        guide_scale = (guide_scale, guide_scale) if isinstance(\n",
    "            guide_scale, float) else guide_scale\n",
    "        F = frame_num\n",
    "        target_shape = (self.vae.model.z_dim, (F - 1) // self.vae_stride[0] + 1,\n",
    "                        size[1] // self.vae_stride[1],\n",
    "                        size[0] // self.vae_stride[2])\n",
    "\n",
    "        seq_len = math.ceil((target_shape[2] * target_shape[3]) /\n",
    "                            (self.patch_size[1] * self.patch_size[2]) *\n",
    "                            target_shape[1] / self.sp_size) * self.sp_size\n",
    "\n",
    "        # if n_prompt == \"\":\n",
    "        #     n_prompt = self.sample_neg_prompt\n",
    "        seed = seed if seed >= 0 else random.randint(0, sys.maxsize)\n",
    "        seed_g = torch.Generator(device=self.device)\n",
    "        seed_g.manual_seed(seed)\n",
    "\n",
    "        # if not self.t5_cpu:\n",
    "        #     self.text_encoder.model.to(self.device)\n",
    "        #     context = self.text_encoder([input_prompt], self.device)\n",
    "        #     context_null = self.text_encoder([n_prompt], self.device)\n",
    "        #     if offload_model:\n",
    "        #         self.text_encoder.model.cpu()\n",
    "        # else:\n",
    "        #     context = self.text_encoder([input_prompt], torch.device('cpu'))\n",
    "        #     context_null = self.text_encoder([n_prompt], torch.device('cpu'))\n",
    "        context = [t.to(self.device) for t in input_prompt]\n",
    "        context_null = [t.to(self.device) for t in n_prompt]\n",
    "\n",
    "        noise = [\n",
    "            torch.randn(\n",
    "                target_shape[0], #dim\n",
    "                target_shape[1], #frame num 3개다 VAE의 3D conv 이후의 shape\n",
    "                target_shape[2], #latent height\n",
    "                target_shape[3], #latent width\n",
    "                dtype=torch.float32,\n",
    "                device=self.device,\n",
    "                generator=seed_g)\n",
    "        ]\n",
    "\n",
    "        @contextmanager\n",
    "        def noop_no_sync():\n",
    "            yield\n",
    "        no_sync_low_noise = getattr(self.low_noise_model, 'no_sync',\n",
    "                                    noop_no_sync)\n",
    "        no_sync_high_noise = getattr(self.high_noise_model, 'no_sync',\n",
    "                                     noop_no_sync)            \n",
    "        # evaluation mode\n",
    "        with (\n",
    "                torch.amp.autocast('cuda', dtype=self.param_dtype),\n",
    "                torch.no_grad(),\n",
    "                no_sync_low_noise(),\n",
    "                no_sync_high_noise(),\n",
    "        ):\n",
    "            boundary = self.boundary * self.num_train_timesteps\n",
    "\n",
    "            if sample_solver == 'unipc':\n",
    "                sample_scheduler = FlowUniPCMultistepScheduler(\n",
    "                    num_train_timesteps=self.num_train_timesteps,\n",
    "                    shift=1,\n",
    "                    use_dynamic_shifting=False)\n",
    "                sample_scheduler.set_timesteps(\n",
    "                    sampling_steps, device=self.device, shift=shift)\n",
    "                timesteps = sample_scheduler.timesteps\n",
    "                # timesteps = sigmas * self.config.num_train_timesteps\n",
    "            #dpm solver 생략\n",
    "            else:\n",
    "                raise NotImplementedError(\"Unsupported solver.\")\n",
    "\n",
    "            # sample videos\n",
    "            latents = noise\n",
    "\n",
    "            arg_c = {'context': context, 'seq_len': seq_len}\n",
    "            arg_null = {'context': context_null, 'seq_len': seq_len}\n",
    "\n",
    "            for _, t in enumerate(tqdm(timesteps)):\n",
    "                latent_model_input = latents\n",
    "                timestep = [t]\n",
    "\n",
    "                timestep = torch.stack(timestep)\n",
    "\n",
    "                model = self._prepare_model_for_timestep(\n",
    "                    t, boundary, offload_model)\n",
    "                #----------------------------------\n",
    "                # for name, layer in model.named_modules():\n",
    "                #     if isinstance(layer, WanSelfAttention):\n",
    "                #         layer.register_forward_hook(save_qkv_hook(name))\n",
    "                #----------------------------------\n",
    "                sample_guide_scale = guide_scale[1] if t.item(\n",
    "                ) >= boundary else guide_scale[0]\n",
    "\n",
    "                noise_pred_cond = model(\n",
    "                    latent_model_input, t=timestep, **arg_c)[0]\n",
    "                noise_pred_uncond = model(\n",
    "                    latent_model_input, t=timestep, **arg_null)[0]\n",
    "\n",
    "                noise_pred = noise_pred_uncond + sample_guide_scale * (\n",
    "                    noise_pred_cond - noise_pred_uncond)\n",
    "\n",
    "                temp_x0 = sample_scheduler.step(\n",
    "                    noise_pred.unsqueeze(0),\n",
    "                    t,\n",
    "                    latents[0].unsqueeze(0),\n",
    "                    return_dict=False,\n",
    "                    generator=seed_g)[0]\n",
    "                latents = [temp_x0.squeeze(0)]\n",
    "\n",
    "            x0 = latents\n",
    "            if offload_model:\n",
    "                self.low_noise_model.cpu()\n",
    "                self.high_noise_model.cpu()\n",
    "                torch.cuda.empty_cache()\n",
    "            if self.rank == 0:\n",
    "                videos = self.vae.decode(x0)\n",
    "\n",
    "        del noise, latents\n",
    "        del sample_scheduler\n",
    "        if offload_model:\n",
    "            gc.collect()\n",
    "            torch.cuda.synchronize()\n",
    "        if dist.is_initialized():\n",
    "            dist.barrier()\n",
    "\n",
    "        return videos[0] if self.rank == 0 else None\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "U7zRca5ge4Nc"
   },
   "outputs": [],
   "source": [
    "# context = torch.load(\"/workspace/a_polaroid_photo_of_wanemma.pt\")\n",
    "# context = torch.load(\"/workspace/pure_color_santa.pt\")\n",
    "# context = torch.load(\"/workspace/FollowCam_Cass_Hamada.pt\")\n",
    "context = torch.load(\"/workspace/golden jinx.pt\")\n",
    "context_null = context = torch.load(\"/workspace/null.pt\")\n",
    "\n",
    "\n",
    "SIZE_CONFIGS = {\n",
    "    '720*1280': (720, 1280),\n",
    "    '1280*720': (1280, 720),\n",
    "    '480*832': (480, 832),\n",
    "    '832*480': (832, 480),\n",
    "    '704*1280': (704, 1280),\n",
    "    '1280*704': (1280, 704),\n",
    "    '1024*704': (1024, 704),\n",
    "    '704*1024': (704, 1024),\n",
    "}\n",
    "logging.info(f\"Generating video ...\")\n",
    "from easydict import EasyDict as edict\n",
    "def _get_args():\n",
    "    args = edict()\n",
    "\n",
    "    args.task = \"t2v-A14B\"                    # choices: list(WAN_CONFIGS.keys())\n",
    "    args.size = \"1280*720\"                    # choices: list(SIZE_CONFIGS.keys())\n",
    "    args.frame_num = None                     # should be 4n+1\n",
    "    args.ckpt_dir = None\n",
    "    args.offload_model = None\n",
    "    args.ulysses_size = 1\n",
    "    args.t5_fsdp = False\n",
    "    args.t5_cpu = False\n",
    "    args.dit_fsdp = False\n",
    "    args.save_file = None\n",
    "    args.prompt = None\n",
    "    args.use_prompt_extend = False\n",
    "    args.prompt_extend_method = \"local_qwen\"  # [\"dashscope\", \"local_qwen\"]\n",
    "    args.prompt_extend_model = None\n",
    "    args.prompt_extend_target_lang = \"zh\"     # [\"zh\", \"en\"]\n",
    "    args.base_seed = -1\n",
    "    args.image = None\n",
    "    args.sample_solver = \"unipc\"              # [\"unipc\", \"dpm++\"]\n",
    "    args.sample_steps = None\n",
    "    args.sample_shift = None\n",
    "    args.sample_guide_scale = None\n",
    "    args.convert_model_dtype = False\n",
    "\n",
    "    # animate\n",
    "    args.src_root_path = None\n",
    "    args.refert_num = 77\n",
    "    args.replace_flag = False\n",
    "    args.use_relighting_lora = False\n",
    "    return args\n",
    "args = _get_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hnOR_j8n8Ti1",
    "outputId": "6553540c-c993-4426-e146-fa8062c6a0dd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
      "done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f4e851dcf44a0daf51c939bc91da03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wan instantiated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "195638c39641438f9ee5ab3ec676f949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.cross_attn.k.lora_A.weight\n",
      "blocks.0.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.0.cross_attn.o.lora_A.weight\n",
      "blocks.0.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.0.cross_attn.q.lora_A.weight\n",
      "blocks.0.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.0.cross_attn.v.lora_A.weight\n",
      "blocks.0.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.0.ffn.0.lora_A.weight\n",
      "blocks.0.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.0.ffn.2.lora_A.weight\n",
      "blocks.0.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.0.self_attn.k.lora_A.weight\n",
      "blocks.0.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.0.self_attn.o.lora_A.weight\n",
      "blocks.0.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.0.self_attn.q.lora_A.weight\n",
      "blocks.0.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.0.self_attn.v.lora_A.weight\n",
      "blocks.0.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.1.cross_attn.k.lora_A.weight\n",
      "blocks.1.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.1.cross_attn.o.lora_A.weight\n",
      "blocks.1.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.1.cross_attn.q.lora_A.weight\n",
      "blocks.1.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.1.cross_attn.v.lora_A.weight\n",
      "blocks.1.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.1.ffn.0.lora_A.weight\n",
      "blocks.1.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.1.ffn.2.lora_A.weight\n",
      "blocks.1.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.1.self_attn.k.lora_A.weight\n",
      "blocks.1.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.1.self_attn.o.lora_A.weight\n",
      "blocks.1.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.1.self_attn.q.lora_A.weight\n",
      "blocks.1.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.1.self_attn.v.lora_A.weight\n",
      "blocks.1.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.2.cross_attn.k.lora_A.weight\n",
      "blocks.2.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.2.cross_attn.o.lora_A.weight\n",
      "blocks.2.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.2.cross_attn.q.lora_A.weight\n",
      "blocks.2.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.2.cross_attn.v.lora_A.weight\n",
      "blocks.2.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.2.ffn.0.lora_A.weight\n",
      "blocks.2.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.2.ffn.2.lora_A.weight\n",
      "blocks.2.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.2.self_attn.k.lora_A.weight\n",
      "blocks.2.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.2.self_attn.o.lora_A.weight\n",
      "blocks.2.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.2.self_attn.q.lora_A.weight\n",
      "blocks.2.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.2.self_attn.v.lora_A.weight\n",
      "blocks.2.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.3.cross_attn.k.lora_A.weight\n",
      "blocks.3.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.3.cross_attn.o.lora_A.weight\n",
      "blocks.3.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.3.cross_attn.q.lora_A.weight\n",
      "blocks.3.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.3.cross_attn.v.lora_A.weight\n",
      "blocks.3.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.3.ffn.0.lora_A.weight\n",
      "blocks.3.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.3.ffn.2.lora_A.weight\n",
      "blocks.3.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.3.self_attn.k.lora_A.weight\n",
      "blocks.3.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.3.self_attn.o.lora_A.weight\n",
      "blocks.3.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.3.self_attn.q.lora_A.weight\n",
      "blocks.3.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.3.self_attn.v.lora_A.weight\n",
      "blocks.3.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.4.cross_attn.k.lora_A.weight\n",
      "blocks.4.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.4.cross_attn.o.lora_A.weight\n",
      "blocks.4.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.4.cross_attn.q.lora_A.weight\n",
      "blocks.4.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.4.cross_attn.v.lora_A.weight\n",
      "blocks.4.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.4.ffn.0.lora_A.weight\n",
      "blocks.4.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.4.ffn.2.lora_A.weight\n",
      "blocks.4.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.4.self_attn.k.lora_A.weight\n",
      "blocks.4.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.4.self_attn.o.lora_A.weight\n",
      "blocks.4.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.4.self_attn.q.lora_A.weight\n",
      "blocks.4.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.4.self_attn.v.lora_A.weight\n",
      "blocks.4.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.5.cross_attn.k.lora_A.weight\n",
      "blocks.5.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.5.cross_attn.o.lora_A.weight\n",
      "blocks.5.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.5.cross_attn.q.lora_A.weight\n",
      "blocks.5.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.5.cross_attn.v.lora_A.weight\n",
      "blocks.5.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.5.ffn.0.lora_A.weight\n",
      "blocks.5.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.5.ffn.2.lora_A.weight\n",
      "blocks.5.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.5.self_attn.k.lora_A.weight\n",
      "blocks.5.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.5.self_attn.o.lora_A.weight\n",
      "blocks.5.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.5.self_attn.q.lora_A.weight\n",
      "blocks.5.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.5.self_attn.v.lora_A.weight\n",
      "blocks.5.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.6.cross_attn.k.lora_A.weight\n",
      "blocks.6.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.6.cross_attn.o.lora_A.weight\n",
      "blocks.6.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.6.cross_attn.q.lora_A.weight\n",
      "blocks.6.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.6.cross_attn.v.lora_A.weight\n",
      "blocks.6.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.6.ffn.0.lora_A.weight\n",
      "blocks.6.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.6.ffn.2.lora_A.weight\n",
      "blocks.6.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.6.self_attn.k.lora_A.weight\n",
      "blocks.6.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.6.self_attn.o.lora_A.weight\n",
      "blocks.6.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.6.self_attn.q.lora_A.weight\n",
      "blocks.6.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.6.self_attn.v.lora_A.weight\n",
      "blocks.6.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.7.cross_attn.k.lora_A.weight\n",
      "blocks.7.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.7.cross_attn.o.lora_A.weight\n",
      "blocks.7.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.7.cross_attn.q.lora_A.weight\n",
      "blocks.7.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.7.cross_attn.v.lora_A.weight\n",
      "blocks.7.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.7.ffn.0.lora_A.weight\n",
      "blocks.7.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.7.ffn.2.lora_A.weight\n",
      "blocks.7.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.7.self_attn.k.lora_A.weight\n",
      "blocks.7.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.7.self_attn.o.lora_A.weight\n",
      "blocks.7.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.7.self_attn.q.lora_A.weight\n",
      "blocks.7.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.7.self_attn.v.lora_A.weight\n",
      "blocks.7.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.8.cross_attn.k.lora_A.weight\n",
      "blocks.8.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.8.cross_attn.o.lora_A.weight\n",
      "blocks.8.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.8.cross_attn.q.lora_A.weight\n",
      "blocks.8.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.8.cross_attn.v.lora_A.weight\n",
      "blocks.8.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.8.ffn.0.lora_A.weight\n",
      "blocks.8.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.8.ffn.2.lora_A.weight\n",
      "blocks.8.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.8.self_attn.k.lora_A.weight\n",
      "blocks.8.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.8.self_attn.o.lora_A.weight\n",
      "blocks.8.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.8.self_attn.q.lora_A.weight\n",
      "blocks.8.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.8.self_attn.v.lora_A.weight\n",
      "blocks.8.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.9.cross_attn.k.lora_A.weight\n",
      "blocks.9.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.9.cross_attn.o.lora_A.weight\n",
      "blocks.9.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.9.cross_attn.q.lora_A.weight\n",
      "blocks.9.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.9.cross_attn.v.lora_A.weight\n",
      "blocks.9.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.9.ffn.0.lora_A.weight\n",
      "blocks.9.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.9.ffn.2.lora_A.weight\n",
      "blocks.9.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.9.self_attn.k.lora_A.weight\n",
      "blocks.9.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.9.self_attn.o.lora_A.weight\n",
      "blocks.9.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.9.self_attn.q.lora_A.weight\n",
      "blocks.9.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.9.self_attn.v.lora_A.weight\n",
      "blocks.9.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.10.cross_attn.k.lora_A.weight\n",
      "blocks.10.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.10.cross_attn.o.lora_A.weight\n",
      "blocks.10.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.10.cross_attn.q.lora_A.weight\n",
      "blocks.10.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.10.cross_attn.v.lora_A.weight\n",
      "blocks.10.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.10.ffn.0.lora_A.weight\n",
      "blocks.10.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.10.ffn.2.lora_A.weight\n",
      "blocks.10.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.10.self_attn.k.lora_A.weight\n",
      "blocks.10.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.10.self_attn.o.lora_A.weight\n",
      "blocks.10.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.10.self_attn.q.lora_A.weight\n",
      "blocks.10.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.10.self_attn.v.lora_A.weight\n",
      "blocks.10.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.11.cross_attn.k.lora_A.weight\n",
      "blocks.11.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.11.cross_attn.o.lora_A.weight\n",
      "blocks.11.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.11.cross_attn.q.lora_A.weight\n",
      "blocks.11.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.11.cross_attn.v.lora_A.weight\n",
      "blocks.11.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.11.ffn.0.lora_A.weight\n",
      "blocks.11.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.11.ffn.2.lora_A.weight\n",
      "blocks.11.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.11.self_attn.k.lora_A.weight\n",
      "blocks.11.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.11.self_attn.o.lora_A.weight\n",
      "blocks.11.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.11.self_attn.q.lora_A.weight\n",
      "blocks.11.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.11.self_attn.v.lora_A.weight\n",
      "blocks.11.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.12.cross_attn.k.lora_A.weight\n",
      "blocks.12.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.12.cross_attn.o.lora_A.weight\n",
      "blocks.12.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.12.cross_attn.q.lora_A.weight\n",
      "blocks.12.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.12.cross_attn.v.lora_A.weight\n",
      "blocks.12.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.12.ffn.0.lora_A.weight\n",
      "blocks.12.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.12.ffn.2.lora_A.weight\n",
      "blocks.12.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.12.self_attn.k.lora_A.weight\n",
      "blocks.12.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.12.self_attn.o.lora_A.weight\n",
      "blocks.12.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.12.self_attn.q.lora_A.weight\n",
      "blocks.12.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.12.self_attn.v.lora_A.weight\n",
      "blocks.12.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.13.cross_attn.k.lora_A.weight\n",
      "blocks.13.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.13.cross_attn.o.lora_A.weight\n",
      "blocks.13.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.13.cross_attn.q.lora_A.weight\n",
      "blocks.13.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.13.cross_attn.v.lora_A.weight\n",
      "blocks.13.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.13.ffn.0.lora_A.weight\n",
      "blocks.13.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.13.ffn.2.lora_A.weight\n",
      "blocks.13.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.13.self_attn.k.lora_A.weight\n",
      "blocks.13.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.13.self_attn.o.lora_A.weight\n",
      "blocks.13.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.13.self_attn.q.lora_A.weight\n",
      "blocks.13.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.13.self_attn.v.lora_A.weight\n",
      "blocks.13.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.14.cross_attn.k.lora_A.weight\n",
      "blocks.14.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.14.cross_attn.o.lora_A.weight\n",
      "blocks.14.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.14.cross_attn.q.lora_A.weight\n",
      "blocks.14.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.14.cross_attn.v.lora_A.weight\n",
      "blocks.14.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.14.ffn.0.lora_A.weight\n",
      "blocks.14.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.14.ffn.2.lora_A.weight\n",
      "blocks.14.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.14.self_attn.k.lora_A.weight\n",
      "blocks.14.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.14.self_attn.o.lora_A.weight\n",
      "blocks.14.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.14.self_attn.q.lora_A.weight\n",
      "blocks.14.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.14.self_attn.v.lora_A.weight\n",
      "blocks.14.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.15.cross_attn.k.lora_A.weight\n",
      "blocks.15.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.15.cross_attn.o.lora_A.weight\n",
      "blocks.15.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.15.cross_attn.q.lora_A.weight\n",
      "blocks.15.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.15.cross_attn.v.lora_A.weight\n",
      "blocks.15.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.15.ffn.0.lora_A.weight\n",
      "blocks.15.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.15.ffn.2.lora_A.weight\n",
      "blocks.15.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.15.self_attn.k.lora_A.weight\n",
      "blocks.15.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.15.self_attn.o.lora_A.weight\n",
      "blocks.15.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.15.self_attn.q.lora_A.weight\n",
      "blocks.15.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.15.self_attn.v.lora_A.weight\n",
      "blocks.15.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.16.cross_attn.k.lora_A.weight\n",
      "blocks.16.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.16.cross_attn.o.lora_A.weight\n",
      "blocks.16.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.16.cross_attn.q.lora_A.weight\n",
      "blocks.16.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.16.cross_attn.v.lora_A.weight\n",
      "blocks.16.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.16.ffn.0.lora_A.weight\n",
      "blocks.16.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.16.ffn.2.lora_A.weight\n",
      "blocks.16.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.16.self_attn.k.lora_A.weight\n",
      "blocks.16.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.16.self_attn.o.lora_A.weight\n",
      "blocks.16.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.16.self_attn.q.lora_A.weight\n",
      "blocks.16.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.16.self_attn.v.lora_A.weight\n",
      "blocks.16.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.17.cross_attn.k.lora_A.weight\n",
      "blocks.17.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.17.cross_attn.o.lora_A.weight\n",
      "blocks.17.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.17.cross_attn.q.lora_A.weight\n",
      "blocks.17.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.17.cross_attn.v.lora_A.weight\n",
      "blocks.17.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.17.ffn.0.lora_A.weight\n",
      "blocks.17.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.17.ffn.2.lora_A.weight\n",
      "blocks.17.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.17.self_attn.k.lora_A.weight\n",
      "blocks.17.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.17.self_attn.o.lora_A.weight\n",
      "blocks.17.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.17.self_attn.q.lora_A.weight\n",
      "blocks.17.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.17.self_attn.v.lora_A.weight\n",
      "blocks.17.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.18.cross_attn.k.lora_A.weight\n",
      "blocks.18.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.18.cross_attn.o.lora_A.weight\n",
      "blocks.18.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.18.cross_attn.q.lora_A.weight\n",
      "blocks.18.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.18.cross_attn.v.lora_A.weight\n",
      "blocks.18.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.18.ffn.0.lora_A.weight\n",
      "blocks.18.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.18.ffn.2.lora_A.weight\n",
      "blocks.18.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.18.self_attn.k.lora_A.weight\n",
      "blocks.18.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.18.self_attn.o.lora_A.weight\n",
      "blocks.18.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.18.self_attn.q.lora_A.weight\n",
      "blocks.18.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.18.self_attn.v.lora_A.weight\n",
      "blocks.18.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.19.cross_attn.k.lora_A.weight\n",
      "blocks.19.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.19.cross_attn.o.lora_A.weight\n",
      "blocks.19.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.19.cross_attn.q.lora_A.weight\n",
      "blocks.19.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.19.cross_attn.v.lora_A.weight\n",
      "blocks.19.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.19.ffn.0.lora_A.weight\n",
      "blocks.19.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.19.ffn.2.lora_A.weight\n",
      "blocks.19.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.19.self_attn.k.lora_A.weight\n",
      "blocks.19.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.19.self_attn.o.lora_A.weight\n",
      "blocks.19.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.19.self_attn.q.lora_A.weight\n",
      "blocks.19.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.19.self_attn.v.lora_A.weight\n",
      "blocks.19.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.20.cross_attn.k.lora_A.weight\n",
      "blocks.20.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.20.cross_attn.o.lora_A.weight\n",
      "blocks.20.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.20.cross_attn.q.lora_A.weight\n",
      "blocks.20.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.20.cross_attn.v.lora_A.weight\n",
      "blocks.20.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.20.ffn.0.lora_A.weight\n",
      "blocks.20.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.20.ffn.2.lora_A.weight\n",
      "blocks.20.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.20.self_attn.k.lora_A.weight\n",
      "blocks.20.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.20.self_attn.o.lora_A.weight\n",
      "blocks.20.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.20.self_attn.q.lora_A.weight\n",
      "blocks.20.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.20.self_attn.v.lora_A.weight\n",
      "blocks.20.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.21.cross_attn.k.lora_A.weight\n",
      "blocks.21.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.21.cross_attn.o.lora_A.weight\n",
      "blocks.21.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.21.cross_attn.q.lora_A.weight\n",
      "blocks.21.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.21.cross_attn.v.lora_A.weight\n",
      "blocks.21.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.21.ffn.0.lora_A.weight\n",
      "blocks.21.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.21.ffn.2.lora_A.weight\n",
      "blocks.21.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.21.self_attn.k.lora_A.weight\n",
      "blocks.21.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.21.self_attn.o.lora_A.weight\n",
      "blocks.21.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.21.self_attn.q.lora_A.weight\n",
      "blocks.21.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.21.self_attn.v.lora_A.weight\n",
      "blocks.21.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.22.cross_attn.k.lora_A.weight\n",
      "blocks.22.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.22.cross_attn.o.lora_A.weight\n",
      "blocks.22.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.22.cross_attn.q.lora_A.weight\n",
      "blocks.22.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.22.cross_attn.v.lora_A.weight\n",
      "blocks.22.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.22.ffn.0.lora_A.weight\n",
      "blocks.22.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.22.ffn.2.lora_A.weight\n",
      "blocks.22.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.22.self_attn.k.lora_A.weight\n",
      "blocks.22.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.22.self_attn.o.lora_A.weight\n",
      "blocks.22.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.22.self_attn.q.lora_A.weight\n",
      "blocks.22.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.22.self_attn.v.lora_A.weight\n",
      "blocks.22.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.23.cross_attn.k.lora_A.weight\n",
      "blocks.23.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.23.cross_attn.o.lora_A.weight\n",
      "blocks.23.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.23.cross_attn.q.lora_A.weight\n",
      "blocks.23.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.23.cross_attn.v.lora_A.weight\n",
      "blocks.23.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.23.ffn.0.lora_A.weight\n",
      "blocks.23.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.23.ffn.2.lora_A.weight\n",
      "blocks.23.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.23.self_attn.k.lora_A.weight\n",
      "blocks.23.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.23.self_attn.o.lora_A.weight\n",
      "blocks.23.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.23.self_attn.q.lora_A.weight\n",
      "blocks.23.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.23.self_attn.v.lora_A.weight\n",
      "blocks.23.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.24.cross_attn.k.lora_A.weight\n",
      "blocks.24.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.24.cross_attn.o.lora_A.weight\n",
      "blocks.24.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.24.cross_attn.q.lora_A.weight\n",
      "blocks.24.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.24.cross_attn.v.lora_A.weight\n",
      "blocks.24.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.24.ffn.0.lora_A.weight\n",
      "blocks.24.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.24.ffn.2.lora_A.weight\n",
      "blocks.24.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.24.self_attn.k.lora_A.weight\n",
      "blocks.24.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.24.self_attn.o.lora_A.weight\n",
      "blocks.24.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.24.self_attn.q.lora_A.weight\n",
      "blocks.24.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.24.self_attn.v.lora_A.weight\n",
      "blocks.24.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.25.cross_attn.k.lora_A.weight\n",
      "blocks.25.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.25.cross_attn.o.lora_A.weight\n",
      "blocks.25.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.25.cross_attn.q.lora_A.weight\n",
      "blocks.25.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.25.cross_attn.v.lora_A.weight\n",
      "blocks.25.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.25.ffn.0.lora_A.weight\n",
      "blocks.25.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.25.ffn.2.lora_A.weight\n",
      "blocks.25.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.25.self_attn.k.lora_A.weight\n",
      "blocks.25.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.25.self_attn.o.lora_A.weight\n",
      "blocks.25.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.25.self_attn.q.lora_A.weight\n",
      "blocks.25.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.25.self_attn.v.lora_A.weight\n",
      "blocks.25.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.26.cross_attn.k.lora_A.weight\n",
      "blocks.26.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.26.cross_attn.o.lora_A.weight\n",
      "blocks.26.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.26.cross_attn.q.lora_A.weight\n",
      "blocks.26.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.26.cross_attn.v.lora_A.weight\n",
      "blocks.26.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.26.ffn.0.lora_A.weight\n",
      "blocks.26.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.26.ffn.2.lora_A.weight\n",
      "blocks.26.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.26.self_attn.k.lora_A.weight\n",
      "blocks.26.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.26.self_attn.o.lora_A.weight\n",
      "blocks.26.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.26.self_attn.q.lora_A.weight\n",
      "blocks.26.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.26.self_attn.v.lora_A.weight\n",
      "blocks.26.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.27.cross_attn.k.lora_A.weight\n",
      "blocks.27.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.27.cross_attn.o.lora_A.weight\n",
      "blocks.27.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.27.cross_attn.q.lora_A.weight\n",
      "blocks.27.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.27.cross_attn.v.lora_A.weight\n",
      "blocks.27.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.27.ffn.0.lora_A.weight\n",
      "blocks.27.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.27.ffn.2.lora_A.weight\n",
      "blocks.27.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.27.self_attn.k.lora_A.weight\n",
      "blocks.27.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.27.self_attn.o.lora_A.weight\n",
      "blocks.27.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.27.self_attn.q.lora_A.weight\n",
      "blocks.27.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.27.self_attn.v.lora_A.weight\n",
      "blocks.27.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.28.cross_attn.k.lora_A.weight\n",
      "blocks.28.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.28.cross_attn.o.lora_A.weight\n",
      "blocks.28.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.28.cross_attn.q.lora_A.weight\n",
      "blocks.28.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.28.cross_attn.v.lora_A.weight\n",
      "blocks.28.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.28.ffn.0.lora_A.weight\n",
      "blocks.28.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.28.ffn.2.lora_A.weight\n",
      "blocks.28.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.28.self_attn.k.lora_A.weight\n",
      "blocks.28.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.28.self_attn.o.lora_A.weight\n",
      "blocks.28.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.28.self_attn.q.lora_A.weight\n",
      "blocks.28.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.28.self_attn.v.lora_A.weight\n",
      "blocks.28.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.29.cross_attn.k.lora_A.weight\n",
      "blocks.29.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.29.cross_attn.o.lora_A.weight\n",
      "blocks.29.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.29.cross_attn.q.lora_A.weight\n",
      "blocks.29.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.29.cross_attn.v.lora_A.weight\n",
      "blocks.29.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.29.ffn.0.lora_A.weight\n",
      "blocks.29.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.29.ffn.2.lora_A.weight\n",
      "blocks.29.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.29.self_attn.k.lora_A.weight\n",
      "blocks.29.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.29.self_attn.o.lora_A.weight\n",
      "blocks.29.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.29.self_attn.q.lora_A.weight\n",
      "blocks.29.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.29.self_attn.v.lora_A.weight\n",
      "blocks.29.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.30.cross_attn.k.lora_A.weight\n",
      "blocks.30.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.30.cross_attn.o.lora_A.weight\n",
      "blocks.30.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.30.cross_attn.q.lora_A.weight\n",
      "blocks.30.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.30.cross_attn.v.lora_A.weight\n",
      "blocks.30.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.30.ffn.0.lora_A.weight\n",
      "blocks.30.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.30.ffn.2.lora_A.weight\n",
      "blocks.30.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.30.self_attn.k.lora_A.weight\n",
      "blocks.30.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.30.self_attn.o.lora_A.weight\n",
      "blocks.30.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.30.self_attn.q.lora_A.weight\n",
      "blocks.30.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.30.self_attn.v.lora_A.weight\n",
      "blocks.30.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.31.cross_attn.k.lora_A.weight\n",
      "blocks.31.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.31.cross_attn.o.lora_A.weight\n",
      "blocks.31.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.31.cross_attn.q.lora_A.weight\n",
      "blocks.31.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.31.cross_attn.v.lora_A.weight\n",
      "blocks.31.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.31.ffn.0.lora_A.weight\n",
      "blocks.31.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.31.ffn.2.lora_A.weight\n",
      "blocks.31.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.31.self_attn.k.lora_A.weight\n",
      "blocks.31.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.31.self_attn.o.lora_A.weight\n",
      "blocks.31.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.31.self_attn.q.lora_A.weight\n",
      "blocks.31.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.31.self_attn.v.lora_A.weight\n",
      "blocks.31.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.32.cross_attn.k.lora_A.weight\n",
      "blocks.32.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.32.cross_attn.o.lora_A.weight\n",
      "blocks.32.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.32.cross_attn.q.lora_A.weight\n",
      "blocks.32.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.32.cross_attn.v.lora_A.weight\n",
      "blocks.32.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.32.ffn.0.lora_A.weight\n",
      "blocks.32.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.32.ffn.2.lora_A.weight\n",
      "blocks.32.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.32.self_attn.k.lora_A.weight\n",
      "blocks.32.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.32.self_attn.o.lora_A.weight\n",
      "blocks.32.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.32.self_attn.q.lora_A.weight\n",
      "blocks.32.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.32.self_attn.v.lora_A.weight\n",
      "blocks.32.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.33.cross_attn.k.lora_A.weight\n",
      "blocks.33.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.33.cross_attn.o.lora_A.weight\n",
      "blocks.33.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.33.cross_attn.q.lora_A.weight\n",
      "blocks.33.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.33.cross_attn.v.lora_A.weight\n",
      "blocks.33.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.33.ffn.0.lora_A.weight\n",
      "blocks.33.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.33.ffn.2.lora_A.weight\n",
      "blocks.33.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.33.self_attn.k.lora_A.weight\n",
      "blocks.33.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.33.self_attn.o.lora_A.weight\n",
      "blocks.33.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.33.self_attn.q.lora_A.weight\n",
      "blocks.33.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.33.self_attn.v.lora_A.weight\n",
      "blocks.33.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.34.cross_attn.k.lora_A.weight\n",
      "blocks.34.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.34.cross_attn.o.lora_A.weight\n",
      "blocks.34.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.34.cross_attn.q.lora_A.weight\n",
      "blocks.34.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.34.cross_attn.v.lora_A.weight\n",
      "blocks.34.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.34.ffn.0.lora_A.weight\n",
      "blocks.34.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.34.ffn.2.lora_A.weight\n",
      "blocks.34.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.34.self_attn.k.lora_A.weight\n",
      "blocks.34.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.34.self_attn.o.lora_A.weight\n",
      "blocks.34.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.34.self_attn.q.lora_A.weight\n",
      "blocks.34.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.34.self_attn.v.lora_A.weight\n",
      "blocks.34.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.35.cross_attn.k.lora_A.weight\n",
      "blocks.35.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.35.cross_attn.o.lora_A.weight\n",
      "blocks.35.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.35.cross_attn.q.lora_A.weight\n",
      "blocks.35.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.35.cross_attn.v.lora_A.weight\n",
      "blocks.35.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.35.ffn.0.lora_A.weight\n",
      "blocks.35.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.35.ffn.2.lora_A.weight\n",
      "blocks.35.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.35.self_attn.k.lora_A.weight\n",
      "blocks.35.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.35.self_attn.o.lora_A.weight\n",
      "blocks.35.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.35.self_attn.q.lora_A.weight\n",
      "blocks.35.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.35.self_attn.v.lora_A.weight\n",
      "blocks.35.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.36.cross_attn.k.lora_A.weight\n",
      "blocks.36.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.36.cross_attn.o.lora_A.weight\n",
      "blocks.36.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.36.cross_attn.q.lora_A.weight\n",
      "blocks.36.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.36.cross_attn.v.lora_A.weight\n",
      "blocks.36.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.36.ffn.0.lora_A.weight\n",
      "blocks.36.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.36.ffn.2.lora_A.weight\n",
      "blocks.36.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.36.self_attn.k.lora_A.weight\n",
      "blocks.36.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.36.self_attn.o.lora_A.weight\n",
      "blocks.36.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.36.self_attn.q.lora_A.weight\n",
      "blocks.36.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.36.self_attn.v.lora_A.weight\n",
      "blocks.36.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.37.cross_attn.k.lora_A.weight\n",
      "blocks.37.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.37.cross_attn.o.lora_A.weight\n",
      "blocks.37.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.37.cross_attn.q.lora_A.weight\n",
      "blocks.37.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.37.cross_attn.v.lora_A.weight\n",
      "blocks.37.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.37.ffn.0.lora_A.weight\n",
      "blocks.37.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.37.ffn.2.lora_A.weight\n",
      "blocks.37.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.37.self_attn.k.lora_A.weight\n",
      "blocks.37.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.37.self_attn.o.lora_A.weight\n",
      "blocks.37.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.37.self_attn.q.lora_A.weight\n",
      "blocks.37.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.37.self_attn.v.lora_A.weight\n",
      "blocks.37.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.38.cross_attn.k.lora_A.weight\n",
      "blocks.38.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.38.cross_attn.o.lora_A.weight\n",
      "blocks.38.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.38.cross_attn.q.lora_A.weight\n",
      "blocks.38.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.38.cross_attn.v.lora_A.weight\n",
      "blocks.38.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.38.ffn.0.lora_A.weight\n",
      "blocks.38.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.38.ffn.2.lora_A.weight\n",
      "blocks.38.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.38.self_attn.k.lora_A.weight\n",
      "blocks.38.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.38.self_attn.o.lora_A.weight\n",
      "blocks.38.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.38.self_attn.q.lora_A.weight\n",
      "blocks.38.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.38.self_attn.v.lora_A.weight\n",
      "blocks.38.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.39.cross_attn.k.lora_A.weight\n",
      "blocks.39.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.39.cross_attn.o.lora_A.weight\n",
      "blocks.39.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.39.cross_attn.q.lora_A.weight\n",
      "blocks.39.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.39.cross_attn.v.lora_A.weight\n",
      "blocks.39.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.39.ffn.0.lora_A.weight\n",
      "blocks.39.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.39.ffn.2.lora_A.weight\n",
      "blocks.39.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.39.self_attn.k.lora_A.weight\n",
      "blocks.39.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.39.self_attn.o.lora_A.weight\n",
      "blocks.39.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.39.self_attn.q.lora_A.weight\n",
      "blocks.39.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.39.self_attn.v.lora_A.weight\n",
      "blocks.39.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "lora_pairs1 keys:dict_keys(['blocks.0.cross_attn.k', 'blocks.0.cross_attn.o', 'blocks.0.cross_attn.q', 'blocks.0.cross_attn.v', 'blocks.0.ffn.0', 'blocks.0.ffn.2', 'blocks.0.self_attn.k', 'blocks.0.self_attn.o', 'blocks.0.self_attn.q', 'blocks.0.self_attn.v', 'blocks.1.cross_attn.k', 'blocks.1.cross_attn.o', 'blocks.1.cross_attn.q', 'blocks.1.cross_attn.v', 'blocks.1.ffn.0', 'blocks.1.ffn.2', 'blocks.1.self_attn.k', 'blocks.1.self_attn.o', 'blocks.1.self_attn.q', 'blocks.1.self_attn.v', 'blocks.2.cross_attn.k', 'blocks.2.cross_attn.o', 'blocks.2.cross_attn.q', 'blocks.2.cross_attn.v', 'blocks.2.ffn.0', 'blocks.2.ffn.2', 'blocks.2.self_attn.k', 'blocks.2.self_attn.o', 'blocks.2.self_attn.q', 'blocks.2.self_attn.v', 'blocks.3.cross_attn.k', 'blocks.3.cross_attn.o', 'blocks.3.cross_attn.q', 'blocks.3.cross_attn.v', 'blocks.3.ffn.0', 'blocks.3.ffn.2', 'blocks.3.self_attn.k', 'blocks.3.self_attn.o', 'blocks.3.self_attn.q', 'blocks.3.self_attn.v', 'blocks.4.cross_attn.k', 'blocks.4.cross_attn.o', 'blocks.4.cross_attn.q', 'blocks.4.cross_attn.v', 'blocks.4.ffn.0', 'blocks.4.ffn.2', 'blocks.4.self_attn.k', 'blocks.4.self_attn.o', 'blocks.4.self_attn.q', 'blocks.4.self_attn.v', 'blocks.5.cross_attn.k', 'blocks.5.cross_attn.o', 'blocks.5.cross_attn.q', 'blocks.5.cross_attn.v', 'blocks.5.ffn.0', 'blocks.5.ffn.2', 'blocks.5.self_attn.k', 'blocks.5.self_attn.o', 'blocks.5.self_attn.q', 'blocks.5.self_attn.v', 'blocks.6.cross_attn.k', 'blocks.6.cross_attn.o', 'blocks.6.cross_attn.q', 'blocks.6.cross_attn.v', 'blocks.6.ffn.0', 'blocks.6.ffn.2', 'blocks.6.self_attn.k', 'blocks.6.self_attn.o', 'blocks.6.self_attn.q', 'blocks.6.self_attn.v', 'blocks.7.cross_attn.k', 'blocks.7.cross_attn.o', 'blocks.7.cross_attn.q', 'blocks.7.cross_attn.v', 'blocks.7.ffn.0', 'blocks.7.ffn.2', 'blocks.7.self_attn.k', 'blocks.7.self_attn.o', 'blocks.7.self_attn.q', 'blocks.7.self_attn.v', 'blocks.8.cross_attn.k', 'blocks.8.cross_attn.o', 'blocks.8.cross_attn.q', 'blocks.8.cross_attn.v', 'blocks.8.ffn.0', 'blocks.8.ffn.2', 'blocks.8.self_attn.k', 'blocks.8.self_attn.o', 'blocks.8.self_attn.q', 'blocks.8.self_attn.v', 'blocks.9.cross_attn.k', 'blocks.9.cross_attn.o', 'blocks.9.cross_attn.q', 'blocks.9.cross_attn.v', 'blocks.9.ffn.0', 'blocks.9.ffn.2', 'blocks.9.self_attn.k', 'blocks.9.self_attn.o', 'blocks.9.self_attn.q', 'blocks.9.self_attn.v', 'blocks.10.cross_attn.k', 'blocks.10.cross_attn.o', 'blocks.10.cross_attn.q', 'blocks.10.cross_attn.v', 'blocks.10.ffn.0', 'blocks.10.ffn.2', 'blocks.10.self_attn.k', 'blocks.10.self_attn.o', 'blocks.10.self_attn.q', 'blocks.10.self_attn.v', 'blocks.11.cross_attn.k', 'blocks.11.cross_attn.o', 'blocks.11.cross_attn.q', 'blocks.11.cross_attn.v', 'blocks.11.ffn.0', 'blocks.11.ffn.2', 'blocks.11.self_attn.k', 'blocks.11.self_attn.o', 'blocks.11.self_attn.q', 'blocks.11.self_attn.v', 'blocks.12.cross_attn.k', 'blocks.12.cross_attn.o', 'blocks.12.cross_attn.q', 'blocks.12.cross_attn.v', 'blocks.12.ffn.0', 'blocks.12.ffn.2', 'blocks.12.self_attn.k', 'blocks.12.self_attn.o', 'blocks.12.self_attn.q', 'blocks.12.self_attn.v', 'blocks.13.cross_attn.k', 'blocks.13.cross_attn.o', 'blocks.13.cross_attn.q', 'blocks.13.cross_attn.v', 'blocks.13.ffn.0', 'blocks.13.ffn.2', 'blocks.13.self_attn.k', 'blocks.13.self_attn.o', 'blocks.13.self_attn.q', 'blocks.13.self_attn.v', 'blocks.14.cross_attn.k', 'blocks.14.cross_attn.o', 'blocks.14.cross_attn.q', 'blocks.14.cross_attn.v', 'blocks.14.ffn.0', 'blocks.14.ffn.2', 'blocks.14.self_attn.k', 'blocks.14.self_attn.o', 'blocks.14.self_attn.q', 'blocks.14.self_attn.v', 'blocks.15.cross_attn.k', 'blocks.15.cross_attn.o', 'blocks.15.cross_attn.q', 'blocks.15.cross_attn.v', 'blocks.15.ffn.0', 'blocks.15.ffn.2', 'blocks.15.self_attn.k', 'blocks.15.self_attn.o', 'blocks.15.self_attn.q', 'blocks.15.self_attn.v', 'blocks.16.cross_attn.k', 'blocks.16.cross_attn.o', 'blocks.16.cross_attn.q', 'blocks.16.cross_attn.v', 'blocks.16.ffn.0', 'blocks.16.ffn.2', 'blocks.16.self_attn.k', 'blocks.16.self_attn.o', 'blocks.16.self_attn.q', 'blocks.16.self_attn.v', 'blocks.17.cross_attn.k', 'blocks.17.cross_attn.o', 'blocks.17.cross_attn.q', 'blocks.17.cross_attn.v', 'blocks.17.ffn.0', 'blocks.17.ffn.2', 'blocks.17.self_attn.k', 'blocks.17.self_attn.o', 'blocks.17.self_attn.q', 'blocks.17.self_attn.v', 'blocks.18.cross_attn.k', 'blocks.18.cross_attn.o', 'blocks.18.cross_attn.q', 'blocks.18.cross_attn.v', 'blocks.18.ffn.0', 'blocks.18.ffn.2', 'blocks.18.self_attn.k', 'blocks.18.self_attn.o', 'blocks.18.self_attn.q', 'blocks.18.self_attn.v', 'blocks.19.cross_attn.k', 'blocks.19.cross_attn.o', 'blocks.19.cross_attn.q', 'blocks.19.cross_attn.v', 'blocks.19.ffn.0', 'blocks.19.ffn.2', 'blocks.19.self_attn.k', 'blocks.19.self_attn.o', 'blocks.19.self_attn.q', 'blocks.19.self_attn.v', 'blocks.20.cross_attn.k', 'blocks.20.cross_attn.o', 'blocks.20.cross_attn.q', 'blocks.20.cross_attn.v', 'blocks.20.ffn.0', 'blocks.20.ffn.2', 'blocks.20.self_attn.k', 'blocks.20.self_attn.o', 'blocks.20.self_attn.q', 'blocks.20.self_attn.v', 'blocks.21.cross_attn.k', 'blocks.21.cross_attn.o', 'blocks.21.cross_attn.q', 'blocks.21.cross_attn.v', 'blocks.21.ffn.0', 'blocks.21.ffn.2', 'blocks.21.self_attn.k', 'blocks.21.self_attn.o', 'blocks.21.self_attn.q', 'blocks.21.self_attn.v', 'blocks.22.cross_attn.k', 'blocks.22.cross_attn.o', 'blocks.22.cross_attn.q', 'blocks.22.cross_attn.v', 'blocks.22.ffn.0', 'blocks.22.ffn.2', 'blocks.22.self_attn.k', 'blocks.22.self_attn.o', 'blocks.22.self_attn.q', 'blocks.22.self_attn.v', 'blocks.23.cross_attn.k', 'blocks.23.cross_attn.o', 'blocks.23.cross_attn.q', 'blocks.23.cross_attn.v', 'blocks.23.ffn.0', 'blocks.23.ffn.2', 'blocks.23.self_attn.k', 'blocks.23.self_attn.o', 'blocks.23.self_attn.q', 'blocks.23.self_attn.v', 'blocks.24.cross_attn.k', 'blocks.24.cross_attn.o', 'blocks.24.cross_attn.q', 'blocks.24.cross_attn.v', 'blocks.24.ffn.0', 'blocks.24.ffn.2', 'blocks.24.self_attn.k', 'blocks.24.self_attn.o', 'blocks.24.self_attn.q', 'blocks.24.self_attn.v', 'blocks.25.cross_attn.k', 'blocks.25.cross_attn.o', 'blocks.25.cross_attn.q', 'blocks.25.cross_attn.v', 'blocks.25.ffn.0', 'blocks.25.ffn.2', 'blocks.25.self_attn.k', 'blocks.25.self_attn.o', 'blocks.25.self_attn.q', 'blocks.25.self_attn.v', 'blocks.26.cross_attn.k', 'blocks.26.cross_attn.o', 'blocks.26.cross_attn.q', 'blocks.26.cross_attn.v', 'blocks.26.ffn.0', 'blocks.26.ffn.2', 'blocks.26.self_attn.k', 'blocks.26.self_attn.o', 'blocks.26.self_attn.q', 'blocks.26.self_attn.v', 'blocks.27.cross_attn.k', 'blocks.27.cross_attn.o', 'blocks.27.cross_attn.q', 'blocks.27.cross_attn.v', 'blocks.27.ffn.0', 'blocks.27.ffn.2', 'blocks.27.self_attn.k', 'blocks.27.self_attn.o', 'blocks.27.self_attn.q', 'blocks.27.self_attn.v', 'blocks.28.cross_attn.k', 'blocks.28.cross_attn.o', 'blocks.28.cross_attn.q', 'blocks.28.cross_attn.v', 'blocks.28.ffn.0', 'blocks.28.ffn.2', 'blocks.28.self_attn.k', 'blocks.28.self_attn.o', 'blocks.28.self_attn.q', 'blocks.28.self_attn.v', 'blocks.29.cross_attn.k', 'blocks.29.cross_attn.o', 'blocks.29.cross_attn.q', 'blocks.29.cross_attn.v', 'blocks.29.ffn.0', 'blocks.29.ffn.2', 'blocks.29.self_attn.k', 'blocks.29.self_attn.o', 'blocks.29.self_attn.q', 'blocks.29.self_attn.v', 'blocks.30.cross_attn.k', 'blocks.30.cross_attn.o', 'blocks.30.cross_attn.q', 'blocks.30.cross_attn.v', 'blocks.30.ffn.0', 'blocks.30.ffn.2', 'blocks.30.self_attn.k', 'blocks.30.self_attn.o', 'blocks.30.self_attn.q', 'blocks.30.self_attn.v', 'blocks.31.cross_attn.k', 'blocks.31.cross_attn.o', 'blocks.31.cross_attn.q', 'blocks.31.cross_attn.v', 'blocks.31.ffn.0', 'blocks.31.ffn.2', 'blocks.31.self_attn.k', 'blocks.31.self_attn.o', 'blocks.31.self_attn.q', 'blocks.31.self_attn.v', 'blocks.32.cross_attn.k', 'blocks.32.cross_attn.o', 'blocks.32.cross_attn.q', 'blocks.32.cross_attn.v', 'blocks.32.ffn.0', 'blocks.32.ffn.2', 'blocks.32.self_attn.k', 'blocks.32.self_attn.o', 'blocks.32.self_attn.q', 'blocks.32.self_attn.v', 'blocks.33.cross_attn.k', 'blocks.33.cross_attn.o', 'blocks.33.cross_attn.q', 'blocks.33.cross_attn.v', 'blocks.33.ffn.0', 'blocks.33.ffn.2', 'blocks.33.self_attn.k', 'blocks.33.self_attn.o', 'blocks.33.self_attn.q', 'blocks.33.self_attn.v', 'blocks.34.cross_attn.k', 'blocks.34.cross_attn.o', 'blocks.34.cross_attn.q', 'blocks.34.cross_attn.v', 'blocks.34.ffn.0', 'blocks.34.ffn.2', 'blocks.34.self_attn.k', 'blocks.34.self_attn.o', 'blocks.34.self_attn.q', 'blocks.34.self_attn.v', 'blocks.35.cross_attn.k', 'blocks.35.cross_attn.o', 'blocks.35.cross_attn.q', 'blocks.35.cross_attn.v', 'blocks.35.ffn.0', 'blocks.35.ffn.2', 'blocks.35.self_attn.k', 'blocks.35.self_attn.o', 'blocks.35.self_attn.q', 'blocks.35.self_attn.v', 'blocks.36.cross_attn.k', 'blocks.36.cross_attn.o', 'blocks.36.cross_attn.q', 'blocks.36.cross_attn.v', 'blocks.36.ffn.0', 'blocks.36.ffn.2', 'blocks.36.self_attn.k', 'blocks.36.self_attn.o', 'blocks.36.self_attn.q', 'blocks.36.self_attn.v', 'blocks.37.cross_attn.k', 'blocks.37.cross_attn.o', 'blocks.37.cross_attn.q', 'blocks.37.cross_attn.v', 'blocks.37.ffn.0', 'blocks.37.ffn.2', 'blocks.37.self_attn.k', 'blocks.37.self_attn.o', 'blocks.37.self_attn.q', 'blocks.37.self_attn.v', 'blocks.38.cross_attn.k', 'blocks.38.cross_attn.o', 'blocks.38.cross_attn.q', 'blocks.38.cross_attn.v', 'blocks.38.ffn.0', 'blocks.38.ffn.2', 'blocks.38.self_attn.k', 'blocks.38.self_attn.o', 'blocks.38.self_attn.q', 'blocks.38.self_attn.v', 'blocks.39.cross_attn.k', 'blocks.39.cross_attn.o', 'blocks.39.cross_attn.q', 'blocks.39.cross_attn.v', 'blocks.39.ffn.0', 'blocks.39.ffn.2', 'blocks.39.self_attn.k', 'blocks.39.self_attn.o', 'blocks.39.self_attn.q', 'blocks.39.self_attn.v'])\n",
      "base_key가 존재:blocks.0.self_attn.q.weight\n",
      "blocks.0.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.0.self_attn.q\n",
      "blocks.0.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.0.self_attn.k.weight\n",
      "blocks.0.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.0.self_attn.k\n",
      "blocks.0.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.0.self_attn.v.weight\n",
      "blocks.0.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.0.self_attn.v\n",
      "blocks.0.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.0.self_attn.o.weight\n",
      "blocks.0.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.0.self_attn.o\n",
      "blocks.0.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.0.cross_attn.q.weight\n",
      "blocks.0.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.0.cross_attn.q\n",
      "blocks.0.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.0.cross_attn.k.weight\n",
      "blocks.0.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.0.cross_attn.k\n",
      "blocks.0.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.0.cross_attn.v.weight\n",
      "blocks.0.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.0.cross_attn.v\n",
      "blocks.0.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.0.cross_attn.o.weight\n",
      "blocks.0.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.0.cross_attn.o\n",
      "blocks.0.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.0.ffn.0.weight\n",
      "blocks.0.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.0.ffn.0\n",
      "blocks.0.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.0.ffn.2.weight\n",
      "blocks.0.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.0.ffn.2\n",
      "blocks.0.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.1.self_attn.q.weight\n",
      "blocks.1.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.1.self_attn.q\n",
      "blocks.1.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.1.self_attn.k.weight\n",
      "blocks.1.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.1.self_attn.k\n",
      "blocks.1.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.1.self_attn.v.weight\n",
      "blocks.1.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.1.self_attn.v\n",
      "blocks.1.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.1.self_attn.o.weight\n",
      "blocks.1.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.1.self_attn.o\n",
      "blocks.1.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.1.cross_attn.q.weight\n",
      "blocks.1.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.1.cross_attn.q\n",
      "blocks.1.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.1.cross_attn.k.weight\n",
      "blocks.1.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.1.cross_attn.k\n",
      "blocks.1.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.1.cross_attn.v.weight\n",
      "blocks.1.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.1.cross_attn.v\n",
      "blocks.1.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.1.cross_attn.o.weight\n",
      "blocks.1.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.1.cross_attn.o\n",
      "blocks.1.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.1.ffn.0.weight\n",
      "blocks.1.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.1.ffn.0\n",
      "blocks.1.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.1.ffn.2.weight\n",
      "blocks.1.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.1.ffn.2\n",
      "blocks.1.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.2.self_attn.q.weight\n",
      "blocks.2.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.2.self_attn.q\n",
      "blocks.2.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.2.self_attn.k.weight\n",
      "blocks.2.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.2.self_attn.k\n",
      "blocks.2.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.2.self_attn.v.weight\n",
      "blocks.2.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.2.self_attn.v\n",
      "blocks.2.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.2.self_attn.o.weight\n",
      "blocks.2.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.2.self_attn.o\n",
      "blocks.2.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.2.cross_attn.q.weight\n",
      "blocks.2.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.2.cross_attn.q\n",
      "blocks.2.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.2.cross_attn.k.weight\n",
      "blocks.2.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.2.cross_attn.k\n",
      "blocks.2.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.2.cross_attn.v.weight\n",
      "blocks.2.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.2.cross_attn.v\n",
      "blocks.2.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.2.cross_attn.o.weight\n",
      "blocks.2.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.2.cross_attn.o\n",
      "blocks.2.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.2.ffn.0.weight\n",
      "blocks.2.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.2.ffn.0\n",
      "blocks.2.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.2.ffn.2.weight\n",
      "blocks.2.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.2.ffn.2\n",
      "blocks.2.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.3.self_attn.q.weight\n",
      "blocks.3.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.3.self_attn.q\n",
      "blocks.3.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.3.self_attn.k.weight\n",
      "blocks.3.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.3.self_attn.k\n",
      "blocks.3.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.3.self_attn.v.weight\n",
      "blocks.3.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.3.self_attn.v\n",
      "blocks.3.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.3.self_attn.o.weight\n",
      "blocks.3.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.3.self_attn.o\n",
      "blocks.3.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.3.cross_attn.q.weight\n",
      "blocks.3.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.3.cross_attn.q\n",
      "blocks.3.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.3.cross_attn.k.weight\n",
      "blocks.3.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.3.cross_attn.k\n",
      "blocks.3.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.3.cross_attn.v.weight\n",
      "blocks.3.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.3.cross_attn.v\n",
      "blocks.3.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.3.cross_attn.o.weight\n",
      "blocks.3.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.3.cross_attn.o\n",
      "blocks.3.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.3.ffn.0.weight\n",
      "blocks.3.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.3.ffn.0\n",
      "blocks.3.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.3.ffn.2.weight\n",
      "blocks.3.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.3.ffn.2\n",
      "blocks.3.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.4.self_attn.q.weight\n",
      "blocks.4.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.4.self_attn.q\n",
      "blocks.4.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.4.self_attn.k.weight\n",
      "blocks.4.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.4.self_attn.k\n",
      "blocks.4.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.4.self_attn.v.weight\n",
      "blocks.4.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.4.self_attn.v\n",
      "blocks.4.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.4.self_attn.o.weight\n",
      "blocks.4.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.4.self_attn.o\n",
      "blocks.4.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.4.cross_attn.q.weight\n",
      "blocks.4.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.4.cross_attn.q\n",
      "blocks.4.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.4.cross_attn.k.weight\n",
      "blocks.4.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.4.cross_attn.k\n",
      "blocks.4.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.4.cross_attn.v.weight\n",
      "blocks.4.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.4.cross_attn.v\n",
      "blocks.4.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.4.cross_attn.o.weight\n",
      "blocks.4.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.4.cross_attn.o\n",
      "blocks.4.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.4.ffn.0.weight\n",
      "blocks.4.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.4.ffn.0\n",
      "blocks.4.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.4.ffn.2.weight\n",
      "blocks.4.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.4.ffn.2\n",
      "blocks.4.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.5.self_attn.q.weight\n",
      "blocks.5.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.5.self_attn.q\n",
      "blocks.5.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.5.self_attn.k.weight\n",
      "blocks.5.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.5.self_attn.k\n",
      "blocks.5.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.5.self_attn.v.weight\n",
      "blocks.5.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.5.self_attn.v\n",
      "blocks.5.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.5.self_attn.o.weight\n",
      "blocks.5.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.5.self_attn.o\n",
      "blocks.5.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.5.cross_attn.q.weight\n",
      "blocks.5.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.5.cross_attn.q\n",
      "blocks.5.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.5.cross_attn.k.weight\n",
      "blocks.5.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.5.cross_attn.k\n",
      "blocks.5.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.5.cross_attn.v.weight\n",
      "blocks.5.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.5.cross_attn.v\n",
      "blocks.5.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.5.cross_attn.o.weight\n",
      "blocks.5.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.5.cross_attn.o\n",
      "blocks.5.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.5.ffn.0.weight\n",
      "blocks.5.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.5.ffn.0\n",
      "blocks.5.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.5.ffn.2.weight\n",
      "blocks.5.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.5.ffn.2\n",
      "blocks.5.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.6.self_attn.q.weight\n",
      "blocks.6.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.6.self_attn.q\n",
      "blocks.6.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.6.self_attn.k.weight\n",
      "blocks.6.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.6.self_attn.k\n",
      "blocks.6.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.6.self_attn.v.weight\n",
      "blocks.6.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.6.self_attn.v\n",
      "blocks.6.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.6.self_attn.o.weight\n",
      "blocks.6.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.6.self_attn.o\n",
      "blocks.6.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.6.cross_attn.q.weight\n",
      "blocks.6.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.6.cross_attn.q\n",
      "blocks.6.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.6.cross_attn.k.weight\n",
      "blocks.6.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.6.cross_attn.k\n",
      "blocks.6.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.6.cross_attn.v.weight\n",
      "blocks.6.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.6.cross_attn.v\n",
      "blocks.6.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.6.cross_attn.o.weight\n",
      "blocks.6.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.6.cross_attn.o\n",
      "blocks.6.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.6.ffn.0.weight\n",
      "blocks.6.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.6.ffn.0\n",
      "blocks.6.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.6.ffn.2.weight\n",
      "blocks.6.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.6.ffn.2\n",
      "blocks.6.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.7.self_attn.q.weight\n",
      "blocks.7.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.7.self_attn.q\n",
      "blocks.7.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.7.self_attn.k.weight\n",
      "blocks.7.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.7.self_attn.k\n",
      "blocks.7.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.7.self_attn.v.weight\n",
      "blocks.7.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.7.self_attn.v\n",
      "blocks.7.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.7.self_attn.o.weight\n",
      "blocks.7.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.7.self_attn.o\n",
      "blocks.7.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.7.cross_attn.q.weight\n",
      "blocks.7.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.7.cross_attn.q\n",
      "blocks.7.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.7.cross_attn.k.weight\n",
      "blocks.7.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.7.cross_attn.k\n",
      "blocks.7.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.7.cross_attn.v.weight\n",
      "blocks.7.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.7.cross_attn.v\n",
      "blocks.7.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.7.cross_attn.o.weight\n",
      "blocks.7.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.7.cross_attn.o\n",
      "blocks.7.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.7.ffn.0.weight\n",
      "blocks.7.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.7.ffn.0\n",
      "blocks.7.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.7.ffn.2.weight\n",
      "blocks.7.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.7.ffn.2\n",
      "blocks.7.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.8.self_attn.q.weight\n",
      "blocks.8.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.8.self_attn.q\n",
      "blocks.8.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.8.self_attn.k.weight\n",
      "blocks.8.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.8.self_attn.k\n",
      "blocks.8.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.8.self_attn.v.weight\n",
      "blocks.8.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.8.self_attn.v\n",
      "blocks.8.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.8.self_attn.o.weight\n",
      "blocks.8.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.8.self_attn.o\n",
      "blocks.8.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.8.cross_attn.q.weight\n",
      "blocks.8.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.8.cross_attn.q\n",
      "blocks.8.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.8.cross_attn.k.weight\n",
      "blocks.8.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.8.cross_attn.k\n",
      "blocks.8.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.8.cross_attn.v.weight\n",
      "blocks.8.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.8.cross_attn.v\n",
      "blocks.8.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.8.cross_attn.o.weight\n",
      "blocks.8.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.8.cross_attn.o\n",
      "blocks.8.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.8.ffn.0.weight\n",
      "blocks.8.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.8.ffn.0\n",
      "blocks.8.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.8.ffn.2.weight\n",
      "blocks.8.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.8.ffn.2\n",
      "blocks.8.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.9.self_attn.q.weight\n",
      "blocks.9.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.9.self_attn.q\n",
      "blocks.9.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.9.self_attn.k.weight\n",
      "blocks.9.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.9.self_attn.k\n",
      "blocks.9.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.9.self_attn.v.weight\n",
      "blocks.9.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.9.self_attn.v\n",
      "blocks.9.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.9.self_attn.o.weight\n",
      "blocks.9.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.9.self_attn.o\n",
      "blocks.9.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.9.cross_attn.q.weight\n",
      "blocks.9.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.9.cross_attn.q\n",
      "blocks.9.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.9.cross_attn.k.weight\n",
      "blocks.9.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.9.cross_attn.k\n",
      "blocks.9.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.9.cross_attn.v.weight\n",
      "blocks.9.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.9.cross_attn.v\n",
      "blocks.9.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.9.cross_attn.o.weight\n",
      "blocks.9.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.9.cross_attn.o\n",
      "blocks.9.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.9.ffn.0.weight\n",
      "blocks.9.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.9.ffn.0\n",
      "blocks.9.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.9.ffn.2.weight\n",
      "blocks.9.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.9.ffn.2\n",
      "blocks.9.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.10.self_attn.q.weight\n",
      "blocks.10.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.10.self_attn.q\n",
      "blocks.10.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.10.self_attn.k.weight\n",
      "blocks.10.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.10.self_attn.k\n",
      "blocks.10.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.10.self_attn.v.weight\n",
      "blocks.10.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.10.self_attn.v\n",
      "blocks.10.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.10.self_attn.o.weight\n",
      "blocks.10.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.10.self_attn.o\n",
      "blocks.10.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.10.cross_attn.q.weight\n",
      "blocks.10.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.10.cross_attn.q\n",
      "blocks.10.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.10.cross_attn.k.weight\n",
      "blocks.10.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.10.cross_attn.k\n",
      "blocks.10.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.10.cross_attn.v.weight\n",
      "blocks.10.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.10.cross_attn.v\n",
      "blocks.10.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.10.cross_attn.o.weight\n",
      "blocks.10.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.10.cross_attn.o\n",
      "blocks.10.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.10.ffn.0.weight\n",
      "blocks.10.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.10.ffn.0\n",
      "blocks.10.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.10.ffn.2.weight\n",
      "blocks.10.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.10.ffn.2\n",
      "blocks.10.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.11.self_attn.q.weight\n",
      "blocks.11.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.11.self_attn.q\n",
      "blocks.11.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.11.self_attn.k.weight\n",
      "blocks.11.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.11.self_attn.k\n",
      "blocks.11.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.11.self_attn.v.weight\n",
      "blocks.11.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.11.self_attn.v\n",
      "blocks.11.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.11.self_attn.o.weight\n",
      "blocks.11.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.11.self_attn.o\n",
      "blocks.11.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.11.cross_attn.q.weight\n",
      "blocks.11.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.11.cross_attn.q\n",
      "blocks.11.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.11.cross_attn.k.weight\n",
      "blocks.11.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.11.cross_attn.k\n",
      "blocks.11.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.11.cross_attn.v.weight\n",
      "blocks.11.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.11.cross_attn.v\n",
      "blocks.11.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.11.cross_attn.o.weight\n",
      "blocks.11.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.11.cross_attn.o\n",
      "blocks.11.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.11.ffn.0.weight\n",
      "blocks.11.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.11.ffn.0\n",
      "blocks.11.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.11.ffn.2.weight\n",
      "blocks.11.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.11.ffn.2\n",
      "blocks.11.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.12.self_attn.q.weight\n",
      "blocks.12.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.12.self_attn.q\n",
      "blocks.12.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.12.self_attn.k.weight\n",
      "blocks.12.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.12.self_attn.k\n",
      "blocks.12.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.12.self_attn.v.weight\n",
      "blocks.12.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.12.self_attn.v\n",
      "blocks.12.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.12.self_attn.o.weight\n",
      "blocks.12.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.12.self_attn.o\n",
      "blocks.12.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.12.cross_attn.q.weight\n",
      "blocks.12.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.12.cross_attn.q\n",
      "blocks.12.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.12.cross_attn.k.weight\n",
      "blocks.12.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.12.cross_attn.k\n",
      "blocks.12.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.12.cross_attn.v.weight\n",
      "blocks.12.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.12.cross_attn.v\n",
      "blocks.12.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.12.cross_attn.o.weight\n",
      "blocks.12.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.12.cross_attn.o\n",
      "blocks.12.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.12.ffn.0.weight\n",
      "blocks.12.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.12.ffn.0\n",
      "blocks.12.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.12.ffn.2.weight\n",
      "blocks.12.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.12.ffn.2\n",
      "blocks.12.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.13.self_attn.q.weight\n",
      "blocks.13.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.13.self_attn.q\n",
      "blocks.13.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.13.self_attn.k.weight\n",
      "blocks.13.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.13.self_attn.k\n",
      "blocks.13.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.13.self_attn.v.weight\n",
      "blocks.13.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.13.self_attn.v\n",
      "blocks.13.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.13.self_attn.o.weight\n",
      "blocks.13.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.13.self_attn.o\n",
      "blocks.13.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.13.cross_attn.q.weight\n",
      "blocks.13.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.13.cross_attn.q\n",
      "blocks.13.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.13.cross_attn.k.weight\n",
      "blocks.13.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.13.cross_attn.k\n",
      "blocks.13.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.13.cross_attn.v.weight\n",
      "blocks.13.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.13.cross_attn.v\n",
      "blocks.13.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.13.cross_attn.o.weight\n",
      "blocks.13.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.13.cross_attn.o\n",
      "blocks.13.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.13.ffn.0.weight\n",
      "blocks.13.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.13.ffn.0\n",
      "blocks.13.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.13.ffn.2.weight\n",
      "blocks.13.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.13.ffn.2\n",
      "blocks.13.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.14.self_attn.q.weight\n",
      "blocks.14.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.14.self_attn.q\n",
      "blocks.14.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.14.self_attn.k.weight\n",
      "blocks.14.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.14.self_attn.k\n",
      "blocks.14.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.14.self_attn.v.weight\n",
      "blocks.14.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.14.self_attn.v\n",
      "blocks.14.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.14.self_attn.o.weight\n",
      "blocks.14.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.14.self_attn.o\n",
      "blocks.14.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.14.cross_attn.q.weight\n",
      "blocks.14.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.14.cross_attn.q\n",
      "blocks.14.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.14.cross_attn.k.weight\n",
      "blocks.14.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.14.cross_attn.k\n",
      "blocks.14.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.14.cross_attn.v.weight\n",
      "blocks.14.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.14.cross_attn.v\n",
      "blocks.14.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.14.cross_attn.o.weight\n",
      "blocks.14.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.14.cross_attn.o\n",
      "blocks.14.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.14.ffn.0.weight\n",
      "blocks.14.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.14.ffn.0\n",
      "blocks.14.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.14.ffn.2.weight\n",
      "blocks.14.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.14.ffn.2\n",
      "blocks.14.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.15.self_attn.q.weight\n",
      "blocks.15.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.15.self_attn.q\n",
      "blocks.15.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.15.self_attn.k.weight\n",
      "blocks.15.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.15.self_attn.k\n",
      "blocks.15.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.15.self_attn.v.weight\n",
      "blocks.15.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.15.self_attn.v\n",
      "blocks.15.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.15.self_attn.o.weight\n",
      "blocks.15.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.15.self_attn.o\n",
      "blocks.15.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.15.cross_attn.q.weight\n",
      "blocks.15.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.15.cross_attn.q\n",
      "blocks.15.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.15.cross_attn.k.weight\n",
      "blocks.15.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.15.cross_attn.k\n",
      "blocks.15.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.15.cross_attn.v.weight\n",
      "blocks.15.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.15.cross_attn.v\n",
      "blocks.15.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.15.cross_attn.o.weight\n",
      "blocks.15.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.15.cross_attn.o\n",
      "blocks.15.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.15.ffn.0.weight\n",
      "blocks.15.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.15.ffn.0\n",
      "blocks.15.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.15.ffn.2.weight\n",
      "blocks.15.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.15.ffn.2\n",
      "blocks.15.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.16.self_attn.q.weight\n",
      "blocks.16.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.16.self_attn.q\n",
      "blocks.16.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.16.self_attn.k.weight\n",
      "blocks.16.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.16.self_attn.k\n",
      "blocks.16.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.16.self_attn.v.weight\n",
      "blocks.16.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.16.self_attn.v\n",
      "blocks.16.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.16.self_attn.o.weight\n",
      "blocks.16.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.16.self_attn.o\n",
      "blocks.16.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.16.cross_attn.q.weight\n",
      "blocks.16.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.16.cross_attn.q\n",
      "blocks.16.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.16.cross_attn.k.weight\n",
      "blocks.16.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.16.cross_attn.k\n",
      "blocks.16.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.16.cross_attn.v.weight\n",
      "blocks.16.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.16.cross_attn.v\n",
      "blocks.16.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.16.cross_attn.o.weight\n",
      "blocks.16.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.16.cross_attn.o\n",
      "blocks.16.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.16.ffn.0.weight\n",
      "blocks.16.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.16.ffn.0\n",
      "blocks.16.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.16.ffn.2.weight\n",
      "blocks.16.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.16.ffn.2\n",
      "blocks.16.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.17.self_attn.q.weight\n",
      "blocks.17.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.17.self_attn.q\n",
      "blocks.17.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.17.self_attn.k.weight\n",
      "blocks.17.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.17.self_attn.k\n",
      "blocks.17.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.17.self_attn.v.weight\n",
      "blocks.17.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.17.self_attn.v\n",
      "blocks.17.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.17.self_attn.o.weight\n",
      "blocks.17.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.17.self_attn.o\n",
      "blocks.17.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.17.cross_attn.q.weight\n",
      "blocks.17.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.17.cross_attn.q\n",
      "blocks.17.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.17.cross_attn.k.weight\n",
      "blocks.17.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.17.cross_attn.k\n",
      "blocks.17.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.17.cross_attn.v.weight\n",
      "blocks.17.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.17.cross_attn.v\n",
      "blocks.17.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.17.cross_attn.o.weight\n",
      "blocks.17.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.17.cross_attn.o\n",
      "blocks.17.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.17.ffn.0.weight\n",
      "blocks.17.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.17.ffn.0\n",
      "blocks.17.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.17.ffn.2.weight\n",
      "blocks.17.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.17.ffn.2\n",
      "blocks.17.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.18.self_attn.q.weight\n",
      "blocks.18.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.18.self_attn.q\n",
      "blocks.18.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.18.self_attn.k.weight\n",
      "blocks.18.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.18.self_attn.k\n",
      "blocks.18.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.18.self_attn.v.weight\n",
      "blocks.18.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.18.self_attn.v\n",
      "blocks.18.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.18.self_attn.o.weight\n",
      "blocks.18.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.18.self_attn.o\n",
      "blocks.18.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.18.cross_attn.q.weight\n",
      "blocks.18.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.18.cross_attn.q\n",
      "blocks.18.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.18.cross_attn.k.weight\n",
      "blocks.18.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.18.cross_attn.k\n",
      "blocks.18.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.18.cross_attn.v.weight\n",
      "blocks.18.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.18.cross_attn.v\n",
      "blocks.18.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.18.cross_attn.o.weight\n",
      "blocks.18.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.18.cross_attn.o\n",
      "blocks.18.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.18.ffn.0.weight\n",
      "blocks.18.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.18.ffn.0\n",
      "blocks.18.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.18.ffn.2.weight\n",
      "blocks.18.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.18.ffn.2\n",
      "blocks.18.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.19.self_attn.q.weight\n",
      "blocks.19.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.19.self_attn.q\n",
      "blocks.19.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.19.self_attn.k.weight\n",
      "blocks.19.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.19.self_attn.k\n",
      "blocks.19.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.19.self_attn.v.weight\n",
      "blocks.19.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.19.self_attn.v\n",
      "blocks.19.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.19.self_attn.o.weight\n",
      "blocks.19.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.19.self_attn.o\n",
      "blocks.19.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.19.cross_attn.q.weight\n",
      "blocks.19.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.19.cross_attn.q\n",
      "blocks.19.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.19.cross_attn.k.weight\n",
      "blocks.19.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.19.cross_attn.k\n",
      "blocks.19.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.19.cross_attn.v.weight\n",
      "blocks.19.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.19.cross_attn.v\n",
      "blocks.19.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.19.cross_attn.o.weight\n",
      "blocks.19.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.19.cross_attn.o\n",
      "blocks.19.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.19.ffn.0.weight\n",
      "blocks.19.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.19.ffn.0\n",
      "blocks.19.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.19.ffn.2.weight\n",
      "blocks.19.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.19.ffn.2\n",
      "blocks.19.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.20.self_attn.q.weight\n",
      "blocks.20.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.20.self_attn.q\n",
      "blocks.20.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.20.self_attn.k.weight\n",
      "blocks.20.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.20.self_attn.k\n",
      "blocks.20.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.20.self_attn.v.weight\n",
      "blocks.20.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.20.self_attn.v\n",
      "blocks.20.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.20.self_attn.o.weight\n",
      "blocks.20.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.20.self_attn.o\n",
      "blocks.20.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.20.cross_attn.q.weight\n",
      "blocks.20.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.20.cross_attn.q\n",
      "blocks.20.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.20.cross_attn.k.weight\n",
      "blocks.20.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.20.cross_attn.k\n",
      "blocks.20.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.20.cross_attn.v.weight\n",
      "blocks.20.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.20.cross_attn.v\n",
      "blocks.20.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.20.cross_attn.o.weight\n",
      "blocks.20.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.20.cross_attn.o\n",
      "blocks.20.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.20.ffn.0.weight\n",
      "blocks.20.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.20.ffn.0\n",
      "blocks.20.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.20.ffn.2.weight\n",
      "blocks.20.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.20.ffn.2\n",
      "blocks.20.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.21.self_attn.q.weight\n",
      "blocks.21.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.21.self_attn.q\n",
      "blocks.21.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.21.self_attn.k.weight\n",
      "blocks.21.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.21.self_attn.k\n",
      "blocks.21.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.21.self_attn.v.weight\n",
      "blocks.21.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.21.self_attn.v\n",
      "blocks.21.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.21.self_attn.o.weight\n",
      "blocks.21.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.21.self_attn.o\n",
      "blocks.21.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.21.cross_attn.q.weight\n",
      "blocks.21.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.21.cross_attn.q\n",
      "blocks.21.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.21.cross_attn.k.weight\n",
      "blocks.21.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.21.cross_attn.k\n",
      "blocks.21.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.21.cross_attn.v.weight\n",
      "blocks.21.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.21.cross_attn.v\n",
      "blocks.21.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.21.cross_attn.o.weight\n",
      "blocks.21.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.21.cross_attn.o\n",
      "blocks.21.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.21.ffn.0.weight\n",
      "blocks.21.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.21.ffn.0\n",
      "blocks.21.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.21.ffn.2.weight\n",
      "blocks.21.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.21.ffn.2\n",
      "blocks.21.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.22.self_attn.q.weight\n",
      "blocks.22.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.22.self_attn.q\n",
      "blocks.22.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.22.self_attn.k.weight\n",
      "blocks.22.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.22.self_attn.k\n",
      "blocks.22.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.22.self_attn.v.weight\n",
      "blocks.22.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.22.self_attn.v\n",
      "blocks.22.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.22.self_attn.o.weight\n",
      "blocks.22.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.22.self_attn.o\n",
      "blocks.22.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.22.cross_attn.q.weight\n",
      "blocks.22.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.22.cross_attn.q\n",
      "blocks.22.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.22.cross_attn.k.weight\n",
      "blocks.22.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.22.cross_attn.k\n",
      "blocks.22.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.22.cross_attn.v.weight\n",
      "blocks.22.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.22.cross_attn.v\n",
      "blocks.22.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.22.cross_attn.o.weight\n",
      "blocks.22.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.22.cross_attn.o\n",
      "blocks.22.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.22.ffn.0.weight\n",
      "blocks.22.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.22.ffn.0\n",
      "blocks.22.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.22.ffn.2.weight\n",
      "blocks.22.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.22.ffn.2\n",
      "blocks.22.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.23.self_attn.q.weight\n",
      "blocks.23.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.23.self_attn.q\n",
      "blocks.23.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.23.self_attn.k.weight\n",
      "blocks.23.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.23.self_attn.k\n",
      "blocks.23.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.23.self_attn.v.weight\n",
      "blocks.23.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.23.self_attn.v\n",
      "blocks.23.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.23.self_attn.o.weight\n",
      "blocks.23.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.23.self_attn.o\n",
      "blocks.23.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.23.cross_attn.q.weight\n",
      "blocks.23.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.23.cross_attn.q\n",
      "blocks.23.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.23.cross_attn.k.weight\n",
      "blocks.23.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.23.cross_attn.k\n",
      "blocks.23.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.23.cross_attn.v.weight\n",
      "blocks.23.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.23.cross_attn.v\n",
      "blocks.23.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.23.cross_attn.o.weight\n",
      "blocks.23.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.23.cross_attn.o\n",
      "blocks.23.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.23.ffn.0.weight\n",
      "blocks.23.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.23.ffn.0\n",
      "blocks.23.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.23.ffn.2.weight\n",
      "blocks.23.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.23.ffn.2\n",
      "blocks.23.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.24.self_attn.q.weight\n",
      "blocks.24.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.24.self_attn.q\n",
      "blocks.24.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.24.self_attn.k.weight\n",
      "blocks.24.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.24.self_attn.k\n",
      "blocks.24.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.24.self_attn.v.weight\n",
      "blocks.24.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.24.self_attn.v\n",
      "blocks.24.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.24.self_attn.o.weight\n",
      "blocks.24.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.24.self_attn.o\n",
      "blocks.24.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.24.cross_attn.q.weight\n",
      "blocks.24.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.24.cross_attn.q\n",
      "blocks.24.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.24.cross_attn.k.weight\n",
      "blocks.24.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.24.cross_attn.k\n",
      "blocks.24.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.24.cross_attn.v.weight\n",
      "blocks.24.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.24.cross_attn.v\n",
      "blocks.24.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.24.cross_attn.o.weight\n",
      "blocks.24.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.24.cross_attn.o\n",
      "blocks.24.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.24.ffn.0.weight\n",
      "blocks.24.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.24.ffn.0\n",
      "blocks.24.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.24.ffn.2.weight\n",
      "blocks.24.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.24.ffn.2\n",
      "blocks.24.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.25.self_attn.q.weight\n",
      "blocks.25.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.25.self_attn.q\n",
      "blocks.25.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.25.self_attn.k.weight\n",
      "blocks.25.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.25.self_attn.k\n",
      "blocks.25.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.25.self_attn.v.weight\n",
      "blocks.25.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.25.self_attn.v\n",
      "blocks.25.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.25.self_attn.o.weight\n",
      "blocks.25.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.25.self_attn.o\n",
      "blocks.25.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.25.cross_attn.q.weight\n",
      "blocks.25.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.25.cross_attn.q\n",
      "blocks.25.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.25.cross_attn.k.weight\n",
      "blocks.25.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.25.cross_attn.k\n",
      "blocks.25.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.25.cross_attn.v.weight\n",
      "blocks.25.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.25.cross_attn.v\n",
      "blocks.25.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.25.cross_attn.o.weight\n",
      "blocks.25.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.25.cross_attn.o\n",
      "blocks.25.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.25.ffn.0.weight\n",
      "blocks.25.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.25.ffn.0\n",
      "blocks.25.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.25.ffn.2.weight\n",
      "blocks.25.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.25.ffn.2\n",
      "blocks.25.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.26.self_attn.q.weight\n",
      "blocks.26.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.26.self_attn.q\n",
      "blocks.26.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.26.self_attn.k.weight\n",
      "blocks.26.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.26.self_attn.k\n",
      "blocks.26.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.26.self_attn.v.weight\n",
      "blocks.26.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.26.self_attn.v\n",
      "blocks.26.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.26.self_attn.o.weight\n",
      "blocks.26.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.26.self_attn.o\n",
      "blocks.26.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.26.cross_attn.q.weight\n",
      "blocks.26.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.26.cross_attn.q\n",
      "blocks.26.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.26.cross_attn.k.weight\n",
      "blocks.26.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.26.cross_attn.k\n",
      "blocks.26.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.26.cross_attn.v.weight\n",
      "blocks.26.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.26.cross_attn.v\n",
      "blocks.26.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.26.cross_attn.o.weight\n",
      "blocks.26.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.26.cross_attn.o\n",
      "blocks.26.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.26.ffn.0.weight\n",
      "blocks.26.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.26.ffn.0\n",
      "blocks.26.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.26.ffn.2.weight\n",
      "blocks.26.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.26.ffn.2\n",
      "blocks.26.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.27.self_attn.q.weight\n",
      "blocks.27.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.27.self_attn.q\n",
      "blocks.27.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.27.self_attn.k.weight\n",
      "blocks.27.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.27.self_attn.k\n",
      "blocks.27.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.27.self_attn.v.weight\n",
      "blocks.27.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.27.self_attn.v\n",
      "blocks.27.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.27.self_attn.o.weight\n",
      "blocks.27.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.27.self_attn.o\n",
      "blocks.27.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.27.cross_attn.q.weight\n",
      "blocks.27.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.27.cross_attn.q\n",
      "blocks.27.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.27.cross_attn.k.weight\n",
      "blocks.27.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.27.cross_attn.k\n",
      "blocks.27.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.27.cross_attn.v.weight\n",
      "blocks.27.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.27.cross_attn.v\n",
      "blocks.27.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.27.cross_attn.o.weight\n",
      "blocks.27.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.27.cross_attn.o\n",
      "blocks.27.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.27.ffn.0.weight\n",
      "blocks.27.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.27.ffn.0\n",
      "blocks.27.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.27.ffn.2.weight\n",
      "blocks.27.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.27.ffn.2\n",
      "blocks.27.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.28.self_attn.q.weight\n",
      "blocks.28.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.28.self_attn.q\n",
      "blocks.28.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.28.self_attn.k.weight\n",
      "blocks.28.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.28.self_attn.k\n",
      "blocks.28.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.28.self_attn.v.weight\n",
      "blocks.28.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.28.self_attn.v\n",
      "blocks.28.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.28.self_attn.o.weight\n",
      "blocks.28.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.28.self_attn.o\n",
      "blocks.28.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.28.cross_attn.q.weight\n",
      "blocks.28.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.28.cross_attn.q\n",
      "blocks.28.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.28.cross_attn.k.weight\n",
      "blocks.28.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.28.cross_attn.k\n",
      "blocks.28.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.28.cross_attn.v.weight\n",
      "blocks.28.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.28.cross_attn.v\n",
      "blocks.28.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.28.cross_attn.o.weight\n",
      "blocks.28.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.28.cross_attn.o\n",
      "blocks.28.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.28.ffn.0.weight\n",
      "blocks.28.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.28.ffn.0\n",
      "blocks.28.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.28.ffn.2.weight\n",
      "blocks.28.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.28.ffn.2\n",
      "blocks.28.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.29.self_attn.q.weight\n",
      "blocks.29.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.29.self_attn.q\n",
      "blocks.29.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.29.self_attn.k.weight\n",
      "blocks.29.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.29.self_attn.k\n",
      "blocks.29.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.29.self_attn.v.weight\n",
      "blocks.29.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.29.self_attn.v\n",
      "blocks.29.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.29.self_attn.o.weight\n",
      "blocks.29.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.29.self_attn.o\n",
      "blocks.29.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.29.cross_attn.q.weight\n",
      "blocks.29.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.29.cross_attn.q\n",
      "blocks.29.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.29.cross_attn.k.weight\n",
      "blocks.29.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.29.cross_attn.k\n",
      "blocks.29.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.29.cross_attn.v.weight\n",
      "blocks.29.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.29.cross_attn.v\n",
      "blocks.29.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.29.cross_attn.o.weight\n",
      "blocks.29.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.29.cross_attn.o\n",
      "blocks.29.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.29.ffn.0.weight\n",
      "blocks.29.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.29.ffn.0\n",
      "blocks.29.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.29.ffn.2.weight\n",
      "blocks.29.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.29.ffn.2\n",
      "blocks.29.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.30.self_attn.q.weight\n",
      "blocks.30.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.30.self_attn.q\n",
      "blocks.30.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.30.self_attn.k.weight\n",
      "blocks.30.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.30.self_attn.k\n",
      "blocks.30.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.30.self_attn.v.weight\n",
      "blocks.30.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.30.self_attn.v\n",
      "blocks.30.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.30.self_attn.o.weight\n",
      "blocks.30.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.30.self_attn.o\n",
      "blocks.30.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.30.cross_attn.q.weight\n",
      "blocks.30.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.30.cross_attn.q\n",
      "blocks.30.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.30.cross_attn.k.weight\n",
      "blocks.30.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.30.cross_attn.k\n",
      "blocks.30.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.30.cross_attn.v.weight\n",
      "blocks.30.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.30.cross_attn.v\n",
      "blocks.30.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.30.cross_attn.o.weight\n",
      "blocks.30.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.30.cross_attn.o\n",
      "blocks.30.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.30.ffn.0.weight\n",
      "blocks.30.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.30.ffn.0\n",
      "blocks.30.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.30.ffn.2.weight\n",
      "blocks.30.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.30.ffn.2\n",
      "blocks.30.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.31.self_attn.q.weight\n",
      "blocks.31.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.31.self_attn.q\n",
      "blocks.31.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.31.self_attn.k.weight\n",
      "blocks.31.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.31.self_attn.k\n",
      "blocks.31.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.31.self_attn.v.weight\n",
      "blocks.31.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.31.self_attn.v\n",
      "blocks.31.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.31.self_attn.o.weight\n",
      "blocks.31.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.31.self_attn.o\n",
      "blocks.31.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.31.cross_attn.q.weight\n",
      "blocks.31.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.31.cross_attn.q\n",
      "blocks.31.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.31.cross_attn.k.weight\n",
      "blocks.31.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.31.cross_attn.k\n",
      "blocks.31.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.31.cross_attn.v.weight\n",
      "blocks.31.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.31.cross_attn.v\n",
      "blocks.31.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.31.cross_attn.o.weight\n",
      "blocks.31.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.31.cross_attn.o\n",
      "blocks.31.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.31.ffn.0.weight\n",
      "blocks.31.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.31.ffn.0\n",
      "blocks.31.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.31.ffn.2.weight\n",
      "blocks.31.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.31.ffn.2\n",
      "blocks.31.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.32.self_attn.q.weight\n",
      "blocks.32.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.32.self_attn.q\n",
      "blocks.32.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.32.self_attn.k.weight\n",
      "blocks.32.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.32.self_attn.k\n",
      "blocks.32.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.32.self_attn.v.weight\n",
      "blocks.32.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.32.self_attn.v\n",
      "blocks.32.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.32.self_attn.o.weight\n",
      "blocks.32.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.32.self_attn.o\n",
      "blocks.32.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.32.cross_attn.q.weight\n",
      "blocks.32.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.32.cross_attn.q\n",
      "blocks.32.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.32.cross_attn.k.weight\n",
      "blocks.32.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.32.cross_attn.k\n",
      "blocks.32.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.32.cross_attn.v.weight\n",
      "blocks.32.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.32.cross_attn.v\n",
      "blocks.32.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.32.cross_attn.o.weight\n",
      "blocks.32.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.32.cross_attn.o\n",
      "blocks.32.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.32.ffn.0.weight\n",
      "blocks.32.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.32.ffn.0\n",
      "blocks.32.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.32.ffn.2.weight\n",
      "blocks.32.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.32.ffn.2\n",
      "blocks.32.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.33.self_attn.q.weight\n",
      "blocks.33.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.33.self_attn.q\n",
      "blocks.33.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.33.self_attn.k.weight\n",
      "blocks.33.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.33.self_attn.k\n",
      "blocks.33.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.33.self_attn.v.weight\n",
      "blocks.33.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.33.self_attn.v\n",
      "blocks.33.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.33.self_attn.o.weight\n",
      "blocks.33.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.33.self_attn.o\n",
      "blocks.33.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.33.cross_attn.q.weight\n",
      "blocks.33.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.33.cross_attn.q\n",
      "blocks.33.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.33.cross_attn.k.weight\n",
      "blocks.33.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.33.cross_attn.k\n",
      "blocks.33.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.33.cross_attn.v.weight\n",
      "blocks.33.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.33.cross_attn.v\n",
      "blocks.33.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.33.cross_attn.o.weight\n",
      "blocks.33.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.33.cross_attn.o\n",
      "blocks.33.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.33.ffn.0.weight\n",
      "blocks.33.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.33.ffn.0\n",
      "blocks.33.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.33.ffn.2.weight\n",
      "blocks.33.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.33.ffn.2\n",
      "blocks.33.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.34.self_attn.q.weight\n",
      "blocks.34.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.34.self_attn.q\n",
      "blocks.34.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.34.self_attn.k.weight\n",
      "blocks.34.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.34.self_attn.k\n",
      "blocks.34.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.34.self_attn.v.weight\n",
      "blocks.34.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.34.self_attn.v\n",
      "blocks.34.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.34.self_attn.o.weight\n",
      "blocks.34.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.34.self_attn.o\n",
      "blocks.34.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.34.cross_attn.q.weight\n",
      "blocks.34.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.34.cross_attn.q\n",
      "blocks.34.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.34.cross_attn.k.weight\n",
      "blocks.34.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.34.cross_attn.k\n",
      "blocks.34.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.34.cross_attn.v.weight\n",
      "blocks.34.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.34.cross_attn.v\n",
      "blocks.34.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.34.cross_attn.o.weight\n",
      "blocks.34.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.34.cross_attn.o\n",
      "blocks.34.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.34.ffn.0.weight\n",
      "blocks.34.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.34.ffn.0\n",
      "blocks.34.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.34.ffn.2.weight\n",
      "blocks.34.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.34.ffn.2\n",
      "blocks.34.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.35.self_attn.q.weight\n",
      "blocks.35.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.35.self_attn.q\n",
      "blocks.35.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.35.self_attn.k.weight\n",
      "blocks.35.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.35.self_attn.k\n",
      "blocks.35.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.35.self_attn.v.weight\n",
      "blocks.35.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.35.self_attn.v\n",
      "blocks.35.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.35.self_attn.o.weight\n",
      "blocks.35.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.35.self_attn.o\n",
      "blocks.35.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.35.cross_attn.q.weight\n",
      "blocks.35.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.35.cross_attn.q\n",
      "blocks.35.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.35.cross_attn.k.weight\n",
      "blocks.35.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.35.cross_attn.k\n",
      "blocks.35.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.35.cross_attn.v.weight\n",
      "blocks.35.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.35.cross_attn.v\n",
      "blocks.35.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.35.cross_attn.o.weight\n",
      "blocks.35.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.35.cross_attn.o\n",
      "blocks.35.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.35.ffn.0.weight\n",
      "blocks.35.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.35.ffn.0\n",
      "blocks.35.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.35.ffn.2.weight\n",
      "blocks.35.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.35.ffn.2\n",
      "blocks.35.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.36.self_attn.q.weight\n",
      "blocks.36.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.36.self_attn.q\n",
      "blocks.36.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.36.self_attn.k.weight\n",
      "blocks.36.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.36.self_attn.k\n",
      "blocks.36.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.36.self_attn.v.weight\n",
      "blocks.36.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.36.self_attn.v\n",
      "blocks.36.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.36.self_attn.o.weight\n",
      "blocks.36.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.36.self_attn.o\n",
      "blocks.36.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.36.cross_attn.q.weight\n",
      "blocks.36.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.36.cross_attn.q\n",
      "blocks.36.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.36.cross_attn.k.weight\n",
      "blocks.36.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.36.cross_attn.k\n",
      "blocks.36.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.36.cross_attn.v.weight\n",
      "blocks.36.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.36.cross_attn.v\n",
      "blocks.36.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.36.cross_attn.o.weight\n",
      "blocks.36.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.36.cross_attn.o\n",
      "blocks.36.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.36.ffn.0.weight\n",
      "blocks.36.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.36.ffn.0\n",
      "blocks.36.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.36.ffn.2.weight\n",
      "blocks.36.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.36.ffn.2\n",
      "blocks.36.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.37.self_attn.q.weight\n",
      "blocks.37.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.37.self_attn.q\n",
      "blocks.37.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.37.self_attn.k.weight\n",
      "blocks.37.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.37.self_attn.k\n",
      "blocks.37.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.37.self_attn.v.weight\n",
      "blocks.37.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.37.self_attn.v\n",
      "blocks.37.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.37.self_attn.o.weight\n",
      "blocks.37.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.37.self_attn.o\n",
      "blocks.37.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.37.cross_attn.q.weight\n",
      "blocks.37.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.37.cross_attn.q\n",
      "blocks.37.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.37.cross_attn.k.weight\n",
      "blocks.37.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.37.cross_attn.k\n",
      "blocks.37.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.37.cross_attn.v.weight\n",
      "blocks.37.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.37.cross_attn.v\n",
      "blocks.37.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.37.cross_attn.o.weight\n",
      "blocks.37.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.37.cross_attn.o\n",
      "blocks.37.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.37.ffn.0.weight\n",
      "blocks.37.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.37.ffn.0\n",
      "blocks.37.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.37.ffn.2.weight\n",
      "blocks.37.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.37.ffn.2\n",
      "blocks.37.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.38.self_attn.q.weight\n",
      "blocks.38.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.38.self_attn.q\n",
      "blocks.38.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.38.self_attn.k.weight\n",
      "blocks.38.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.38.self_attn.k\n",
      "blocks.38.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.38.self_attn.v.weight\n",
      "blocks.38.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.38.self_attn.v\n",
      "blocks.38.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.38.self_attn.o.weight\n",
      "blocks.38.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.38.self_attn.o\n",
      "blocks.38.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.38.cross_attn.q.weight\n",
      "blocks.38.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.38.cross_attn.q\n",
      "blocks.38.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.38.cross_attn.k.weight\n",
      "blocks.38.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.38.cross_attn.k\n",
      "blocks.38.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.38.cross_attn.v.weight\n",
      "blocks.38.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.38.cross_attn.v\n",
      "blocks.38.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.38.cross_attn.o.weight\n",
      "blocks.38.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.38.cross_attn.o\n",
      "blocks.38.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.38.ffn.0.weight\n",
      "blocks.38.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.38.ffn.0\n",
      "blocks.38.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.38.ffn.2.weight\n",
      "blocks.38.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.38.ffn.2\n",
      "blocks.38.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.39.self_attn.q.weight\n",
      "blocks.39.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.39.self_attn.q\n",
      "blocks.39.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.39.self_attn.k.weight\n",
      "blocks.39.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.39.self_attn.k\n",
      "blocks.39.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.39.self_attn.v.weight\n",
      "blocks.39.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.39.self_attn.v\n",
      "blocks.39.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.39.self_attn.o.weight\n",
      "blocks.39.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.39.self_attn.o\n",
      "blocks.39.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.39.cross_attn.q.weight\n",
      "blocks.39.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.39.cross_attn.q\n",
      "blocks.39.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.39.cross_attn.k.weight\n",
      "blocks.39.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.39.cross_attn.k\n",
      "blocks.39.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.39.cross_attn.v.weight\n",
      "blocks.39.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.39.cross_attn.v\n",
      "blocks.39.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.39.cross_attn.o.weight\n",
      "blocks.39.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.39.cross_attn.o\n",
      "blocks.39.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.39.ffn.0.weight\n",
      "blocks.39.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.39.ffn.0\n",
      "blocks.39.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.39.ffn.2.weight\n",
      "blocks.39.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.39.ffn.2\n",
      "blocks.39.ffn.2 lora 추가\n",
      "blocks.0.cross_attn.k.lora_A.weight\n",
      "blocks.0.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.0.cross_attn.o.lora_A.weight\n",
      "blocks.0.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.0.cross_attn.q.lora_A.weight\n",
      "blocks.0.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.0.cross_attn.v.lora_A.weight\n",
      "blocks.0.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.0.ffn.0.lora_A.weight\n",
      "blocks.0.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.0.ffn.2.lora_A.weight\n",
      "blocks.0.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.0.self_attn.k.lora_A.weight\n",
      "blocks.0.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.0.self_attn.o.lora_A.weight\n",
      "blocks.0.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.0.self_attn.q.lora_A.weight\n",
      "blocks.0.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.0.self_attn.v.lora_A.weight\n",
      "blocks.0.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.1.cross_attn.k.lora_A.weight\n",
      "blocks.1.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.1.cross_attn.o.lora_A.weight\n",
      "blocks.1.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.1.cross_attn.q.lora_A.weight\n",
      "blocks.1.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.1.cross_attn.v.lora_A.weight\n",
      "blocks.1.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.1.ffn.0.lora_A.weight\n",
      "blocks.1.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.1.ffn.2.lora_A.weight\n",
      "blocks.1.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.1.self_attn.k.lora_A.weight\n",
      "blocks.1.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.1.self_attn.o.lora_A.weight\n",
      "blocks.1.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.1.self_attn.q.lora_A.weight\n",
      "blocks.1.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.1.self_attn.v.lora_A.weight\n",
      "blocks.1.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.2.cross_attn.k.lora_A.weight\n",
      "blocks.2.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.2.cross_attn.o.lora_A.weight\n",
      "blocks.2.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.2.cross_attn.q.lora_A.weight\n",
      "blocks.2.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.2.cross_attn.v.lora_A.weight\n",
      "blocks.2.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.2.ffn.0.lora_A.weight\n",
      "blocks.2.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.2.ffn.2.lora_A.weight\n",
      "blocks.2.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.2.self_attn.k.lora_A.weight\n",
      "blocks.2.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.2.self_attn.o.lora_A.weight\n",
      "blocks.2.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.2.self_attn.q.lora_A.weight\n",
      "blocks.2.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.2.self_attn.v.lora_A.weight\n",
      "blocks.2.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.3.cross_attn.k.lora_A.weight\n",
      "blocks.3.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.3.cross_attn.o.lora_A.weight\n",
      "blocks.3.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.3.cross_attn.q.lora_A.weight\n",
      "blocks.3.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.3.cross_attn.v.lora_A.weight\n",
      "blocks.3.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.3.ffn.0.lora_A.weight\n",
      "blocks.3.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.3.ffn.2.lora_A.weight\n",
      "blocks.3.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.3.self_attn.k.lora_A.weight\n",
      "blocks.3.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.3.self_attn.o.lora_A.weight\n",
      "blocks.3.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.3.self_attn.q.lora_A.weight\n",
      "blocks.3.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.3.self_attn.v.lora_A.weight\n",
      "blocks.3.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.4.cross_attn.k.lora_A.weight\n",
      "blocks.4.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.4.cross_attn.o.lora_A.weight\n",
      "blocks.4.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.4.cross_attn.q.lora_A.weight\n",
      "blocks.4.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.4.cross_attn.v.lora_A.weight\n",
      "blocks.4.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.4.ffn.0.lora_A.weight\n",
      "blocks.4.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.4.ffn.2.lora_A.weight\n",
      "blocks.4.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.4.self_attn.k.lora_A.weight\n",
      "blocks.4.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.4.self_attn.o.lora_A.weight\n",
      "blocks.4.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.4.self_attn.q.lora_A.weight\n",
      "blocks.4.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.4.self_attn.v.lora_A.weight\n",
      "blocks.4.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.5.cross_attn.k.lora_A.weight\n",
      "blocks.5.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.5.cross_attn.o.lora_A.weight\n",
      "blocks.5.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.5.cross_attn.q.lora_A.weight\n",
      "blocks.5.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.5.cross_attn.v.lora_A.weight\n",
      "blocks.5.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.5.ffn.0.lora_A.weight\n",
      "blocks.5.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.5.ffn.2.lora_A.weight\n",
      "blocks.5.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.5.self_attn.k.lora_A.weight\n",
      "blocks.5.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.5.self_attn.o.lora_A.weight\n",
      "blocks.5.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.5.self_attn.q.lora_A.weight\n",
      "blocks.5.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.5.self_attn.v.lora_A.weight\n",
      "blocks.5.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.6.cross_attn.k.lora_A.weight\n",
      "blocks.6.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.6.cross_attn.o.lora_A.weight\n",
      "blocks.6.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.6.cross_attn.q.lora_A.weight\n",
      "blocks.6.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.6.cross_attn.v.lora_A.weight\n",
      "blocks.6.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.6.ffn.0.lora_A.weight\n",
      "blocks.6.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.6.ffn.2.lora_A.weight\n",
      "blocks.6.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.6.self_attn.k.lora_A.weight\n",
      "blocks.6.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.6.self_attn.o.lora_A.weight\n",
      "blocks.6.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.6.self_attn.q.lora_A.weight\n",
      "blocks.6.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.6.self_attn.v.lora_A.weight\n",
      "blocks.6.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.7.cross_attn.k.lora_A.weight\n",
      "blocks.7.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.7.cross_attn.o.lora_A.weight\n",
      "blocks.7.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.7.cross_attn.q.lora_A.weight\n",
      "blocks.7.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.7.cross_attn.v.lora_A.weight\n",
      "blocks.7.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.7.ffn.0.lora_A.weight\n",
      "blocks.7.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.7.ffn.2.lora_A.weight\n",
      "blocks.7.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.7.self_attn.k.lora_A.weight\n",
      "blocks.7.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.7.self_attn.o.lora_A.weight\n",
      "blocks.7.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.7.self_attn.q.lora_A.weight\n",
      "blocks.7.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.7.self_attn.v.lora_A.weight\n",
      "blocks.7.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.8.cross_attn.k.lora_A.weight\n",
      "blocks.8.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.8.cross_attn.o.lora_A.weight\n",
      "blocks.8.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.8.cross_attn.q.lora_A.weight\n",
      "blocks.8.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.8.cross_attn.v.lora_A.weight\n",
      "blocks.8.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.8.ffn.0.lora_A.weight\n",
      "blocks.8.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.8.ffn.2.lora_A.weight\n",
      "blocks.8.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.8.self_attn.k.lora_A.weight\n",
      "blocks.8.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.8.self_attn.o.lora_A.weight\n",
      "blocks.8.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.8.self_attn.q.lora_A.weight\n",
      "blocks.8.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.8.self_attn.v.lora_A.weight\n",
      "blocks.8.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.9.cross_attn.k.lora_A.weight\n",
      "blocks.9.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.9.cross_attn.o.lora_A.weight\n",
      "blocks.9.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.9.cross_attn.q.lora_A.weight\n",
      "blocks.9.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.9.cross_attn.v.lora_A.weight\n",
      "blocks.9.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.9.ffn.0.lora_A.weight\n",
      "blocks.9.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.9.ffn.2.lora_A.weight\n",
      "blocks.9.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.9.self_attn.k.lora_A.weight\n",
      "blocks.9.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.9.self_attn.o.lora_A.weight\n",
      "blocks.9.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.9.self_attn.q.lora_A.weight\n",
      "blocks.9.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.9.self_attn.v.lora_A.weight\n",
      "blocks.9.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.10.cross_attn.k.lora_A.weight\n",
      "blocks.10.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.10.cross_attn.o.lora_A.weight\n",
      "blocks.10.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.10.cross_attn.q.lora_A.weight\n",
      "blocks.10.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.10.cross_attn.v.lora_A.weight\n",
      "blocks.10.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.10.ffn.0.lora_A.weight\n",
      "blocks.10.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.10.ffn.2.lora_A.weight\n",
      "blocks.10.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.10.self_attn.k.lora_A.weight\n",
      "blocks.10.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.10.self_attn.o.lora_A.weight\n",
      "blocks.10.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.10.self_attn.q.lora_A.weight\n",
      "blocks.10.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.10.self_attn.v.lora_A.weight\n",
      "blocks.10.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.11.cross_attn.k.lora_A.weight\n",
      "blocks.11.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.11.cross_attn.o.lora_A.weight\n",
      "blocks.11.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.11.cross_attn.q.lora_A.weight\n",
      "blocks.11.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.11.cross_attn.v.lora_A.weight\n",
      "blocks.11.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.11.ffn.0.lora_A.weight\n",
      "blocks.11.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.11.ffn.2.lora_A.weight\n",
      "blocks.11.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.11.self_attn.k.lora_A.weight\n",
      "blocks.11.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.11.self_attn.o.lora_A.weight\n",
      "blocks.11.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.11.self_attn.q.lora_A.weight\n",
      "blocks.11.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.11.self_attn.v.lora_A.weight\n",
      "blocks.11.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.12.cross_attn.k.lora_A.weight\n",
      "blocks.12.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.12.cross_attn.o.lora_A.weight\n",
      "blocks.12.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.12.cross_attn.q.lora_A.weight\n",
      "blocks.12.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.12.cross_attn.v.lora_A.weight\n",
      "blocks.12.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.12.ffn.0.lora_A.weight\n",
      "blocks.12.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.12.ffn.2.lora_A.weight\n",
      "blocks.12.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.12.self_attn.k.lora_A.weight\n",
      "blocks.12.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.12.self_attn.o.lora_A.weight\n",
      "blocks.12.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.12.self_attn.q.lora_A.weight\n",
      "blocks.12.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.12.self_attn.v.lora_A.weight\n",
      "blocks.12.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.13.cross_attn.k.lora_A.weight\n",
      "blocks.13.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.13.cross_attn.o.lora_A.weight\n",
      "blocks.13.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.13.cross_attn.q.lora_A.weight\n",
      "blocks.13.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.13.cross_attn.v.lora_A.weight\n",
      "blocks.13.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.13.ffn.0.lora_A.weight\n",
      "blocks.13.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.13.ffn.2.lora_A.weight\n",
      "blocks.13.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.13.self_attn.k.lora_A.weight\n",
      "blocks.13.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.13.self_attn.o.lora_A.weight\n",
      "blocks.13.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.13.self_attn.q.lora_A.weight\n",
      "blocks.13.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.13.self_attn.v.lora_A.weight\n",
      "blocks.13.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.14.cross_attn.k.lora_A.weight\n",
      "blocks.14.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.14.cross_attn.o.lora_A.weight\n",
      "blocks.14.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.14.cross_attn.q.lora_A.weight\n",
      "blocks.14.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.14.cross_attn.v.lora_A.weight\n",
      "blocks.14.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.14.ffn.0.lora_A.weight\n",
      "blocks.14.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.14.ffn.2.lora_A.weight\n",
      "blocks.14.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.14.self_attn.k.lora_A.weight\n",
      "blocks.14.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.14.self_attn.o.lora_A.weight\n",
      "blocks.14.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.14.self_attn.q.lora_A.weight\n",
      "blocks.14.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.14.self_attn.v.lora_A.weight\n",
      "blocks.14.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.15.cross_attn.k.lora_A.weight\n",
      "blocks.15.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.15.cross_attn.o.lora_A.weight\n",
      "blocks.15.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.15.cross_attn.q.lora_A.weight\n",
      "blocks.15.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.15.cross_attn.v.lora_A.weight\n",
      "blocks.15.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.15.ffn.0.lora_A.weight\n",
      "blocks.15.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.15.ffn.2.lora_A.weight\n",
      "blocks.15.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.15.self_attn.k.lora_A.weight\n",
      "blocks.15.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.15.self_attn.o.lora_A.weight\n",
      "blocks.15.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.15.self_attn.q.lora_A.weight\n",
      "blocks.15.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.15.self_attn.v.lora_A.weight\n",
      "blocks.15.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.16.cross_attn.k.lora_A.weight\n",
      "blocks.16.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.16.cross_attn.o.lora_A.weight\n",
      "blocks.16.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.16.cross_attn.q.lora_A.weight\n",
      "blocks.16.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.16.cross_attn.v.lora_A.weight\n",
      "blocks.16.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.16.ffn.0.lora_A.weight\n",
      "blocks.16.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.16.ffn.2.lora_A.weight\n",
      "blocks.16.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.16.self_attn.k.lora_A.weight\n",
      "blocks.16.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.16.self_attn.o.lora_A.weight\n",
      "blocks.16.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.16.self_attn.q.lora_A.weight\n",
      "blocks.16.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.16.self_attn.v.lora_A.weight\n",
      "blocks.16.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.17.cross_attn.k.lora_A.weight\n",
      "blocks.17.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.17.cross_attn.o.lora_A.weight\n",
      "blocks.17.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.17.cross_attn.q.lora_A.weight\n",
      "blocks.17.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.17.cross_attn.v.lora_A.weight\n",
      "blocks.17.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.17.ffn.0.lora_A.weight\n",
      "blocks.17.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.17.ffn.2.lora_A.weight\n",
      "blocks.17.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.17.self_attn.k.lora_A.weight\n",
      "blocks.17.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.17.self_attn.o.lora_A.weight\n",
      "blocks.17.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.17.self_attn.q.lora_A.weight\n",
      "blocks.17.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.17.self_attn.v.lora_A.weight\n",
      "blocks.17.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.18.cross_attn.k.lora_A.weight\n",
      "blocks.18.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.18.cross_attn.o.lora_A.weight\n",
      "blocks.18.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.18.cross_attn.q.lora_A.weight\n",
      "blocks.18.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.18.cross_attn.v.lora_A.weight\n",
      "blocks.18.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.18.ffn.0.lora_A.weight\n",
      "blocks.18.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.18.ffn.2.lora_A.weight\n",
      "blocks.18.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.18.self_attn.k.lora_A.weight\n",
      "blocks.18.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.18.self_attn.o.lora_A.weight\n",
      "blocks.18.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.18.self_attn.q.lora_A.weight\n",
      "blocks.18.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.18.self_attn.v.lora_A.weight\n",
      "blocks.18.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.19.cross_attn.k.lora_A.weight\n",
      "blocks.19.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.19.cross_attn.o.lora_A.weight\n",
      "blocks.19.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.19.cross_attn.q.lora_A.weight\n",
      "blocks.19.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.19.cross_attn.v.lora_A.weight\n",
      "blocks.19.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.19.ffn.0.lora_A.weight\n",
      "blocks.19.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.19.ffn.2.lora_A.weight\n",
      "blocks.19.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.19.self_attn.k.lora_A.weight\n",
      "blocks.19.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.19.self_attn.o.lora_A.weight\n",
      "blocks.19.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.19.self_attn.q.lora_A.weight\n",
      "blocks.19.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.19.self_attn.v.lora_A.weight\n",
      "blocks.19.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.20.cross_attn.k.lora_A.weight\n",
      "blocks.20.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.20.cross_attn.o.lora_A.weight\n",
      "blocks.20.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.20.cross_attn.q.lora_A.weight\n",
      "blocks.20.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.20.cross_attn.v.lora_A.weight\n",
      "blocks.20.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.20.ffn.0.lora_A.weight\n",
      "blocks.20.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.20.ffn.2.lora_A.weight\n",
      "blocks.20.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.20.self_attn.k.lora_A.weight\n",
      "blocks.20.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.20.self_attn.o.lora_A.weight\n",
      "blocks.20.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.20.self_attn.q.lora_A.weight\n",
      "blocks.20.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.20.self_attn.v.lora_A.weight\n",
      "blocks.20.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.21.cross_attn.k.lora_A.weight\n",
      "blocks.21.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.21.cross_attn.o.lora_A.weight\n",
      "blocks.21.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.21.cross_attn.q.lora_A.weight\n",
      "blocks.21.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.21.cross_attn.v.lora_A.weight\n",
      "blocks.21.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.21.ffn.0.lora_A.weight\n",
      "blocks.21.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.21.ffn.2.lora_A.weight\n",
      "blocks.21.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.21.self_attn.k.lora_A.weight\n",
      "blocks.21.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.21.self_attn.o.lora_A.weight\n",
      "blocks.21.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.21.self_attn.q.lora_A.weight\n",
      "blocks.21.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.21.self_attn.v.lora_A.weight\n",
      "blocks.21.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.22.cross_attn.k.lora_A.weight\n",
      "blocks.22.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.22.cross_attn.o.lora_A.weight\n",
      "blocks.22.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.22.cross_attn.q.lora_A.weight\n",
      "blocks.22.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.22.cross_attn.v.lora_A.weight\n",
      "blocks.22.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.22.ffn.0.lora_A.weight\n",
      "blocks.22.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.22.ffn.2.lora_A.weight\n",
      "blocks.22.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.22.self_attn.k.lora_A.weight\n",
      "blocks.22.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.22.self_attn.o.lora_A.weight\n",
      "blocks.22.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.22.self_attn.q.lora_A.weight\n",
      "blocks.22.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.22.self_attn.v.lora_A.weight\n",
      "blocks.22.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.23.cross_attn.k.lora_A.weight\n",
      "blocks.23.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.23.cross_attn.o.lora_A.weight\n",
      "blocks.23.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.23.cross_attn.q.lora_A.weight\n",
      "blocks.23.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.23.cross_attn.v.lora_A.weight\n",
      "blocks.23.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.23.ffn.0.lora_A.weight\n",
      "blocks.23.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.23.ffn.2.lora_A.weight\n",
      "blocks.23.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.23.self_attn.k.lora_A.weight\n",
      "blocks.23.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.23.self_attn.o.lora_A.weight\n",
      "blocks.23.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.23.self_attn.q.lora_A.weight\n",
      "blocks.23.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.23.self_attn.v.lora_A.weight\n",
      "blocks.23.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.24.cross_attn.k.lora_A.weight\n",
      "blocks.24.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.24.cross_attn.o.lora_A.weight\n",
      "blocks.24.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.24.cross_attn.q.lora_A.weight\n",
      "blocks.24.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.24.cross_attn.v.lora_A.weight\n",
      "blocks.24.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.24.ffn.0.lora_A.weight\n",
      "blocks.24.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.24.ffn.2.lora_A.weight\n",
      "blocks.24.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.24.self_attn.k.lora_A.weight\n",
      "blocks.24.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.24.self_attn.o.lora_A.weight\n",
      "blocks.24.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.24.self_attn.q.lora_A.weight\n",
      "blocks.24.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.24.self_attn.v.lora_A.weight\n",
      "blocks.24.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.25.cross_attn.k.lora_A.weight\n",
      "blocks.25.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.25.cross_attn.o.lora_A.weight\n",
      "blocks.25.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.25.cross_attn.q.lora_A.weight\n",
      "blocks.25.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.25.cross_attn.v.lora_A.weight\n",
      "blocks.25.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.25.ffn.0.lora_A.weight\n",
      "blocks.25.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.25.ffn.2.lora_A.weight\n",
      "blocks.25.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.25.self_attn.k.lora_A.weight\n",
      "blocks.25.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.25.self_attn.o.lora_A.weight\n",
      "blocks.25.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.25.self_attn.q.lora_A.weight\n",
      "blocks.25.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.25.self_attn.v.lora_A.weight\n",
      "blocks.25.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.26.cross_attn.k.lora_A.weight\n",
      "blocks.26.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.26.cross_attn.o.lora_A.weight\n",
      "blocks.26.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.26.cross_attn.q.lora_A.weight\n",
      "blocks.26.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.26.cross_attn.v.lora_A.weight\n",
      "blocks.26.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.26.ffn.0.lora_A.weight\n",
      "blocks.26.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.26.ffn.2.lora_A.weight\n",
      "blocks.26.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.26.self_attn.k.lora_A.weight\n",
      "blocks.26.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.26.self_attn.o.lora_A.weight\n",
      "blocks.26.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.26.self_attn.q.lora_A.weight\n",
      "blocks.26.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.26.self_attn.v.lora_A.weight\n",
      "blocks.26.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.27.cross_attn.k.lora_A.weight\n",
      "blocks.27.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.27.cross_attn.o.lora_A.weight\n",
      "blocks.27.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.27.cross_attn.q.lora_A.weight\n",
      "blocks.27.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.27.cross_attn.v.lora_A.weight\n",
      "blocks.27.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.27.ffn.0.lora_A.weight\n",
      "blocks.27.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.27.ffn.2.lora_A.weight\n",
      "blocks.27.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.27.self_attn.k.lora_A.weight\n",
      "blocks.27.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.27.self_attn.o.lora_A.weight\n",
      "blocks.27.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.27.self_attn.q.lora_A.weight\n",
      "blocks.27.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.27.self_attn.v.lora_A.weight\n",
      "blocks.27.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.28.cross_attn.k.lora_A.weight\n",
      "blocks.28.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.28.cross_attn.o.lora_A.weight\n",
      "blocks.28.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.28.cross_attn.q.lora_A.weight\n",
      "blocks.28.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.28.cross_attn.v.lora_A.weight\n",
      "blocks.28.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.28.ffn.0.lora_A.weight\n",
      "blocks.28.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.28.ffn.2.lora_A.weight\n",
      "blocks.28.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.28.self_attn.k.lora_A.weight\n",
      "blocks.28.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.28.self_attn.o.lora_A.weight\n",
      "blocks.28.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.28.self_attn.q.lora_A.weight\n",
      "blocks.28.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.28.self_attn.v.lora_A.weight\n",
      "blocks.28.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.29.cross_attn.k.lora_A.weight\n",
      "blocks.29.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.29.cross_attn.o.lora_A.weight\n",
      "blocks.29.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.29.cross_attn.q.lora_A.weight\n",
      "blocks.29.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.29.cross_attn.v.lora_A.weight\n",
      "blocks.29.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.29.ffn.0.lora_A.weight\n",
      "blocks.29.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.29.ffn.2.lora_A.weight\n",
      "blocks.29.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.29.self_attn.k.lora_A.weight\n",
      "blocks.29.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.29.self_attn.o.lora_A.weight\n",
      "blocks.29.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.29.self_attn.q.lora_A.weight\n",
      "blocks.29.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.29.self_attn.v.lora_A.weight\n",
      "blocks.29.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.30.cross_attn.k.lora_A.weight\n",
      "blocks.30.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.30.cross_attn.o.lora_A.weight\n",
      "blocks.30.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.30.cross_attn.q.lora_A.weight\n",
      "blocks.30.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.30.cross_attn.v.lora_A.weight\n",
      "blocks.30.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.30.ffn.0.lora_A.weight\n",
      "blocks.30.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.30.ffn.2.lora_A.weight\n",
      "blocks.30.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.30.self_attn.k.lora_A.weight\n",
      "blocks.30.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.30.self_attn.o.lora_A.weight\n",
      "blocks.30.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.30.self_attn.q.lora_A.weight\n",
      "blocks.30.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.30.self_attn.v.lora_A.weight\n",
      "blocks.30.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.31.cross_attn.k.lora_A.weight\n",
      "blocks.31.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.31.cross_attn.o.lora_A.weight\n",
      "blocks.31.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.31.cross_attn.q.lora_A.weight\n",
      "blocks.31.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.31.cross_attn.v.lora_A.weight\n",
      "blocks.31.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.31.ffn.0.lora_A.weight\n",
      "blocks.31.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.31.ffn.2.lora_A.weight\n",
      "blocks.31.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.31.self_attn.k.lora_A.weight\n",
      "blocks.31.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.31.self_attn.o.lora_A.weight\n",
      "blocks.31.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.31.self_attn.q.lora_A.weight\n",
      "blocks.31.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.31.self_attn.v.lora_A.weight\n",
      "blocks.31.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.32.cross_attn.k.lora_A.weight\n",
      "blocks.32.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.32.cross_attn.o.lora_A.weight\n",
      "blocks.32.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.32.cross_attn.q.lora_A.weight\n",
      "blocks.32.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.32.cross_attn.v.lora_A.weight\n",
      "blocks.32.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.32.ffn.0.lora_A.weight\n",
      "blocks.32.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.32.ffn.2.lora_A.weight\n",
      "blocks.32.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.32.self_attn.k.lora_A.weight\n",
      "blocks.32.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.32.self_attn.o.lora_A.weight\n",
      "blocks.32.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.32.self_attn.q.lora_A.weight\n",
      "blocks.32.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.32.self_attn.v.lora_A.weight\n",
      "blocks.32.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.33.cross_attn.k.lora_A.weight\n",
      "blocks.33.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.33.cross_attn.o.lora_A.weight\n",
      "blocks.33.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.33.cross_attn.q.lora_A.weight\n",
      "blocks.33.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.33.cross_attn.v.lora_A.weight\n",
      "blocks.33.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.33.ffn.0.lora_A.weight\n",
      "blocks.33.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.33.ffn.2.lora_A.weight\n",
      "blocks.33.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.33.self_attn.k.lora_A.weight\n",
      "blocks.33.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.33.self_attn.o.lora_A.weight\n",
      "blocks.33.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.33.self_attn.q.lora_A.weight\n",
      "blocks.33.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.33.self_attn.v.lora_A.weight\n",
      "blocks.33.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.34.cross_attn.k.lora_A.weight\n",
      "blocks.34.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.34.cross_attn.o.lora_A.weight\n",
      "blocks.34.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.34.cross_attn.q.lora_A.weight\n",
      "blocks.34.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.34.cross_attn.v.lora_A.weight\n",
      "blocks.34.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.34.ffn.0.lora_A.weight\n",
      "blocks.34.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.34.ffn.2.lora_A.weight\n",
      "blocks.34.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.34.self_attn.k.lora_A.weight\n",
      "blocks.34.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.34.self_attn.o.lora_A.weight\n",
      "blocks.34.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.34.self_attn.q.lora_A.weight\n",
      "blocks.34.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.34.self_attn.v.lora_A.weight\n",
      "blocks.34.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.35.cross_attn.k.lora_A.weight\n",
      "blocks.35.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.35.cross_attn.o.lora_A.weight\n",
      "blocks.35.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.35.cross_attn.q.lora_A.weight\n",
      "blocks.35.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.35.cross_attn.v.lora_A.weight\n",
      "blocks.35.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.35.ffn.0.lora_A.weight\n",
      "blocks.35.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.35.ffn.2.lora_A.weight\n",
      "blocks.35.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.35.self_attn.k.lora_A.weight\n",
      "blocks.35.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.35.self_attn.o.lora_A.weight\n",
      "blocks.35.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.35.self_attn.q.lora_A.weight\n",
      "blocks.35.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.35.self_attn.v.lora_A.weight\n",
      "blocks.35.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.36.cross_attn.k.lora_A.weight\n",
      "blocks.36.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.36.cross_attn.o.lora_A.weight\n",
      "blocks.36.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.36.cross_attn.q.lora_A.weight\n",
      "blocks.36.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.36.cross_attn.v.lora_A.weight\n",
      "blocks.36.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.36.ffn.0.lora_A.weight\n",
      "blocks.36.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.36.ffn.2.lora_A.weight\n",
      "blocks.36.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.36.self_attn.k.lora_A.weight\n",
      "blocks.36.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.36.self_attn.o.lora_A.weight\n",
      "blocks.36.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.36.self_attn.q.lora_A.weight\n",
      "blocks.36.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.36.self_attn.v.lora_A.weight\n",
      "blocks.36.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.37.cross_attn.k.lora_A.weight\n",
      "blocks.37.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.37.cross_attn.o.lora_A.weight\n",
      "blocks.37.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.37.cross_attn.q.lora_A.weight\n",
      "blocks.37.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.37.cross_attn.v.lora_A.weight\n",
      "blocks.37.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.37.ffn.0.lora_A.weight\n",
      "blocks.37.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.37.ffn.2.lora_A.weight\n",
      "blocks.37.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.37.self_attn.k.lora_A.weight\n",
      "blocks.37.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.37.self_attn.o.lora_A.weight\n",
      "blocks.37.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.37.self_attn.q.lora_A.weight\n",
      "blocks.37.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.37.self_attn.v.lora_A.weight\n",
      "blocks.37.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.38.cross_attn.k.lora_A.weight\n",
      "blocks.38.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.38.cross_attn.o.lora_A.weight\n",
      "blocks.38.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.38.cross_attn.q.lora_A.weight\n",
      "blocks.38.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.38.cross_attn.v.lora_A.weight\n",
      "blocks.38.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.38.ffn.0.lora_A.weight\n",
      "blocks.38.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.38.ffn.2.lora_A.weight\n",
      "blocks.38.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.38.self_attn.k.lora_A.weight\n",
      "blocks.38.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.38.self_attn.o.lora_A.weight\n",
      "blocks.38.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.38.self_attn.q.lora_A.weight\n",
      "blocks.38.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.38.self_attn.v.lora_A.weight\n",
      "blocks.38.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.39.cross_attn.k.lora_A.weight\n",
      "blocks.39.cross_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.39.cross_attn.o.lora_A.weight\n",
      "blocks.39.cross_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.39.cross_attn.q.lora_A.weight\n",
      "blocks.39.cross_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.39.cross_attn.v.lora_A.weight\n",
      "blocks.39.cross_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.39.ffn.0.lora_A.weight\n",
      "blocks.39.ffn.0.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.39.ffn.2.lora_A.weight\n",
      "blocks.39.ffn.2.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.39.self_attn.k.lora_A.weight\n",
      "blocks.39.self_attn.k.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.39.self_attn.o.lora_A.weight\n",
      "blocks.39.self_attn.o.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.39.self_attn.q.lora_A.weight\n",
      "blocks.39.self_attn.q.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "blocks.39.self_attn.v.lora_A.weight\n",
      "blocks.39.self_attn.v.lora_A.weight\n",
      "lora_A 존재 x\n",
      "lora_A 존재 x\n",
      "lora_pairs1 keys:dict_keys(['blocks.0.cross_attn.k', 'blocks.0.cross_attn.o', 'blocks.0.cross_attn.q', 'blocks.0.cross_attn.v', 'blocks.0.ffn.0', 'blocks.0.ffn.2', 'blocks.0.self_attn.k', 'blocks.0.self_attn.o', 'blocks.0.self_attn.q', 'blocks.0.self_attn.v', 'blocks.1.cross_attn.k', 'blocks.1.cross_attn.o', 'blocks.1.cross_attn.q', 'blocks.1.cross_attn.v', 'blocks.1.ffn.0', 'blocks.1.ffn.2', 'blocks.1.self_attn.k', 'blocks.1.self_attn.o', 'blocks.1.self_attn.q', 'blocks.1.self_attn.v', 'blocks.2.cross_attn.k', 'blocks.2.cross_attn.o', 'blocks.2.cross_attn.q', 'blocks.2.cross_attn.v', 'blocks.2.ffn.0', 'blocks.2.ffn.2', 'blocks.2.self_attn.k', 'blocks.2.self_attn.o', 'blocks.2.self_attn.q', 'blocks.2.self_attn.v', 'blocks.3.cross_attn.k', 'blocks.3.cross_attn.o', 'blocks.3.cross_attn.q', 'blocks.3.cross_attn.v', 'blocks.3.ffn.0', 'blocks.3.ffn.2', 'blocks.3.self_attn.k', 'blocks.3.self_attn.o', 'blocks.3.self_attn.q', 'blocks.3.self_attn.v', 'blocks.4.cross_attn.k', 'blocks.4.cross_attn.o', 'blocks.4.cross_attn.q', 'blocks.4.cross_attn.v', 'blocks.4.ffn.0', 'blocks.4.ffn.2', 'blocks.4.self_attn.k', 'blocks.4.self_attn.o', 'blocks.4.self_attn.q', 'blocks.4.self_attn.v', 'blocks.5.cross_attn.k', 'blocks.5.cross_attn.o', 'blocks.5.cross_attn.q', 'blocks.5.cross_attn.v', 'blocks.5.ffn.0', 'blocks.5.ffn.2', 'blocks.5.self_attn.k', 'blocks.5.self_attn.o', 'blocks.5.self_attn.q', 'blocks.5.self_attn.v', 'blocks.6.cross_attn.k', 'blocks.6.cross_attn.o', 'blocks.6.cross_attn.q', 'blocks.6.cross_attn.v', 'blocks.6.ffn.0', 'blocks.6.ffn.2', 'blocks.6.self_attn.k', 'blocks.6.self_attn.o', 'blocks.6.self_attn.q', 'blocks.6.self_attn.v', 'blocks.7.cross_attn.k', 'blocks.7.cross_attn.o', 'blocks.7.cross_attn.q', 'blocks.7.cross_attn.v', 'blocks.7.ffn.0', 'blocks.7.ffn.2', 'blocks.7.self_attn.k', 'blocks.7.self_attn.o', 'blocks.7.self_attn.q', 'blocks.7.self_attn.v', 'blocks.8.cross_attn.k', 'blocks.8.cross_attn.o', 'blocks.8.cross_attn.q', 'blocks.8.cross_attn.v', 'blocks.8.ffn.0', 'blocks.8.ffn.2', 'blocks.8.self_attn.k', 'blocks.8.self_attn.o', 'blocks.8.self_attn.q', 'blocks.8.self_attn.v', 'blocks.9.cross_attn.k', 'blocks.9.cross_attn.o', 'blocks.9.cross_attn.q', 'blocks.9.cross_attn.v', 'blocks.9.ffn.0', 'blocks.9.ffn.2', 'blocks.9.self_attn.k', 'blocks.9.self_attn.o', 'blocks.9.self_attn.q', 'blocks.9.self_attn.v', 'blocks.10.cross_attn.k', 'blocks.10.cross_attn.o', 'blocks.10.cross_attn.q', 'blocks.10.cross_attn.v', 'blocks.10.ffn.0', 'blocks.10.ffn.2', 'blocks.10.self_attn.k', 'blocks.10.self_attn.o', 'blocks.10.self_attn.q', 'blocks.10.self_attn.v', 'blocks.11.cross_attn.k', 'blocks.11.cross_attn.o', 'blocks.11.cross_attn.q', 'blocks.11.cross_attn.v', 'blocks.11.ffn.0', 'blocks.11.ffn.2', 'blocks.11.self_attn.k', 'blocks.11.self_attn.o', 'blocks.11.self_attn.q', 'blocks.11.self_attn.v', 'blocks.12.cross_attn.k', 'blocks.12.cross_attn.o', 'blocks.12.cross_attn.q', 'blocks.12.cross_attn.v', 'blocks.12.ffn.0', 'blocks.12.ffn.2', 'blocks.12.self_attn.k', 'blocks.12.self_attn.o', 'blocks.12.self_attn.q', 'blocks.12.self_attn.v', 'blocks.13.cross_attn.k', 'blocks.13.cross_attn.o', 'blocks.13.cross_attn.q', 'blocks.13.cross_attn.v', 'blocks.13.ffn.0', 'blocks.13.ffn.2', 'blocks.13.self_attn.k', 'blocks.13.self_attn.o', 'blocks.13.self_attn.q', 'blocks.13.self_attn.v', 'blocks.14.cross_attn.k', 'blocks.14.cross_attn.o', 'blocks.14.cross_attn.q', 'blocks.14.cross_attn.v', 'blocks.14.ffn.0', 'blocks.14.ffn.2', 'blocks.14.self_attn.k', 'blocks.14.self_attn.o', 'blocks.14.self_attn.q', 'blocks.14.self_attn.v', 'blocks.15.cross_attn.k', 'blocks.15.cross_attn.o', 'blocks.15.cross_attn.q', 'blocks.15.cross_attn.v', 'blocks.15.ffn.0', 'blocks.15.ffn.2', 'blocks.15.self_attn.k', 'blocks.15.self_attn.o', 'blocks.15.self_attn.q', 'blocks.15.self_attn.v', 'blocks.16.cross_attn.k', 'blocks.16.cross_attn.o', 'blocks.16.cross_attn.q', 'blocks.16.cross_attn.v', 'blocks.16.ffn.0', 'blocks.16.ffn.2', 'blocks.16.self_attn.k', 'blocks.16.self_attn.o', 'blocks.16.self_attn.q', 'blocks.16.self_attn.v', 'blocks.17.cross_attn.k', 'blocks.17.cross_attn.o', 'blocks.17.cross_attn.q', 'blocks.17.cross_attn.v', 'blocks.17.ffn.0', 'blocks.17.ffn.2', 'blocks.17.self_attn.k', 'blocks.17.self_attn.o', 'blocks.17.self_attn.q', 'blocks.17.self_attn.v', 'blocks.18.cross_attn.k', 'blocks.18.cross_attn.o', 'blocks.18.cross_attn.q', 'blocks.18.cross_attn.v', 'blocks.18.ffn.0', 'blocks.18.ffn.2', 'blocks.18.self_attn.k', 'blocks.18.self_attn.o', 'blocks.18.self_attn.q', 'blocks.18.self_attn.v', 'blocks.19.cross_attn.k', 'blocks.19.cross_attn.o', 'blocks.19.cross_attn.q', 'blocks.19.cross_attn.v', 'blocks.19.ffn.0', 'blocks.19.ffn.2', 'blocks.19.self_attn.k', 'blocks.19.self_attn.o', 'blocks.19.self_attn.q', 'blocks.19.self_attn.v', 'blocks.20.cross_attn.k', 'blocks.20.cross_attn.o', 'blocks.20.cross_attn.q', 'blocks.20.cross_attn.v', 'blocks.20.ffn.0', 'blocks.20.ffn.2', 'blocks.20.self_attn.k', 'blocks.20.self_attn.o', 'blocks.20.self_attn.q', 'blocks.20.self_attn.v', 'blocks.21.cross_attn.k', 'blocks.21.cross_attn.o', 'blocks.21.cross_attn.q', 'blocks.21.cross_attn.v', 'blocks.21.ffn.0', 'blocks.21.ffn.2', 'blocks.21.self_attn.k', 'blocks.21.self_attn.o', 'blocks.21.self_attn.q', 'blocks.21.self_attn.v', 'blocks.22.cross_attn.k', 'blocks.22.cross_attn.o', 'blocks.22.cross_attn.q', 'blocks.22.cross_attn.v', 'blocks.22.ffn.0', 'blocks.22.ffn.2', 'blocks.22.self_attn.k', 'blocks.22.self_attn.o', 'blocks.22.self_attn.q', 'blocks.22.self_attn.v', 'blocks.23.cross_attn.k', 'blocks.23.cross_attn.o', 'blocks.23.cross_attn.q', 'blocks.23.cross_attn.v', 'blocks.23.ffn.0', 'blocks.23.ffn.2', 'blocks.23.self_attn.k', 'blocks.23.self_attn.o', 'blocks.23.self_attn.q', 'blocks.23.self_attn.v', 'blocks.24.cross_attn.k', 'blocks.24.cross_attn.o', 'blocks.24.cross_attn.q', 'blocks.24.cross_attn.v', 'blocks.24.ffn.0', 'blocks.24.ffn.2', 'blocks.24.self_attn.k', 'blocks.24.self_attn.o', 'blocks.24.self_attn.q', 'blocks.24.self_attn.v', 'blocks.25.cross_attn.k', 'blocks.25.cross_attn.o', 'blocks.25.cross_attn.q', 'blocks.25.cross_attn.v', 'blocks.25.ffn.0', 'blocks.25.ffn.2', 'blocks.25.self_attn.k', 'blocks.25.self_attn.o', 'blocks.25.self_attn.q', 'blocks.25.self_attn.v', 'blocks.26.cross_attn.k', 'blocks.26.cross_attn.o', 'blocks.26.cross_attn.q', 'blocks.26.cross_attn.v', 'blocks.26.ffn.0', 'blocks.26.ffn.2', 'blocks.26.self_attn.k', 'blocks.26.self_attn.o', 'blocks.26.self_attn.q', 'blocks.26.self_attn.v', 'blocks.27.cross_attn.k', 'blocks.27.cross_attn.o', 'blocks.27.cross_attn.q', 'blocks.27.cross_attn.v', 'blocks.27.ffn.0', 'blocks.27.ffn.2', 'blocks.27.self_attn.k', 'blocks.27.self_attn.o', 'blocks.27.self_attn.q', 'blocks.27.self_attn.v', 'blocks.28.cross_attn.k', 'blocks.28.cross_attn.o', 'blocks.28.cross_attn.q', 'blocks.28.cross_attn.v', 'blocks.28.ffn.0', 'blocks.28.ffn.2', 'blocks.28.self_attn.k', 'blocks.28.self_attn.o', 'blocks.28.self_attn.q', 'blocks.28.self_attn.v', 'blocks.29.cross_attn.k', 'blocks.29.cross_attn.o', 'blocks.29.cross_attn.q', 'blocks.29.cross_attn.v', 'blocks.29.ffn.0', 'blocks.29.ffn.2', 'blocks.29.self_attn.k', 'blocks.29.self_attn.o', 'blocks.29.self_attn.q', 'blocks.29.self_attn.v', 'blocks.30.cross_attn.k', 'blocks.30.cross_attn.o', 'blocks.30.cross_attn.q', 'blocks.30.cross_attn.v', 'blocks.30.ffn.0', 'blocks.30.ffn.2', 'blocks.30.self_attn.k', 'blocks.30.self_attn.o', 'blocks.30.self_attn.q', 'blocks.30.self_attn.v', 'blocks.31.cross_attn.k', 'blocks.31.cross_attn.o', 'blocks.31.cross_attn.q', 'blocks.31.cross_attn.v', 'blocks.31.ffn.0', 'blocks.31.ffn.2', 'blocks.31.self_attn.k', 'blocks.31.self_attn.o', 'blocks.31.self_attn.q', 'blocks.31.self_attn.v', 'blocks.32.cross_attn.k', 'blocks.32.cross_attn.o', 'blocks.32.cross_attn.q', 'blocks.32.cross_attn.v', 'blocks.32.ffn.0', 'blocks.32.ffn.2', 'blocks.32.self_attn.k', 'blocks.32.self_attn.o', 'blocks.32.self_attn.q', 'blocks.32.self_attn.v', 'blocks.33.cross_attn.k', 'blocks.33.cross_attn.o', 'blocks.33.cross_attn.q', 'blocks.33.cross_attn.v', 'blocks.33.ffn.0', 'blocks.33.ffn.2', 'blocks.33.self_attn.k', 'blocks.33.self_attn.o', 'blocks.33.self_attn.q', 'blocks.33.self_attn.v', 'blocks.34.cross_attn.k', 'blocks.34.cross_attn.o', 'blocks.34.cross_attn.q', 'blocks.34.cross_attn.v', 'blocks.34.ffn.0', 'blocks.34.ffn.2', 'blocks.34.self_attn.k', 'blocks.34.self_attn.o', 'blocks.34.self_attn.q', 'blocks.34.self_attn.v', 'blocks.35.cross_attn.k', 'blocks.35.cross_attn.o', 'blocks.35.cross_attn.q', 'blocks.35.cross_attn.v', 'blocks.35.ffn.0', 'blocks.35.ffn.2', 'blocks.35.self_attn.k', 'blocks.35.self_attn.o', 'blocks.35.self_attn.q', 'blocks.35.self_attn.v', 'blocks.36.cross_attn.k', 'blocks.36.cross_attn.o', 'blocks.36.cross_attn.q', 'blocks.36.cross_attn.v', 'blocks.36.ffn.0', 'blocks.36.ffn.2', 'blocks.36.self_attn.k', 'blocks.36.self_attn.o', 'blocks.36.self_attn.q', 'blocks.36.self_attn.v', 'blocks.37.cross_attn.k', 'blocks.37.cross_attn.o', 'blocks.37.cross_attn.q', 'blocks.37.cross_attn.v', 'blocks.37.ffn.0', 'blocks.37.ffn.2', 'blocks.37.self_attn.k', 'blocks.37.self_attn.o', 'blocks.37.self_attn.q', 'blocks.37.self_attn.v', 'blocks.38.cross_attn.k', 'blocks.38.cross_attn.o', 'blocks.38.cross_attn.q', 'blocks.38.cross_attn.v', 'blocks.38.ffn.0', 'blocks.38.ffn.2', 'blocks.38.self_attn.k', 'blocks.38.self_attn.o', 'blocks.38.self_attn.q', 'blocks.38.self_attn.v', 'blocks.39.cross_attn.k', 'blocks.39.cross_attn.o', 'blocks.39.cross_attn.q', 'blocks.39.cross_attn.v', 'blocks.39.ffn.0', 'blocks.39.ffn.2', 'blocks.39.self_attn.k', 'blocks.39.self_attn.o', 'blocks.39.self_attn.q', 'blocks.39.self_attn.v'])\n",
      "base_key가 존재:blocks.0.self_attn.q.weight\n",
      "blocks.0.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.0.self_attn.q\n",
      "blocks.0.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.0.self_attn.k.weight\n",
      "blocks.0.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.0.self_attn.k\n",
      "blocks.0.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.0.self_attn.v.weight\n",
      "blocks.0.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.0.self_attn.v\n",
      "blocks.0.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.0.self_attn.o.weight\n",
      "blocks.0.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.0.self_attn.o\n",
      "blocks.0.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.0.cross_attn.q.weight\n",
      "blocks.0.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.0.cross_attn.q\n",
      "blocks.0.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.0.cross_attn.k.weight\n",
      "blocks.0.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.0.cross_attn.k\n",
      "blocks.0.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.0.cross_attn.v.weight\n",
      "blocks.0.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.0.cross_attn.v\n",
      "blocks.0.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.0.cross_attn.o.weight\n",
      "blocks.0.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.0.cross_attn.o\n",
      "blocks.0.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.0.ffn.0.weight\n",
      "blocks.0.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.0.ffn.0\n",
      "blocks.0.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.0.ffn.2.weight\n",
      "blocks.0.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.0.ffn.2\n",
      "blocks.0.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.1.self_attn.q.weight\n",
      "blocks.1.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.1.self_attn.q\n",
      "blocks.1.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.1.self_attn.k.weight\n",
      "blocks.1.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.1.self_attn.k\n",
      "blocks.1.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.1.self_attn.v.weight\n",
      "blocks.1.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.1.self_attn.v\n",
      "blocks.1.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.1.self_attn.o.weight\n",
      "blocks.1.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.1.self_attn.o\n",
      "blocks.1.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.1.cross_attn.q.weight\n",
      "blocks.1.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.1.cross_attn.q\n",
      "blocks.1.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.1.cross_attn.k.weight\n",
      "blocks.1.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.1.cross_attn.k\n",
      "blocks.1.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.1.cross_attn.v.weight\n",
      "blocks.1.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.1.cross_attn.v\n",
      "blocks.1.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.1.cross_attn.o.weight\n",
      "blocks.1.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.1.cross_attn.o\n",
      "blocks.1.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.1.ffn.0.weight\n",
      "blocks.1.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.1.ffn.0\n",
      "blocks.1.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.1.ffn.2.weight\n",
      "blocks.1.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.1.ffn.2\n",
      "blocks.1.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.2.self_attn.q.weight\n",
      "blocks.2.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.2.self_attn.q\n",
      "blocks.2.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.2.self_attn.k.weight\n",
      "blocks.2.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.2.self_attn.k\n",
      "blocks.2.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.2.self_attn.v.weight\n",
      "blocks.2.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.2.self_attn.v\n",
      "blocks.2.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.2.self_attn.o.weight\n",
      "blocks.2.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.2.self_attn.o\n",
      "blocks.2.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.2.cross_attn.q.weight\n",
      "blocks.2.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.2.cross_attn.q\n",
      "blocks.2.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.2.cross_attn.k.weight\n",
      "blocks.2.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.2.cross_attn.k\n",
      "blocks.2.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.2.cross_attn.v.weight\n",
      "blocks.2.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.2.cross_attn.v\n",
      "blocks.2.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.2.cross_attn.o.weight\n",
      "blocks.2.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.2.cross_attn.o\n",
      "blocks.2.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.2.ffn.0.weight\n",
      "blocks.2.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.2.ffn.0\n",
      "blocks.2.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.2.ffn.2.weight\n",
      "blocks.2.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.2.ffn.2\n",
      "blocks.2.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.3.self_attn.q.weight\n",
      "blocks.3.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.3.self_attn.q\n",
      "blocks.3.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.3.self_attn.k.weight\n",
      "blocks.3.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.3.self_attn.k\n",
      "blocks.3.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.3.self_attn.v.weight\n",
      "blocks.3.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.3.self_attn.v\n",
      "blocks.3.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.3.self_attn.o.weight\n",
      "blocks.3.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.3.self_attn.o\n",
      "blocks.3.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.3.cross_attn.q.weight\n",
      "blocks.3.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.3.cross_attn.q\n",
      "blocks.3.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.3.cross_attn.k.weight\n",
      "blocks.3.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.3.cross_attn.k\n",
      "blocks.3.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.3.cross_attn.v.weight\n",
      "blocks.3.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.3.cross_attn.v\n",
      "blocks.3.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.3.cross_attn.o.weight\n",
      "blocks.3.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.3.cross_attn.o\n",
      "blocks.3.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.3.ffn.0.weight\n",
      "blocks.3.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.3.ffn.0\n",
      "blocks.3.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.3.ffn.2.weight\n",
      "blocks.3.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.3.ffn.2\n",
      "blocks.3.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.4.self_attn.q.weight\n",
      "blocks.4.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.4.self_attn.q\n",
      "blocks.4.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.4.self_attn.k.weight\n",
      "blocks.4.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.4.self_attn.k\n",
      "blocks.4.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.4.self_attn.v.weight\n",
      "blocks.4.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.4.self_attn.v\n",
      "blocks.4.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.4.self_attn.o.weight\n",
      "blocks.4.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.4.self_attn.o\n",
      "blocks.4.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.4.cross_attn.q.weight\n",
      "blocks.4.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.4.cross_attn.q\n",
      "blocks.4.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.4.cross_attn.k.weight\n",
      "blocks.4.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.4.cross_attn.k\n",
      "blocks.4.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.4.cross_attn.v.weight\n",
      "blocks.4.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.4.cross_attn.v\n",
      "blocks.4.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.4.cross_attn.o.weight\n",
      "blocks.4.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.4.cross_attn.o\n",
      "blocks.4.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.4.ffn.0.weight\n",
      "blocks.4.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.4.ffn.0\n",
      "blocks.4.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.4.ffn.2.weight\n",
      "blocks.4.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.4.ffn.2\n",
      "blocks.4.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.5.self_attn.q.weight\n",
      "blocks.5.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.5.self_attn.q\n",
      "blocks.5.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.5.self_attn.k.weight\n",
      "blocks.5.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.5.self_attn.k\n",
      "blocks.5.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.5.self_attn.v.weight\n",
      "blocks.5.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.5.self_attn.v\n",
      "blocks.5.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.5.self_attn.o.weight\n",
      "blocks.5.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.5.self_attn.o\n",
      "blocks.5.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.5.cross_attn.q.weight\n",
      "blocks.5.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.5.cross_attn.q\n",
      "blocks.5.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.5.cross_attn.k.weight\n",
      "blocks.5.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.5.cross_attn.k\n",
      "blocks.5.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.5.cross_attn.v.weight\n",
      "blocks.5.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.5.cross_attn.v\n",
      "blocks.5.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.5.cross_attn.o.weight\n",
      "blocks.5.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.5.cross_attn.o\n",
      "blocks.5.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.5.ffn.0.weight\n",
      "blocks.5.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.5.ffn.0\n",
      "blocks.5.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.5.ffn.2.weight\n",
      "blocks.5.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.5.ffn.2\n",
      "blocks.5.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.6.self_attn.q.weight\n",
      "blocks.6.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.6.self_attn.q\n",
      "blocks.6.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.6.self_attn.k.weight\n",
      "blocks.6.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.6.self_attn.k\n",
      "blocks.6.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.6.self_attn.v.weight\n",
      "blocks.6.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.6.self_attn.v\n",
      "blocks.6.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.6.self_attn.o.weight\n",
      "blocks.6.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.6.self_attn.o\n",
      "blocks.6.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.6.cross_attn.q.weight\n",
      "blocks.6.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.6.cross_attn.q\n",
      "blocks.6.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.6.cross_attn.k.weight\n",
      "blocks.6.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.6.cross_attn.k\n",
      "blocks.6.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.6.cross_attn.v.weight\n",
      "blocks.6.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.6.cross_attn.v\n",
      "blocks.6.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.6.cross_attn.o.weight\n",
      "blocks.6.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.6.cross_attn.o\n",
      "blocks.6.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.6.ffn.0.weight\n",
      "blocks.6.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.6.ffn.0\n",
      "blocks.6.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.6.ffn.2.weight\n",
      "blocks.6.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.6.ffn.2\n",
      "blocks.6.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.7.self_attn.q.weight\n",
      "blocks.7.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.7.self_attn.q\n",
      "blocks.7.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.7.self_attn.k.weight\n",
      "blocks.7.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.7.self_attn.k\n",
      "blocks.7.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.7.self_attn.v.weight\n",
      "blocks.7.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.7.self_attn.v\n",
      "blocks.7.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.7.self_attn.o.weight\n",
      "blocks.7.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.7.self_attn.o\n",
      "blocks.7.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.7.cross_attn.q.weight\n",
      "blocks.7.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.7.cross_attn.q\n",
      "blocks.7.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.7.cross_attn.k.weight\n",
      "blocks.7.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.7.cross_attn.k\n",
      "blocks.7.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.7.cross_attn.v.weight\n",
      "blocks.7.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.7.cross_attn.v\n",
      "blocks.7.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.7.cross_attn.o.weight\n",
      "blocks.7.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.7.cross_attn.o\n",
      "blocks.7.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.7.ffn.0.weight\n",
      "blocks.7.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.7.ffn.0\n",
      "blocks.7.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.7.ffn.2.weight\n",
      "blocks.7.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.7.ffn.2\n",
      "blocks.7.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.8.self_attn.q.weight\n",
      "blocks.8.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.8.self_attn.q\n",
      "blocks.8.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.8.self_attn.k.weight\n",
      "blocks.8.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.8.self_attn.k\n",
      "blocks.8.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.8.self_attn.v.weight\n",
      "blocks.8.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.8.self_attn.v\n",
      "blocks.8.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.8.self_attn.o.weight\n",
      "blocks.8.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.8.self_attn.o\n",
      "blocks.8.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.8.cross_attn.q.weight\n",
      "blocks.8.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.8.cross_attn.q\n",
      "blocks.8.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.8.cross_attn.k.weight\n",
      "blocks.8.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.8.cross_attn.k\n",
      "blocks.8.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.8.cross_attn.v.weight\n",
      "blocks.8.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.8.cross_attn.v\n",
      "blocks.8.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.8.cross_attn.o.weight\n",
      "blocks.8.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.8.cross_attn.o\n",
      "blocks.8.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.8.ffn.0.weight\n",
      "blocks.8.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.8.ffn.0\n",
      "blocks.8.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.8.ffn.2.weight\n",
      "blocks.8.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.8.ffn.2\n",
      "blocks.8.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.9.self_attn.q.weight\n",
      "blocks.9.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.9.self_attn.q\n",
      "blocks.9.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.9.self_attn.k.weight\n",
      "blocks.9.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.9.self_attn.k\n",
      "blocks.9.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.9.self_attn.v.weight\n",
      "blocks.9.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.9.self_attn.v\n",
      "blocks.9.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.9.self_attn.o.weight\n",
      "blocks.9.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.9.self_attn.o\n",
      "blocks.9.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.9.cross_attn.q.weight\n",
      "blocks.9.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.9.cross_attn.q\n",
      "blocks.9.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.9.cross_attn.k.weight\n",
      "blocks.9.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.9.cross_attn.k\n",
      "blocks.9.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.9.cross_attn.v.weight\n",
      "blocks.9.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.9.cross_attn.v\n",
      "blocks.9.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.9.cross_attn.o.weight\n",
      "blocks.9.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.9.cross_attn.o\n",
      "blocks.9.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.9.ffn.0.weight\n",
      "blocks.9.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.9.ffn.0\n",
      "blocks.9.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.9.ffn.2.weight\n",
      "blocks.9.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.9.ffn.2\n",
      "blocks.9.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.10.self_attn.q.weight\n",
      "blocks.10.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.10.self_attn.q\n",
      "blocks.10.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.10.self_attn.k.weight\n",
      "blocks.10.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.10.self_attn.k\n",
      "blocks.10.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.10.self_attn.v.weight\n",
      "blocks.10.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.10.self_attn.v\n",
      "blocks.10.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.10.self_attn.o.weight\n",
      "blocks.10.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.10.self_attn.o\n",
      "blocks.10.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.10.cross_attn.q.weight\n",
      "blocks.10.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.10.cross_attn.q\n",
      "blocks.10.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.10.cross_attn.k.weight\n",
      "blocks.10.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.10.cross_attn.k\n",
      "blocks.10.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.10.cross_attn.v.weight\n",
      "blocks.10.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.10.cross_attn.v\n",
      "blocks.10.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.10.cross_attn.o.weight\n",
      "blocks.10.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.10.cross_attn.o\n",
      "blocks.10.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.10.ffn.0.weight\n",
      "blocks.10.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.10.ffn.0\n",
      "blocks.10.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.10.ffn.2.weight\n",
      "blocks.10.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.10.ffn.2\n",
      "blocks.10.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.11.self_attn.q.weight\n",
      "blocks.11.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.11.self_attn.q\n",
      "blocks.11.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.11.self_attn.k.weight\n",
      "blocks.11.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.11.self_attn.k\n",
      "blocks.11.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.11.self_attn.v.weight\n",
      "blocks.11.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.11.self_attn.v\n",
      "blocks.11.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.11.self_attn.o.weight\n",
      "blocks.11.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.11.self_attn.o\n",
      "blocks.11.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.11.cross_attn.q.weight\n",
      "blocks.11.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.11.cross_attn.q\n",
      "blocks.11.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.11.cross_attn.k.weight\n",
      "blocks.11.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.11.cross_attn.k\n",
      "blocks.11.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.11.cross_attn.v.weight\n",
      "blocks.11.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.11.cross_attn.v\n",
      "blocks.11.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.11.cross_attn.o.weight\n",
      "blocks.11.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.11.cross_attn.o\n",
      "blocks.11.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.11.ffn.0.weight\n",
      "blocks.11.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.11.ffn.0\n",
      "blocks.11.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.11.ffn.2.weight\n",
      "blocks.11.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.11.ffn.2\n",
      "blocks.11.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.12.self_attn.q.weight\n",
      "blocks.12.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.12.self_attn.q\n",
      "blocks.12.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.12.self_attn.k.weight\n",
      "blocks.12.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.12.self_attn.k\n",
      "blocks.12.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.12.self_attn.v.weight\n",
      "blocks.12.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.12.self_attn.v\n",
      "blocks.12.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.12.self_attn.o.weight\n",
      "blocks.12.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.12.self_attn.o\n",
      "blocks.12.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.12.cross_attn.q.weight\n",
      "blocks.12.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.12.cross_attn.q\n",
      "blocks.12.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.12.cross_attn.k.weight\n",
      "blocks.12.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.12.cross_attn.k\n",
      "blocks.12.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.12.cross_attn.v.weight\n",
      "blocks.12.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.12.cross_attn.v\n",
      "blocks.12.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.12.cross_attn.o.weight\n",
      "blocks.12.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.12.cross_attn.o\n",
      "blocks.12.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.12.ffn.0.weight\n",
      "blocks.12.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.12.ffn.0\n",
      "blocks.12.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.12.ffn.2.weight\n",
      "blocks.12.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.12.ffn.2\n",
      "blocks.12.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.13.self_attn.q.weight\n",
      "blocks.13.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.13.self_attn.q\n",
      "blocks.13.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.13.self_attn.k.weight\n",
      "blocks.13.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.13.self_attn.k\n",
      "blocks.13.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.13.self_attn.v.weight\n",
      "blocks.13.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.13.self_attn.v\n",
      "blocks.13.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.13.self_attn.o.weight\n",
      "blocks.13.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.13.self_attn.o\n",
      "blocks.13.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.13.cross_attn.q.weight\n",
      "blocks.13.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.13.cross_attn.q\n",
      "blocks.13.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.13.cross_attn.k.weight\n",
      "blocks.13.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.13.cross_attn.k\n",
      "blocks.13.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.13.cross_attn.v.weight\n",
      "blocks.13.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.13.cross_attn.v\n",
      "blocks.13.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.13.cross_attn.o.weight\n",
      "blocks.13.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.13.cross_attn.o\n",
      "blocks.13.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.13.ffn.0.weight\n",
      "blocks.13.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.13.ffn.0\n",
      "blocks.13.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.13.ffn.2.weight\n",
      "blocks.13.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.13.ffn.2\n",
      "blocks.13.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.14.self_attn.q.weight\n",
      "blocks.14.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.14.self_attn.q\n",
      "blocks.14.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.14.self_attn.k.weight\n",
      "blocks.14.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.14.self_attn.k\n",
      "blocks.14.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.14.self_attn.v.weight\n",
      "blocks.14.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.14.self_attn.v\n",
      "blocks.14.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.14.self_attn.o.weight\n",
      "blocks.14.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.14.self_attn.o\n",
      "blocks.14.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.14.cross_attn.q.weight\n",
      "blocks.14.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.14.cross_attn.q\n",
      "blocks.14.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.14.cross_attn.k.weight\n",
      "blocks.14.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.14.cross_attn.k\n",
      "blocks.14.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.14.cross_attn.v.weight\n",
      "blocks.14.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.14.cross_attn.v\n",
      "blocks.14.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.14.cross_attn.o.weight\n",
      "blocks.14.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.14.cross_attn.o\n",
      "blocks.14.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.14.ffn.0.weight\n",
      "blocks.14.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.14.ffn.0\n",
      "blocks.14.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.14.ffn.2.weight\n",
      "blocks.14.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.14.ffn.2\n",
      "blocks.14.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.15.self_attn.q.weight\n",
      "blocks.15.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.15.self_attn.q\n",
      "blocks.15.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.15.self_attn.k.weight\n",
      "blocks.15.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.15.self_attn.k\n",
      "blocks.15.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.15.self_attn.v.weight\n",
      "blocks.15.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.15.self_attn.v\n",
      "blocks.15.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.15.self_attn.o.weight\n",
      "blocks.15.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.15.self_attn.o\n",
      "blocks.15.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.15.cross_attn.q.weight\n",
      "blocks.15.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.15.cross_attn.q\n",
      "blocks.15.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.15.cross_attn.k.weight\n",
      "blocks.15.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.15.cross_attn.k\n",
      "blocks.15.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.15.cross_attn.v.weight\n",
      "blocks.15.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.15.cross_attn.v\n",
      "blocks.15.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.15.cross_attn.o.weight\n",
      "blocks.15.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.15.cross_attn.o\n",
      "blocks.15.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.15.ffn.0.weight\n",
      "blocks.15.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.15.ffn.0\n",
      "blocks.15.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.15.ffn.2.weight\n",
      "blocks.15.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.15.ffn.2\n",
      "blocks.15.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.16.self_attn.q.weight\n",
      "blocks.16.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.16.self_attn.q\n",
      "blocks.16.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.16.self_attn.k.weight\n",
      "blocks.16.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.16.self_attn.k\n",
      "blocks.16.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.16.self_attn.v.weight\n",
      "blocks.16.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.16.self_attn.v\n",
      "blocks.16.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.16.self_attn.o.weight\n",
      "blocks.16.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.16.self_attn.o\n",
      "blocks.16.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.16.cross_attn.q.weight\n",
      "blocks.16.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.16.cross_attn.q\n",
      "blocks.16.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.16.cross_attn.k.weight\n",
      "blocks.16.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.16.cross_attn.k\n",
      "blocks.16.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.16.cross_attn.v.weight\n",
      "blocks.16.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.16.cross_attn.v\n",
      "blocks.16.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.16.cross_attn.o.weight\n",
      "blocks.16.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.16.cross_attn.o\n",
      "blocks.16.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.16.ffn.0.weight\n",
      "blocks.16.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.16.ffn.0\n",
      "blocks.16.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.16.ffn.2.weight\n",
      "blocks.16.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.16.ffn.2\n",
      "blocks.16.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.17.self_attn.q.weight\n",
      "blocks.17.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.17.self_attn.q\n",
      "blocks.17.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.17.self_attn.k.weight\n",
      "blocks.17.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.17.self_attn.k\n",
      "blocks.17.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.17.self_attn.v.weight\n",
      "blocks.17.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.17.self_attn.v\n",
      "blocks.17.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.17.self_attn.o.weight\n",
      "blocks.17.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.17.self_attn.o\n",
      "blocks.17.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.17.cross_attn.q.weight\n",
      "blocks.17.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.17.cross_attn.q\n",
      "blocks.17.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.17.cross_attn.k.weight\n",
      "blocks.17.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.17.cross_attn.k\n",
      "blocks.17.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.17.cross_attn.v.weight\n",
      "blocks.17.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.17.cross_attn.v\n",
      "blocks.17.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.17.cross_attn.o.weight\n",
      "blocks.17.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.17.cross_attn.o\n",
      "blocks.17.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.17.ffn.0.weight\n",
      "blocks.17.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.17.ffn.0\n",
      "blocks.17.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.17.ffn.2.weight\n",
      "blocks.17.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.17.ffn.2\n",
      "blocks.17.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.18.self_attn.q.weight\n",
      "blocks.18.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.18.self_attn.q\n",
      "blocks.18.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.18.self_attn.k.weight\n",
      "blocks.18.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.18.self_attn.k\n",
      "blocks.18.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.18.self_attn.v.weight\n",
      "blocks.18.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.18.self_attn.v\n",
      "blocks.18.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.18.self_attn.o.weight\n",
      "blocks.18.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.18.self_attn.o\n",
      "blocks.18.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.18.cross_attn.q.weight\n",
      "blocks.18.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.18.cross_attn.q\n",
      "blocks.18.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.18.cross_attn.k.weight\n",
      "blocks.18.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.18.cross_attn.k\n",
      "blocks.18.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.18.cross_attn.v.weight\n",
      "blocks.18.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.18.cross_attn.v\n",
      "blocks.18.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.18.cross_attn.o.weight\n",
      "blocks.18.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.18.cross_attn.o\n",
      "blocks.18.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.18.ffn.0.weight\n",
      "blocks.18.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.18.ffn.0\n",
      "blocks.18.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.18.ffn.2.weight\n",
      "blocks.18.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.18.ffn.2\n",
      "blocks.18.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.19.self_attn.q.weight\n",
      "blocks.19.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.19.self_attn.q\n",
      "blocks.19.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.19.self_attn.k.weight\n",
      "blocks.19.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.19.self_attn.k\n",
      "blocks.19.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.19.self_attn.v.weight\n",
      "blocks.19.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.19.self_attn.v\n",
      "blocks.19.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.19.self_attn.o.weight\n",
      "blocks.19.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.19.self_attn.o\n",
      "blocks.19.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.19.cross_attn.q.weight\n",
      "blocks.19.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.19.cross_attn.q\n",
      "blocks.19.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.19.cross_attn.k.weight\n",
      "blocks.19.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.19.cross_attn.k\n",
      "blocks.19.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.19.cross_attn.v.weight\n",
      "blocks.19.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.19.cross_attn.v\n",
      "blocks.19.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.19.cross_attn.o.weight\n",
      "blocks.19.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.19.cross_attn.o\n",
      "blocks.19.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.19.ffn.0.weight\n",
      "blocks.19.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.19.ffn.0\n",
      "blocks.19.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.19.ffn.2.weight\n",
      "blocks.19.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.19.ffn.2\n",
      "blocks.19.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.20.self_attn.q.weight\n",
      "blocks.20.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.20.self_attn.q\n",
      "blocks.20.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.20.self_attn.k.weight\n",
      "blocks.20.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.20.self_attn.k\n",
      "blocks.20.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.20.self_attn.v.weight\n",
      "blocks.20.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.20.self_attn.v\n",
      "blocks.20.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.20.self_attn.o.weight\n",
      "blocks.20.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.20.self_attn.o\n",
      "blocks.20.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.20.cross_attn.q.weight\n",
      "blocks.20.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.20.cross_attn.q\n",
      "blocks.20.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.20.cross_attn.k.weight\n",
      "blocks.20.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.20.cross_attn.k\n",
      "blocks.20.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.20.cross_attn.v.weight\n",
      "blocks.20.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.20.cross_attn.v\n",
      "blocks.20.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.20.cross_attn.o.weight\n",
      "blocks.20.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.20.cross_attn.o\n",
      "blocks.20.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.20.ffn.0.weight\n",
      "blocks.20.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.20.ffn.0\n",
      "blocks.20.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.20.ffn.2.weight\n",
      "blocks.20.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.20.ffn.2\n",
      "blocks.20.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.21.self_attn.q.weight\n",
      "blocks.21.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.21.self_attn.q\n",
      "blocks.21.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.21.self_attn.k.weight\n",
      "blocks.21.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.21.self_attn.k\n",
      "blocks.21.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.21.self_attn.v.weight\n",
      "blocks.21.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.21.self_attn.v\n",
      "blocks.21.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.21.self_attn.o.weight\n",
      "blocks.21.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.21.self_attn.o\n",
      "blocks.21.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.21.cross_attn.q.weight\n",
      "blocks.21.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.21.cross_attn.q\n",
      "blocks.21.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.21.cross_attn.k.weight\n",
      "blocks.21.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.21.cross_attn.k\n",
      "blocks.21.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.21.cross_attn.v.weight\n",
      "blocks.21.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.21.cross_attn.v\n",
      "blocks.21.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.21.cross_attn.o.weight\n",
      "blocks.21.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.21.cross_attn.o\n",
      "blocks.21.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.21.ffn.0.weight\n",
      "blocks.21.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.21.ffn.0\n",
      "blocks.21.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.21.ffn.2.weight\n",
      "blocks.21.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.21.ffn.2\n",
      "blocks.21.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.22.self_attn.q.weight\n",
      "blocks.22.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.22.self_attn.q\n",
      "blocks.22.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.22.self_attn.k.weight\n",
      "blocks.22.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.22.self_attn.k\n",
      "blocks.22.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.22.self_attn.v.weight\n",
      "blocks.22.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.22.self_attn.v\n",
      "blocks.22.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.22.self_attn.o.weight\n",
      "blocks.22.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.22.self_attn.o\n",
      "blocks.22.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.22.cross_attn.q.weight\n",
      "blocks.22.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.22.cross_attn.q\n",
      "blocks.22.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.22.cross_attn.k.weight\n",
      "blocks.22.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.22.cross_attn.k\n",
      "blocks.22.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.22.cross_attn.v.weight\n",
      "blocks.22.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.22.cross_attn.v\n",
      "blocks.22.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.22.cross_attn.o.weight\n",
      "blocks.22.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.22.cross_attn.o\n",
      "blocks.22.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.22.ffn.0.weight\n",
      "blocks.22.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.22.ffn.0\n",
      "blocks.22.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.22.ffn.2.weight\n",
      "blocks.22.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.22.ffn.2\n",
      "blocks.22.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.23.self_attn.q.weight\n",
      "blocks.23.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.23.self_attn.q\n",
      "blocks.23.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.23.self_attn.k.weight\n",
      "blocks.23.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.23.self_attn.k\n",
      "blocks.23.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.23.self_attn.v.weight\n",
      "blocks.23.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.23.self_attn.v\n",
      "blocks.23.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.23.self_attn.o.weight\n",
      "blocks.23.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.23.self_attn.o\n",
      "blocks.23.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.23.cross_attn.q.weight\n",
      "blocks.23.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.23.cross_attn.q\n",
      "blocks.23.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.23.cross_attn.k.weight\n",
      "blocks.23.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.23.cross_attn.k\n",
      "blocks.23.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.23.cross_attn.v.weight\n",
      "blocks.23.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.23.cross_attn.v\n",
      "blocks.23.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.23.cross_attn.o.weight\n",
      "blocks.23.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.23.cross_attn.o\n",
      "blocks.23.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.23.ffn.0.weight\n",
      "blocks.23.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.23.ffn.0\n",
      "blocks.23.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.23.ffn.2.weight\n",
      "blocks.23.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.23.ffn.2\n",
      "blocks.23.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.24.self_attn.q.weight\n",
      "blocks.24.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.24.self_attn.q\n",
      "blocks.24.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.24.self_attn.k.weight\n",
      "blocks.24.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.24.self_attn.k\n",
      "blocks.24.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.24.self_attn.v.weight\n",
      "blocks.24.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.24.self_attn.v\n",
      "blocks.24.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.24.self_attn.o.weight\n",
      "blocks.24.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.24.self_attn.o\n",
      "blocks.24.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.24.cross_attn.q.weight\n",
      "blocks.24.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.24.cross_attn.q\n",
      "blocks.24.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.24.cross_attn.k.weight\n",
      "blocks.24.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.24.cross_attn.k\n",
      "blocks.24.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.24.cross_attn.v.weight\n",
      "blocks.24.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.24.cross_attn.v\n",
      "blocks.24.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.24.cross_attn.o.weight\n",
      "blocks.24.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.24.cross_attn.o\n",
      "blocks.24.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.24.ffn.0.weight\n",
      "blocks.24.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.24.ffn.0\n",
      "blocks.24.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.24.ffn.2.weight\n",
      "blocks.24.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.24.ffn.2\n",
      "blocks.24.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.25.self_attn.q.weight\n",
      "blocks.25.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.25.self_attn.q\n",
      "blocks.25.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.25.self_attn.k.weight\n",
      "blocks.25.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.25.self_attn.k\n",
      "blocks.25.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.25.self_attn.v.weight\n",
      "blocks.25.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.25.self_attn.v\n",
      "blocks.25.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.25.self_attn.o.weight\n",
      "blocks.25.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.25.self_attn.o\n",
      "blocks.25.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.25.cross_attn.q.weight\n",
      "blocks.25.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.25.cross_attn.q\n",
      "blocks.25.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.25.cross_attn.k.weight\n",
      "blocks.25.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.25.cross_attn.k\n",
      "blocks.25.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.25.cross_attn.v.weight\n",
      "blocks.25.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.25.cross_attn.v\n",
      "blocks.25.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.25.cross_attn.o.weight\n",
      "blocks.25.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.25.cross_attn.o\n",
      "blocks.25.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.25.ffn.0.weight\n",
      "blocks.25.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.25.ffn.0\n",
      "blocks.25.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.25.ffn.2.weight\n",
      "blocks.25.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.25.ffn.2\n",
      "blocks.25.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.26.self_attn.q.weight\n",
      "blocks.26.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.26.self_attn.q\n",
      "blocks.26.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.26.self_attn.k.weight\n",
      "blocks.26.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.26.self_attn.k\n",
      "blocks.26.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.26.self_attn.v.weight\n",
      "blocks.26.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.26.self_attn.v\n",
      "blocks.26.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.26.self_attn.o.weight\n",
      "blocks.26.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.26.self_attn.o\n",
      "blocks.26.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.26.cross_attn.q.weight\n",
      "blocks.26.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.26.cross_attn.q\n",
      "blocks.26.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.26.cross_attn.k.weight\n",
      "blocks.26.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.26.cross_attn.k\n",
      "blocks.26.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.26.cross_attn.v.weight\n",
      "blocks.26.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.26.cross_attn.v\n",
      "blocks.26.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.26.cross_attn.o.weight\n",
      "blocks.26.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.26.cross_attn.o\n",
      "blocks.26.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.26.ffn.0.weight\n",
      "blocks.26.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.26.ffn.0\n",
      "blocks.26.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.26.ffn.2.weight\n",
      "blocks.26.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.26.ffn.2\n",
      "blocks.26.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.27.self_attn.q.weight\n",
      "blocks.27.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.27.self_attn.q\n",
      "blocks.27.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.27.self_attn.k.weight\n",
      "blocks.27.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.27.self_attn.k\n",
      "blocks.27.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.27.self_attn.v.weight\n",
      "blocks.27.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.27.self_attn.v\n",
      "blocks.27.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.27.self_attn.o.weight\n",
      "blocks.27.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.27.self_attn.o\n",
      "blocks.27.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.27.cross_attn.q.weight\n",
      "blocks.27.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.27.cross_attn.q\n",
      "blocks.27.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.27.cross_attn.k.weight\n",
      "blocks.27.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.27.cross_attn.k\n",
      "blocks.27.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.27.cross_attn.v.weight\n",
      "blocks.27.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.27.cross_attn.v\n",
      "blocks.27.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.27.cross_attn.o.weight\n",
      "blocks.27.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.27.cross_attn.o\n",
      "blocks.27.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.27.ffn.0.weight\n",
      "blocks.27.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.27.ffn.0\n",
      "blocks.27.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.27.ffn.2.weight\n",
      "blocks.27.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.27.ffn.2\n",
      "blocks.27.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.28.self_attn.q.weight\n",
      "blocks.28.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.28.self_attn.q\n",
      "blocks.28.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.28.self_attn.k.weight\n",
      "blocks.28.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.28.self_attn.k\n",
      "blocks.28.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.28.self_attn.v.weight\n",
      "blocks.28.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.28.self_attn.v\n",
      "blocks.28.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.28.self_attn.o.weight\n",
      "blocks.28.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.28.self_attn.o\n",
      "blocks.28.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.28.cross_attn.q.weight\n",
      "blocks.28.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.28.cross_attn.q\n",
      "blocks.28.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.28.cross_attn.k.weight\n",
      "blocks.28.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.28.cross_attn.k\n",
      "blocks.28.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.28.cross_attn.v.weight\n",
      "blocks.28.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.28.cross_attn.v\n",
      "blocks.28.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.28.cross_attn.o.weight\n",
      "blocks.28.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.28.cross_attn.o\n",
      "blocks.28.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.28.ffn.0.weight\n",
      "blocks.28.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.28.ffn.0\n",
      "blocks.28.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.28.ffn.2.weight\n",
      "blocks.28.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.28.ffn.2\n",
      "blocks.28.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.29.self_attn.q.weight\n",
      "blocks.29.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.29.self_attn.q\n",
      "blocks.29.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.29.self_attn.k.weight\n",
      "blocks.29.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.29.self_attn.k\n",
      "blocks.29.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.29.self_attn.v.weight\n",
      "blocks.29.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.29.self_attn.v\n",
      "blocks.29.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.29.self_attn.o.weight\n",
      "blocks.29.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.29.self_attn.o\n",
      "blocks.29.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.29.cross_attn.q.weight\n",
      "blocks.29.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.29.cross_attn.q\n",
      "blocks.29.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.29.cross_attn.k.weight\n",
      "blocks.29.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.29.cross_attn.k\n",
      "blocks.29.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.29.cross_attn.v.weight\n",
      "blocks.29.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.29.cross_attn.v\n",
      "blocks.29.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.29.cross_attn.o.weight\n",
      "blocks.29.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.29.cross_attn.o\n",
      "blocks.29.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.29.ffn.0.weight\n",
      "blocks.29.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.29.ffn.0\n",
      "blocks.29.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.29.ffn.2.weight\n",
      "blocks.29.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.29.ffn.2\n",
      "blocks.29.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.30.self_attn.q.weight\n",
      "blocks.30.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.30.self_attn.q\n",
      "blocks.30.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.30.self_attn.k.weight\n",
      "blocks.30.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.30.self_attn.k\n",
      "blocks.30.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.30.self_attn.v.weight\n",
      "blocks.30.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.30.self_attn.v\n",
      "blocks.30.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.30.self_attn.o.weight\n",
      "blocks.30.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.30.self_attn.o\n",
      "blocks.30.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.30.cross_attn.q.weight\n",
      "blocks.30.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.30.cross_attn.q\n",
      "blocks.30.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.30.cross_attn.k.weight\n",
      "blocks.30.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.30.cross_attn.k\n",
      "blocks.30.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.30.cross_attn.v.weight\n",
      "blocks.30.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.30.cross_attn.v\n",
      "blocks.30.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.30.cross_attn.o.weight\n",
      "blocks.30.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.30.cross_attn.o\n",
      "blocks.30.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.30.ffn.0.weight\n",
      "blocks.30.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.30.ffn.0\n",
      "blocks.30.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.30.ffn.2.weight\n",
      "blocks.30.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.30.ffn.2\n",
      "blocks.30.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.31.self_attn.q.weight\n",
      "blocks.31.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.31.self_attn.q\n",
      "blocks.31.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.31.self_attn.k.weight\n",
      "blocks.31.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.31.self_attn.k\n",
      "blocks.31.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.31.self_attn.v.weight\n",
      "blocks.31.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.31.self_attn.v\n",
      "blocks.31.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.31.self_attn.o.weight\n",
      "blocks.31.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.31.self_attn.o\n",
      "blocks.31.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.31.cross_attn.q.weight\n",
      "blocks.31.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.31.cross_attn.q\n",
      "blocks.31.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.31.cross_attn.k.weight\n",
      "blocks.31.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.31.cross_attn.k\n",
      "blocks.31.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.31.cross_attn.v.weight\n",
      "blocks.31.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.31.cross_attn.v\n",
      "blocks.31.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.31.cross_attn.o.weight\n",
      "blocks.31.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.31.cross_attn.o\n",
      "blocks.31.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.31.ffn.0.weight\n",
      "blocks.31.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.31.ffn.0\n",
      "blocks.31.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.31.ffn.2.weight\n",
      "blocks.31.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.31.ffn.2\n",
      "blocks.31.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.32.self_attn.q.weight\n",
      "blocks.32.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.32.self_attn.q\n",
      "blocks.32.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.32.self_attn.k.weight\n",
      "blocks.32.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.32.self_attn.k\n",
      "blocks.32.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.32.self_attn.v.weight\n",
      "blocks.32.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.32.self_attn.v\n",
      "blocks.32.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.32.self_attn.o.weight\n",
      "blocks.32.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.32.self_attn.o\n",
      "blocks.32.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.32.cross_attn.q.weight\n",
      "blocks.32.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.32.cross_attn.q\n",
      "blocks.32.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.32.cross_attn.k.weight\n",
      "blocks.32.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.32.cross_attn.k\n",
      "blocks.32.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.32.cross_attn.v.weight\n",
      "blocks.32.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.32.cross_attn.v\n",
      "blocks.32.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.32.cross_attn.o.weight\n",
      "blocks.32.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.32.cross_attn.o\n",
      "blocks.32.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.32.ffn.0.weight\n",
      "blocks.32.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.32.ffn.0\n",
      "blocks.32.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.32.ffn.2.weight\n",
      "blocks.32.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.32.ffn.2\n",
      "blocks.32.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.33.self_attn.q.weight\n",
      "blocks.33.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.33.self_attn.q\n",
      "blocks.33.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.33.self_attn.k.weight\n",
      "blocks.33.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.33.self_attn.k\n",
      "blocks.33.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.33.self_attn.v.weight\n",
      "blocks.33.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.33.self_attn.v\n",
      "blocks.33.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.33.self_attn.o.weight\n",
      "blocks.33.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.33.self_attn.o\n",
      "blocks.33.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.33.cross_attn.q.weight\n",
      "blocks.33.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.33.cross_attn.q\n",
      "blocks.33.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.33.cross_attn.k.weight\n",
      "blocks.33.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.33.cross_attn.k\n",
      "blocks.33.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.33.cross_attn.v.weight\n",
      "blocks.33.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.33.cross_attn.v\n",
      "blocks.33.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.33.cross_attn.o.weight\n",
      "blocks.33.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.33.cross_attn.o\n",
      "blocks.33.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.33.ffn.0.weight\n",
      "blocks.33.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.33.ffn.0\n",
      "blocks.33.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.33.ffn.2.weight\n",
      "blocks.33.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.33.ffn.2\n",
      "blocks.33.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.34.self_attn.q.weight\n",
      "blocks.34.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.34.self_attn.q\n",
      "blocks.34.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.34.self_attn.k.weight\n",
      "blocks.34.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.34.self_attn.k\n",
      "blocks.34.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.34.self_attn.v.weight\n",
      "blocks.34.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.34.self_attn.v\n",
      "blocks.34.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.34.self_attn.o.weight\n",
      "blocks.34.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.34.self_attn.o\n",
      "blocks.34.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.34.cross_attn.q.weight\n",
      "blocks.34.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.34.cross_attn.q\n",
      "blocks.34.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.34.cross_attn.k.weight\n",
      "blocks.34.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.34.cross_attn.k\n",
      "blocks.34.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.34.cross_attn.v.weight\n",
      "blocks.34.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.34.cross_attn.v\n",
      "blocks.34.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.34.cross_attn.o.weight\n",
      "blocks.34.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.34.cross_attn.o\n",
      "blocks.34.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.34.ffn.0.weight\n",
      "blocks.34.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.34.ffn.0\n",
      "blocks.34.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.34.ffn.2.weight\n",
      "blocks.34.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.34.ffn.2\n",
      "blocks.34.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.35.self_attn.q.weight\n",
      "blocks.35.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.35.self_attn.q\n",
      "blocks.35.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.35.self_attn.k.weight\n",
      "blocks.35.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.35.self_attn.k\n",
      "blocks.35.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.35.self_attn.v.weight\n",
      "blocks.35.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.35.self_attn.v\n",
      "blocks.35.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.35.self_attn.o.weight\n",
      "blocks.35.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.35.self_attn.o\n",
      "blocks.35.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.35.cross_attn.q.weight\n",
      "blocks.35.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.35.cross_attn.q\n",
      "blocks.35.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.35.cross_attn.k.weight\n",
      "blocks.35.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.35.cross_attn.k\n",
      "blocks.35.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.35.cross_attn.v.weight\n",
      "blocks.35.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.35.cross_attn.v\n",
      "blocks.35.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.35.cross_attn.o.weight\n",
      "blocks.35.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.35.cross_attn.o\n",
      "blocks.35.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.35.ffn.0.weight\n",
      "blocks.35.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.35.ffn.0\n",
      "blocks.35.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.35.ffn.2.weight\n",
      "blocks.35.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.35.ffn.2\n",
      "blocks.35.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.36.self_attn.q.weight\n",
      "blocks.36.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.36.self_attn.q\n",
      "blocks.36.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.36.self_attn.k.weight\n",
      "blocks.36.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.36.self_attn.k\n",
      "blocks.36.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.36.self_attn.v.weight\n",
      "blocks.36.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.36.self_attn.v\n",
      "blocks.36.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.36.self_attn.o.weight\n",
      "blocks.36.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.36.self_attn.o\n",
      "blocks.36.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.36.cross_attn.q.weight\n",
      "blocks.36.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.36.cross_attn.q\n",
      "blocks.36.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.36.cross_attn.k.weight\n",
      "blocks.36.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.36.cross_attn.k\n",
      "blocks.36.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.36.cross_attn.v.weight\n",
      "blocks.36.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.36.cross_attn.v\n",
      "blocks.36.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.36.cross_attn.o.weight\n",
      "blocks.36.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.36.cross_attn.o\n",
      "blocks.36.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.36.ffn.0.weight\n",
      "blocks.36.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.36.ffn.0\n",
      "blocks.36.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.36.ffn.2.weight\n",
      "blocks.36.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.36.ffn.2\n",
      "blocks.36.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.37.self_attn.q.weight\n",
      "blocks.37.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.37.self_attn.q\n",
      "blocks.37.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.37.self_attn.k.weight\n",
      "blocks.37.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.37.self_attn.k\n",
      "blocks.37.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.37.self_attn.v.weight\n",
      "blocks.37.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.37.self_attn.v\n",
      "blocks.37.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.37.self_attn.o.weight\n",
      "blocks.37.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.37.self_attn.o\n",
      "blocks.37.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.37.cross_attn.q.weight\n",
      "blocks.37.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.37.cross_attn.q\n",
      "blocks.37.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.37.cross_attn.k.weight\n",
      "blocks.37.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.37.cross_attn.k\n",
      "blocks.37.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.37.cross_attn.v.weight\n",
      "blocks.37.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.37.cross_attn.v\n",
      "blocks.37.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.37.cross_attn.o.weight\n",
      "blocks.37.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.37.cross_attn.o\n",
      "blocks.37.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.37.ffn.0.weight\n",
      "blocks.37.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.37.ffn.0\n",
      "blocks.37.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.37.ffn.2.weight\n",
      "blocks.37.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.37.ffn.2\n",
      "blocks.37.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.38.self_attn.q.weight\n",
      "blocks.38.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.38.self_attn.q\n",
      "blocks.38.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.38.self_attn.k.weight\n",
      "blocks.38.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.38.self_attn.k\n",
      "blocks.38.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.38.self_attn.v.weight\n",
      "blocks.38.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.38.self_attn.v\n",
      "blocks.38.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.38.self_attn.o.weight\n",
      "blocks.38.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.38.self_attn.o\n",
      "blocks.38.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.38.cross_attn.q.weight\n",
      "blocks.38.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.38.cross_attn.q\n",
      "blocks.38.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.38.cross_attn.k.weight\n",
      "blocks.38.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.38.cross_attn.k\n",
      "blocks.38.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.38.cross_attn.v.weight\n",
      "blocks.38.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.38.cross_attn.v\n",
      "blocks.38.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.38.cross_attn.o.weight\n",
      "blocks.38.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.38.cross_attn.o\n",
      "blocks.38.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.38.ffn.0.weight\n",
      "blocks.38.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.38.ffn.0\n",
      "blocks.38.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.38.ffn.2.weight\n",
      "blocks.38.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.38.ffn.2\n",
      "blocks.38.ffn.2 lora 추가\n",
      "base_key가 존재:blocks.39.self_attn.q.weight\n",
      "blocks.39.self_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.39.self_attn.q\n",
      "blocks.39.self_attn.q lora 추가\n",
      "base_key가 존재:blocks.39.self_attn.k.weight\n",
      "blocks.39.self_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.39.self_attn.k\n",
      "blocks.39.self_attn.k lora 추가\n",
      "base_key가 존재:blocks.39.self_attn.v.weight\n",
      "blocks.39.self_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.39.self_attn.v\n",
      "blocks.39.self_attn.v lora 추가\n",
      "base_key가 존재:blocks.39.self_attn.o.weight\n",
      "blocks.39.self_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.39.self_attn.o\n",
      "blocks.39.self_attn.o lora 추가\n",
      "base_key가 존재:blocks.39.cross_attn.q.weight\n",
      "blocks.39.cross_attn.q.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.39.cross_attn.q\n",
      "blocks.39.cross_attn.q lora 추가\n",
      "base_key가 존재:blocks.39.cross_attn.k.weight\n",
      "blocks.39.cross_attn.k.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.39.cross_attn.k\n",
      "blocks.39.cross_attn.k lora 추가\n",
      "base_key가 존재:blocks.39.cross_attn.v.weight\n",
      "blocks.39.cross_attn.v.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.39.cross_attn.v\n",
      "blocks.39.cross_attn.v lora 추가\n",
      "base_key가 존재:blocks.39.cross_attn.o.weight\n",
      "blocks.39.cross_attn.o.weight\n",
      "torch.Size([5120, 5120])\n",
      "base:blocks.39.cross_attn.o\n",
      "blocks.39.cross_attn.o lora 추가\n",
      "base_key가 존재:blocks.39.ffn.0.weight\n",
      "blocks.39.ffn.0.weight\n",
      "torch.Size([13824, 5120])\n",
      "base:blocks.39.ffn.0\n",
      "blocks.39.ffn.0 lora 추가\n",
      "base_key가 존재:blocks.39.ffn.2.weight\n",
      "blocks.39.ffn.2.weight\n",
      "torch.Size([5120, 13824])\n",
      "base:blocks.39.ffn.2\n",
      "blocks.39.ffn.2 lora 추가\n",
      "model configured\n",
      "model configured\n"
     ]
    }
   ],
   "source": [
    "!pip install einops\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/workspace')\n",
    "from vae import Wan2_1_VAE\n",
    "from t5 import T5EncoderModel\n",
    "from solver import (\n",
    "    FlowDPMSolverMultistepScheduler,\n",
    "    get_sampling_sigmas,\n",
    "    retrieve_timesteps,\n",
    ")\n",
    "print('done')\n",
    "config = t2v_A14B\n",
    "checkpoint_dir = '/workspace/Wan2.2-T2V-A14B'\n",
    "# with torch.no_grad():\n",
    "with torch.autocast(\"cuda\", dtype=torch.float16):\n",
    "    wan_t2v = WanT2V(\n",
    "        config=t2v_A14B,\n",
    "        checkpoint_dir='/workspace/Wan2.2-T2V-A14B/',\n",
    "        device_id=0\n",
    "        # rank=rank,\n",
    "        # t5_fsdp=args.t5_fsdp,\n",
    "        # dit_fsdp=args.dit_fsdp,\n",
    "        # use_sp=(args.ulysses_size > 1),\n",
    "        # t5_cpu=args.t5_cpu,\n",
    "        # convert_model_dtype=args.convert_model_dtype,\n",
    "    )\n",
    "logging.info(f\"Generating video ...\")\n",
    "# def generate(self, input_prompt,\n",
    "#                  shift=5.0,\n",
    "#                  sample_solver='unipc',\n",
    "#                  sampling_steps=50,\n",
    "#                  guide_scale=5.0,\n",
    "#                  offload_model=True):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [27:58<00:00, 33.58s/it]\n",
      "/workspace/vae.py:658: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(dtype=self.dtype):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch.distributed as dist\n",
    "with torch.autocast(\"cuda\", dtype=torch.float16):\n",
    "    video = wan_t2v.generate(\n",
    "        input_prompt=context,\n",
    "        frame_num = 81,\n",
    "        size=(480,480), #576, 576 # 480, 832 # 640, 360\n",
    "        n_prompt = context_null,\n",
    "        seed = 7) #원래 -1\n",
    "print('done')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file\n",
    "\n",
    "# save_file({\"video\": video.cpu()}, \"video_asian_kajiwari.safetensors\")\n",
    "save_file({\"video\": video.cpu()}, \"video_goldenjinx.safetensors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "save_dict = {}\n",
    "\n",
    "for i in range(40):\n",
    "    block = wan_t2v.low_noise_model.blocks[i].self_attn\n",
    "\n",
    "    save_dict[f\"block_{i}_rope_q\"] = block.rope_q.cpu()\n",
    "    save_dict[f\"block_{i}_rope_k\"] = block.rope_k.cpu()\n",
    "\n",
    "torch.save(save_dict, \"santa_pure_rope.pt\")\n",
    "\n",
    "print(\"rope_q, rope_k 저장 완료 -> rope_tensors.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wan_t2v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m save_dict = {}\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m40\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     block = \u001b[43mwan_t2v\u001b[49m.low_noise_model.blocks[i].self_attn\n\u001b[32m      9\u001b[39m     save_dict[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mblock_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_rope_q\u001b[39m\u001b[33m\"\u001b[39m] = block.rope_q.cpu()\n\u001b[32m     10\u001b[39m     save_dict[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mblock_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_rope_k\u001b[39m\u001b[33m\"\u001b[39m] = block.rope_k.cpu()\n",
      "\u001b[31mNameError\u001b[39m: name 'wan_t2v' is not defined"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import save_file\n",
    "import torch\n",
    "\n",
    "save_dict = {}\n",
    "\n",
    "for i in range(40):\n",
    "    block = wan_t2v.low_noise_model.blocks[i].self_attn\n",
    "    \n",
    "    save_dict[f\"block_{i}_rope_q\"] = block.rope_q.cpu()\n",
    "    save_dict[f\"block_{i}_rope_k\"] = block.rope_k.cpu()\n",
    "\n",
    "# SafeTensors로 저장\n",
    "save_file(save_dict, \"goldenjinx_rope.safetensors\")\n",
    "\n",
    "print(\"rope_q, rope_k 저장 완료 -> goldenjinx_rope.safetensors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'video' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mvideo\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'video' is not defined"
     ]
    }
   ],
   "source": [
    "video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wan_t2v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mwan_t2v\u001b[49m.num_train_timesteps\n",
      "\u001b[31mNameError\u001b[39m: name 'wan_t2v' is not defined"
     ]
    }
   ],
   "source": [
    "wan_t2v.num_train_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-4.1100e+00,  4.3280e+00,  1.6262e+00,  ...,  3.4435e-02,\n",
       "           -7.1589e-02, -3.1678e-01],\n",
       "          [-1.7510e-01, -2.2578e+00,  2.4379e+00,  ...,  2.5613e-02,\n",
       "           -7.8997e-02, -1.2627e-01],\n",
       "          [ 2.8725e-02,  8.4995e-01, -6.8676e-01,  ..., -2.0485e-01,\n",
       "            3.3646e-01,  4.4429e-01],\n",
       "          ...,\n",
       "          [ 9.7561e-04,  5.3510e-03,  4.2057e-02,  ...,  5.1344e-02,\n",
       "           -2.2095e-02, -6.8312e-02],\n",
       "          [ 3.5170e+00, -4.6381e+00, -2.0324e+00,  ...,  2.6086e-01,\n",
       "            1.6318e-01, -6.9312e-02],\n",
       "          [ 5.0469e+00, -3.5098e+00, -2.2986e+00,  ...,  1.2680e-01,\n",
       "           -6.2988e-02,  6.0329e-02]],\n",
       "\n",
       "         [[-4.0584e+00,  4.2928e+00,  1.6022e+00,  ...,  4.3430e-02,\n",
       "           -7.7771e-02, -2.7492e-01],\n",
       "          [-3.5267e-01, -2.1768e+00,  2.4766e+00,  ...,  5.4800e-02,\n",
       "           -5.6608e-02, -1.4087e-01],\n",
       "          [-4.3259e-02,  9.5518e-01, -7.5140e-01,  ..., -1.9625e-01,\n",
       "            3.5945e-01,  4.9058e-01],\n",
       "          ...,\n",
       "          [ 1.5732e-02, -5.4996e-03, -5.1069e-03,  ...,  5.8256e-02,\n",
       "           -2.2204e-02, -6.5017e-02],\n",
       "          [ 3.5170e+00, -4.5272e+00, -2.1096e+00,  ...,  2.7471e-01,\n",
       "            1.6615e-01, -3.9689e-02],\n",
       "          [ 4.9473e+00, -3.5098e+00, -2.3478e+00,  ...,  1.5197e-01,\n",
       "           -4.2365e-02,  7.4414e-02]],\n",
       "\n",
       "         [[-4.1788e+00,  4.0993e+00,  1.7544e+00,  ..., -4.3660e-02,\n",
       "           -5.1697e-02, -3.3682e-01],\n",
       "          [-2.2566e-01, -2.0599e+00,  2.5540e+00,  ...,  6.6986e-02,\n",
       "           -6.8001e-02, -1.6201e-01],\n",
       "          [-3.2902e-03,  9.4304e-01, -7.3928e-01,  ..., -1.7986e-01,\n",
       "            3.5747e-01,  4.3032e-01],\n",
       "          ...,\n",
       "          [ 1.9878e-02,  1.1891e-03,  1.3458e-02,  ...,  6.5643e-02,\n",
       "           -1.0014e-02, -7.7737e-02],\n",
       "          [ 3.6598e+00, -4.5087e+00, -2.1867e+00,  ...,  2.7462e-01,\n",
       "            1.7209e-01, -5.1045e-02],\n",
       "          [ 4.9141e+00, -3.4767e+00, -2.3314e+00,  ...,  1.8882e-01,\n",
       "           -7.4295e-02,  4.1591e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.4711e+00,  4.3983e+00,  1.6022e+00,  ..., -2.3428e-01,\n",
       "           -1.2422e-02, -9.9729e-02],\n",
       "          [ 4.5131e-01, -2.2128e+00,  2.2734e+00,  ...,  3.2572e-02,\n",
       "           -2.6461e-02, -7.3559e-02],\n",
       "          [ 7.4615e-02,  6.7996e-01, -6.7464e-01,  ..., -1.7025e-01,\n",
       "            2.4704e-01,  2.3323e-01],\n",
       "          ...,\n",
       "          [ 3.2683e-02, -2.9371e-02,  9.0723e-03,  ...,  6.1162e-02,\n",
       "            2.7057e-02,  6.6501e-03],\n",
       "          [ 4.3025e+00, -5.2848e+00, -2.3497e+00,  ...,  2.0045e-01,\n",
       "            2.2276e-01, -5.7502e-02],\n",
       "          [ 5.0137e+00, -3.5760e+00, -2.2001e+00,  ...,  1.2408e-01,\n",
       "           -4.4159e-02, -8.7817e-02]],\n",
       "\n",
       "         [[-4.4711e+00,  4.3808e+00,  1.7544e+00,  ..., -6.1373e-02,\n",
       "            2.2876e-02, -1.4828e-01],\n",
       "          [ 5.8942e-01, -2.0958e+00,  2.2831e+00,  ...,  3.1160e-02,\n",
       "           -7.4472e-03, -1.8919e-01],\n",
       "          [ 4.5090e-02,  6.3139e-01, -6.1808e-01,  ..., -1.1354e-01,\n",
       "            2.3284e-01,  1.7491e-01],\n",
       "          ...,\n",
       "          [ 3.9512e-02, -4.0430e-02,  9.5530e-03,  ...,  5.1629e-02,\n",
       "            7.7437e-03, -3.2474e-02],\n",
       "          [ 4.4453e+00, -5.3957e+00, -2.6241e+00,  ...,  1.8255e-01,\n",
       "            1.9547e-01, -1.4789e-01],\n",
       "          [ 4.8809e+00, -3.5595e+00, -2.1672e+00,  ..., -2.5602e-02,\n",
       "           -3.7219e-02,  1.7893e-02]],\n",
       "\n",
       "         [[-4.6775e+00,  4.6095e+00,  1.6342e+00,  ..., -1.4928e-01,\n",
       "            1.2647e-01, -4.2443e-02],\n",
       "          [ 1.2824e-01, -2.3567e+00,  2.0606e+00,  ...,  1.0225e-02,\n",
       "            4.5719e-02, -2.1913e-01],\n",
       "          [ 1.4648e-01,  3.1570e-01, -3.5954e-01,  ..., -1.3531e-01,\n",
       "            2.9569e-01,  8.6770e-02],\n",
       "          ...,\n",
       "          [ 3.7561e-02, -1.1742e-03,  3.8302e-03,  ...,  4.7646e-02,\n",
       "            1.9531e-02, -1.0807e-02],\n",
       "          [ 4.2133e+00, -5.5435e+00, -2.3840e+00,  ...,  1.5501e-01,\n",
       "            1.4676e-01, -8.6866e-02],\n",
       "          [ 4.8145e+00, -3.5760e+00, -2.0687e+00,  ...,  3.4937e-02,\n",
       "           -3.8226e-02, -3.6626e-04]]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wan_t2v.low_noise_model.blocks[0].self_attn.rope_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WanAttentionBlock(\n",
       "  (norm1): WanLayerNorm((5120,), eps=1e-06, elementwise_affine=False)\n",
       "  (self_attn): WanSelfAttention(\n",
       "    (q): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "    (k): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "    (v): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "    (o): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "    (norm_q): WanRMSNorm()\n",
       "    (norm_k): WanRMSNorm()\n",
       "  )\n",
       "  (norm3): WanLayerNorm((5120,), eps=1e-06, elementwise_affine=True)\n",
       "  (cross_attn): WanCrossAttention(\n",
       "    (q): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "    (k): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "    (v): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "    (o): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "    (norm_q): WanRMSNorm()\n",
       "    (norm_k): WanRMSNorm()\n",
       "  )\n",
       "  (norm2): WanLayerNorm((5120,), eps=1e-06, elementwise_affine=False)\n",
       "  (ffn): Sequential(\n",
       "    (0): Linear(in_features=5120, out_features=13824, bias=True)\n",
       "    (1): GELU(approximate='tanh')\n",
       "    (2): Linear(in_features=13824, out_features=5120, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wan_t2v.low_noise_model.blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= load_file('/workspace/CassHamadaWan2.2LowNoise.safetensors')\n",
    "b=load_file('/workspace/5DFollowcam_Redmond_low_noise.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3g4IeN3X53L"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import binascii\n",
    "import logging\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "import imageio\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "def rand_name(length=8, suffix=''):\n",
    "    name = binascii.b2a_hex(os.urandom(length)).decode('utf-8')\n",
    "    if suffix:\n",
    "        if not suffix.startswith('.'):\n",
    "            suffix = '.' + suffix\n",
    "        name += suffix\n",
    "    return name\n",
    "\n",
    "\n",
    "def save_video(tensor,\n",
    "               save_file=None,\n",
    "               fps=30,\n",
    "               suffix='.mp4',\n",
    "               nrow=8,\n",
    "               normalize=True,\n",
    "               value_range=(-1, 1)):\n",
    "    # cache file\n",
    "    cache_file = osp.join('/tmp', rand_name(\n",
    "        suffix=suffix)) if save_file is None else save_file\n",
    "\n",
    "    # save to cache\n",
    "    try:\n",
    "        # preprocess\n",
    "        tensor = tensor.clamp(min(value_range), max(value_range))\n",
    "        tensor = torch.stack([\n",
    "            torchvision.utils.make_grid(\n",
    "                u, nrow=nrow, normalize=normalize, value_range=value_range)\n",
    "            for u in tensor.unbind(2)\n",
    "        ],\n",
    "                             dim=1).permute(1, 2, 3, 0)\n",
    "        tensor = (tensor * 255).type(torch.uint8).cpu()\n",
    "\n",
    "        # write video\n",
    "        writer = imageio.get_writer(\n",
    "            cache_file, fps=fps, codec='libx264', quality=8)\n",
    "        for frame in tensor.numpy():\n",
    "            writer.append_data(frame)\n",
    "        writer.close()\n",
    "    except Exception as e:\n",
    "        logging.info(f'save_video failed, error: {e}')\n",
    "\n",
    "\n",
    "# save_video(\n",
    "#     tensor=video[None],\n",
    "#     save_file=args.save_file,\n",
    "#     fps=cfg.sample_fps,\n",
    "#     nrow=1,\n",
    "#     normalize=True,\n",
    "#     value_range=(-1, 1))\n",
    "save_video(\n",
    "    tensor=video[None],\n",
    "    save_file=\"/content/b.mp4\",\n",
    "    fps=10,\n",
    "    nrow=1,\n",
    "    normalize=True,\n",
    "    value_range=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diffusion_model.blocks.0.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.0.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.0.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.0.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.0.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.0.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.0.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.0.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.0.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.0.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.0.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.0.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.0.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.0.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.0.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.0.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.0.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.0.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.0.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.0.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.0.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.0.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.0.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.0.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.0.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.0.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.0.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.0.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.0.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.0.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.0.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.0.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.0.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.0.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.0.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.0.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.0.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.0.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.0.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.0.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.1.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.1.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.1.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.1.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.1.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.1.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.1.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.1.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.1.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.1.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.1.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.1.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.1.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.1.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.1.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.1.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.1.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.1.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.1.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.1.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.1.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.1.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.1.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.1.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.1.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.1.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.1.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.1.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.1.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.1.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.1.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.1.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.1.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.1.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.1.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.1.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.1.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.1.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.1.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.1.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.10.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.10.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.10.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.10.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.10.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.10.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.10.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.10.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.10.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.10.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.10.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.10.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.10.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.10.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.10.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.10.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.10.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.10.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.10.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.10.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.10.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.10.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.10.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.10.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.10.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.10.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.10.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.10.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.10.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.10.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.10.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.10.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.10.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.10.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.10.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.10.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.10.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.10.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.10.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.10.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.11.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.11.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.11.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.11.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.11.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.11.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.11.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.11.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.11.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.11.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.11.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.11.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.11.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.11.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.11.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.11.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.11.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.11.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.11.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.11.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.11.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.11.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.11.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.11.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.11.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.11.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.11.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.11.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.11.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.11.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.11.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.11.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.11.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.11.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.11.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.11.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.11.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.11.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.11.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.11.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.12.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.12.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.12.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.12.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.12.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.12.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.12.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.12.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.12.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.12.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.12.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.12.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.12.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.12.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.12.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.12.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.12.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.12.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.12.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.12.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.12.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.12.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.12.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.12.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.12.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.12.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.12.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.12.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.12.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.12.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.12.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.12.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.12.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.12.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.12.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.12.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.12.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.12.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.12.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.12.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.13.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.13.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.13.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.13.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.13.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.13.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.13.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.13.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.13.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.13.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.13.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.13.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.13.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.13.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.13.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.13.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.13.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.13.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.13.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.13.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.13.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.13.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.13.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.13.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.13.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.13.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.13.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.13.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.13.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.13.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.13.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.13.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.13.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.13.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.13.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.13.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.13.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.13.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.13.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.13.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.14.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.14.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.14.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.14.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.14.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.14.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.14.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.14.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.14.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.14.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.14.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.14.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.14.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.14.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.14.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.14.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.14.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.14.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.14.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.14.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.14.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.14.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.14.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.14.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.14.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.14.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.14.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.14.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.14.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.14.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.14.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.14.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.14.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.14.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.14.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.14.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.14.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.14.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.14.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.14.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.15.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.15.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.15.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.15.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.15.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.15.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.15.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.15.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.15.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.15.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.15.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.15.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.15.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.15.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.15.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.15.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.15.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.15.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.15.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.15.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.15.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.15.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.15.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.15.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.15.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.15.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.15.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.15.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.15.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.15.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.15.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.15.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.15.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.15.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.15.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.15.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.15.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.15.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.15.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.15.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.16.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.16.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.16.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.16.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.16.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.16.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.16.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.16.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.16.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.16.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.16.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.16.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.16.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.16.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.16.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.16.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.16.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.16.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.16.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.16.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.16.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.16.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.16.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.16.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.16.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.16.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.16.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.16.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.16.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.16.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.16.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.16.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.16.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.16.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.16.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.16.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.16.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.16.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.16.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.16.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.17.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.17.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.17.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.17.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.17.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.17.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.17.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.17.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.17.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.17.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.17.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.17.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.17.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.17.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.17.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.17.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.17.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.17.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.17.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.17.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.17.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.17.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.17.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.17.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.17.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.17.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.17.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.17.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.17.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.17.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.17.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.17.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.17.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.17.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.17.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.17.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.17.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.17.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.17.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.17.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.18.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.18.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.18.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.18.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.18.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.18.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.18.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.18.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.18.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.18.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.18.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.18.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.18.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.18.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.18.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.18.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.18.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.18.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.18.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.18.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.18.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.18.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.18.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.18.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.18.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.18.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.18.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.18.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.18.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.18.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.18.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.18.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.18.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.18.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.18.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.18.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.18.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.18.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.18.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.18.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.19.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.19.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.19.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.19.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.19.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.19.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.19.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.19.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.19.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.19.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.19.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.19.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.19.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.19.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.19.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.19.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.19.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.19.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.19.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.19.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.19.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.19.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.19.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.19.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.19.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.19.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.19.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.19.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.19.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.19.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.19.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.19.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.19.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.19.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.19.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.19.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.19.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.19.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.19.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.19.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.2.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.2.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.2.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.2.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.2.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.2.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.2.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.2.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.2.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.2.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.2.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.2.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.2.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.2.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.2.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.2.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.2.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.2.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.2.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.2.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.2.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.2.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.2.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.2.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.2.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.2.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.2.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.2.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.2.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.2.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.2.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.2.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.2.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.2.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.2.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.2.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.2.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.2.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.2.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.2.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.20.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.20.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.20.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.20.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.20.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.20.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.20.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.20.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.20.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.20.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.20.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.20.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.20.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.20.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.20.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.20.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.20.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.20.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.20.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.20.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.20.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.20.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.20.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.20.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.20.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.20.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.20.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.20.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.20.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.20.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.20.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.20.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.20.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.20.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.20.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.20.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.20.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.20.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.20.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.20.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.21.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.21.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.21.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.21.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.21.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.21.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.21.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.21.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.21.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.21.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.21.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.21.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.21.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.21.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.21.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.21.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.21.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.21.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.21.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.21.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.21.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.21.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.21.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.21.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.21.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.21.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.21.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.21.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.21.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.21.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.21.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.21.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.21.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.21.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.21.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.21.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.21.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.21.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.21.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.21.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.22.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.22.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.22.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.22.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.22.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.22.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.22.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.22.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.22.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.22.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.22.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.22.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.22.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.22.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.22.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.22.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.22.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.22.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.22.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.22.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.22.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.22.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.22.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.22.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.22.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.22.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.22.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.22.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.22.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.22.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.22.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.22.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.22.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.22.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.22.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.22.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.22.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.22.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.22.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.22.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.23.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.23.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.23.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.23.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.23.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.23.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.23.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.23.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.23.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.23.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.23.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.23.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.23.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.23.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.23.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.23.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.23.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.23.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.23.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.23.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.23.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.23.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.23.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.23.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.23.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.23.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.23.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.23.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.23.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.23.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.23.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.23.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.23.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.23.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.23.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.23.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.23.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.23.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.23.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.23.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.24.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.24.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.24.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.24.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.24.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.24.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.24.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.24.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.24.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.24.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.24.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.24.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.24.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.24.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.24.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.24.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.24.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.24.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.24.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.24.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.24.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.24.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.24.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.24.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.24.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.24.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.24.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.24.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.24.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.24.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.24.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.24.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.24.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.24.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.24.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.24.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.24.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.24.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.24.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.24.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.25.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.25.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.25.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.25.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.25.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.25.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.25.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.25.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.25.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.25.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.25.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.25.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.25.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.25.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.25.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.25.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.25.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.25.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.25.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.25.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.25.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.25.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.25.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.25.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.25.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.25.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.25.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.25.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.25.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.25.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.25.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.25.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.25.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.25.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.25.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.25.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.25.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.25.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.25.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.25.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.26.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.26.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.26.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.26.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.26.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.26.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.26.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.26.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.26.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.26.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.26.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.26.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.26.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.26.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.26.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.26.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.26.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.26.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.26.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.26.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.26.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.26.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.26.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.26.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.26.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.26.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.26.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.26.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.26.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.26.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.26.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.26.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.26.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.26.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.26.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.26.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.26.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.26.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.26.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.26.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.27.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.27.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.27.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.27.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.27.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.27.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.27.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.27.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.27.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.27.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.27.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.27.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.27.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.27.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.27.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.27.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.27.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.27.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.27.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.27.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.27.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.27.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.27.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.27.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.27.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.27.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.27.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.27.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.27.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.27.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.27.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.27.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.27.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.27.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.27.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.27.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.27.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.27.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.27.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.27.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.28.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.28.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.28.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.28.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.28.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.28.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.28.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.28.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.28.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.28.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.28.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.28.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.28.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.28.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.28.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.28.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.28.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.28.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.28.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.28.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.28.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.28.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.28.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.28.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.28.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.28.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.28.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.28.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.28.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.28.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.28.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.28.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.28.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.28.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.28.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.28.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.28.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.28.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.28.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.28.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.29.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.29.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.29.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.29.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.29.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.29.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.29.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.29.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.29.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.29.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.29.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.29.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.29.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.29.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.29.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.29.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.29.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.29.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.29.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.29.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.29.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.29.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.29.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.29.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.29.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.29.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.29.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.29.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.29.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.29.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.29.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.29.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.29.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.29.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.29.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.29.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.29.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.29.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.29.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.29.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.3.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.3.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.3.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.3.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.3.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.3.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.3.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.3.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.3.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.3.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.3.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.3.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.3.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.3.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.3.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.3.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.3.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.3.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.3.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.3.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.3.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.3.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.3.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.3.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.3.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.3.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.3.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.3.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.3.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.3.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.3.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.3.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.3.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.3.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.3.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.3.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.3.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.3.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.3.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.3.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.30.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.30.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.30.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.30.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.30.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.30.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.30.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.30.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.30.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.30.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.30.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.30.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.30.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.30.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.30.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.30.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.30.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.30.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.30.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.30.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.30.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.30.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.30.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.30.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.30.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.30.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.30.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.30.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.30.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.30.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.30.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.30.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.30.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.30.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.30.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.30.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.30.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.30.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.30.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.30.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.31.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.31.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.31.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.31.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.31.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.31.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.31.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.31.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.31.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.31.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.31.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.31.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.31.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.31.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.31.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.31.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.31.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.31.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.31.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.31.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.31.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.31.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.31.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.31.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.31.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.31.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.31.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.31.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.31.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.31.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.31.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.31.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.31.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.31.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.31.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.31.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.31.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.31.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.31.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.31.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.32.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.32.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.32.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.32.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.32.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.32.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.32.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.32.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.32.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.32.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.32.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.32.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.32.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.32.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.32.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.32.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.32.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.32.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.32.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.32.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.32.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.32.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.32.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.32.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.32.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.32.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.32.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.32.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.32.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.32.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.32.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.32.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.32.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.32.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.32.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.32.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.32.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.32.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.32.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.32.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.33.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.33.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.33.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.33.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.33.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.33.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.33.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.33.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.33.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.33.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.33.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.33.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.33.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.33.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.33.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.33.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.33.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.33.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.33.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.33.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.33.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.33.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.33.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.33.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.33.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.33.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.33.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.33.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.33.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.33.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.33.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.33.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.33.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.33.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.33.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.33.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.33.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.33.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.33.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.33.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.34.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.34.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.34.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.34.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.34.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.34.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.34.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.34.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.34.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.34.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.34.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.34.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.34.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.34.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.34.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.34.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.34.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.34.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.34.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.34.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.34.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.34.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.34.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.34.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.34.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.34.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.34.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.34.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.34.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.34.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.34.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.34.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.34.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.34.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.34.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.34.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.34.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.34.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.34.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.34.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.35.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.35.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.35.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.35.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.35.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.35.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.35.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.35.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.35.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.35.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.35.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.35.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.35.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.35.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.35.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.35.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.35.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.35.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.35.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.35.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.35.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.35.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.35.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.35.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.35.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.35.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.35.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.35.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.35.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.35.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.35.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.35.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.35.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.35.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.35.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.35.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.35.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.35.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.35.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.35.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.36.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.36.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.36.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.36.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.36.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.36.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.36.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.36.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.36.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.36.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.36.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.36.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.36.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.36.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.36.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.36.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.36.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.36.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.36.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.36.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.36.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.36.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.36.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.36.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.36.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.36.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.36.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.36.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.36.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.36.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.36.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.36.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.36.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.36.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.36.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.36.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.36.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.36.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.36.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.36.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.37.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.37.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.37.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.37.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.37.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.37.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.37.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.37.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.37.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.37.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.37.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.37.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.37.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.37.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.37.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.37.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.37.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.37.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.37.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.37.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.37.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.37.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.37.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.37.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.37.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.37.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.37.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.37.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.37.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.37.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.37.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.37.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.37.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.37.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.37.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.37.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.37.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.37.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.37.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.37.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.38.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.38.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.38.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.38.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.38.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.38.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.38.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.38.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.38.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.38.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.38.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.38.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.38.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.38.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.38.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.38.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.38.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.38.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.38.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.38.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.38.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.38.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.38.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.38.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.38.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.38.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.38.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.38.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.38.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.38.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.38.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.38.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.38.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.38.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.38.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.38.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.38.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.38.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.38.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.38.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.39.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.39.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.39.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.39.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.39.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.39.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.39.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.39.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.39.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.39.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.39.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.39.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.39.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.39.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.39.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.39.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.39.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.39.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.39.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.39.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.39.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.39.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.39.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.39.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.39.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.39.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.39.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.39.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.39.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.39.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.39.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.39.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.39.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.39.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.39.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.39.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.39.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.39.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.39.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.39.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.4.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.4.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.4.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.4.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.4.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.4.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.4.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.4.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.4.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.4.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.4.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.4.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.4.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.4.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.4.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.4.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.4.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.4.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.4.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.4.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.4.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.4.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.4.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.4.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.4.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.4.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.4.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.4.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.4.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.4.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.4.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.4.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.4.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.4.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.4.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.4.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.4.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.4.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.4.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.4.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.5.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.5.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.5.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.5.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.5.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.5.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.5.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.5.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.5.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.5.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.5.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.5.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.5.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.5.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.5.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.5.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.5.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.5.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.5.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.5.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.5.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.5.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.5.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.5.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.5.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.5.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.5.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.5.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.5.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.5.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.5.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.5.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.5.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.5.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.5.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.5.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.5.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.5.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.5.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.5.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.6.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.6.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.6.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.6.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.6.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.6.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.6.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.6.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.6.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.6.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.6.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.6.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.6.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.6.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.6.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.6.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.6.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.6.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.6.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.6.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.6.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.6.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.6.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.6.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.6.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.6.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.6.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.6.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.6.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.6.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.6.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.6.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.6.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.6.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.6.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.6.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.6.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.6.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.6.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.6.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.7.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.7.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.7.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.7.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.7.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.7.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.7.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.7.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.7.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.7.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.7.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.7.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.7.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.7.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.7.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.7.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.7.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.7.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.7.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.7.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.7.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.7.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.7.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.7.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.7.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.7.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.7.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.7.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.7.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.7.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.7.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.7.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.7.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.7.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.7.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.7.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.7.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.7.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.7.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.7.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.8.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.8.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.8.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.8.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.8.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.8.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.8.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.8.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.8.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.8.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.8.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.8.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.8.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.8.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.8.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.8.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.8.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.8.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.8.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.8.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.8.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.8.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.8.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.8.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.8.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.8.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.8.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.8.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.8.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.8.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.8.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.8.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.8.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.8.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.8.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.8.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.8.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.8.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.8.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.8.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.9.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.9.cross_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.9.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.9.cross_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.9.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.9.cross_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.9.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.9.cross_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.9.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.9.cross_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.9.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.9.cross_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.9.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.9.cross_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.9.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.9.cross_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.9.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.9.ffn.0.lora_A.weight\n",
      "diffusion_model.blocks.9.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.9.ffn.0.lora_B.weight\n",
      "diffusion_model.blocks.9.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.9.ffn.2.lora_A.weight\n",
      "diffusion_model.blocks.9.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.9.ffn.2.lora_B.weight\n",
      "diffusion_model.blocks.9.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.9.self_attn.k.lora_A.weight\n",
      "diffusion_model.blocks.9.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.9.self_attn.k.lora_B.weight\n",
      "diffusion_model.blocks.9.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.9.self_attn.o.lora_A.weight\n",
      "diffusion_model.blocks.9.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.9.self_attn.o.lora_B.weight\n",
      "diffusion_model.blocks.9.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.9.self_attn.q.lora_A.weight\n",
      "diffusion_model.blocks.9.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.9.self_attn.q.lora_B.weight\n",
      "diffusion_model.blocks.9.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.9.self_attn.v.lora_A.weight\n",
      "diffusion_model.blocks.9.self_attn.v.lora_B.weight\n",
      "diffusion_model.blocks.9.self_attn.v.lora_B.weight\n"
     ]
    }
   ],
   "source": [
    "for i,v in zip(a.keys(),b.keys()):\n",
    "    print(i)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video lora\n",
    "# style content는 h, w\n",
    "# motion, camera는 frame num temporal\n",
    "def insert(path1,path2, model, k=1):\n",
    "\n",
    "  lora1 = load_file(path1)\n",
    "  lora1 = sort_state_dict_by_blocknum(lora1)\n",
    "  lora2 = load_file(path2)\n",
    "  lora2 = sort_state_dict_by_blocknum(lora2)\n",
    "\n",
    "  lora_pairs1 = {}\n",
    "  lora_pairs2 = {}\n",
    "  for key1,key2 in zip(lora1.keys(),lora2.keys()):\n",
    "\n",
    "      if 'lora_A.weight' in key1:\n",
    "        print(key1)\n",
    "        base_key = key1.replace('.lora_A.weight', '')\n",
    "        lora_b_key = base_key + '.lora_B.weight'\n",
    "        if lora_b_key in lora1:\n",
    "            lora_pairs1[base_key] = {\n",
    "                'A': lora1[key1],\n",
    "                'B': lora1[lora_b_key]\n",
    "            }\n",
    "      else:\n",
    "        print('lora_A 존재 x')\n",
    "      if 'lora_A.weight' in key2:\n",
    "          print(key2)\n",
    "          base_key = key2.replace('.lora_A.weight', '')\n",
    "          lora_b_key = base_key + '.lora_B.weight'\n",
    "          if lora_b_key in lora2:\n",
    "              lora_pairs2[base_key] = {\n",
    "                  'A': lora2[key2],\n",
    "                  'B': lora2[lora_b_key]\n",
    "              }\n",
    "      else:\n",
    "        print('lora_A 존재 x')\n",
    "  assert len(lora_pairs1.keys())==len(lora_pairs2.keys())\n",
    "  merge_lora = {}\n",
    "  for key1, key2 in zip(lora_pairs1.keys(),lora_pairs2.keys()):\n",
    "    if key1 == key2:\n",
    "      lora_pairs1[key1]['A'] = lora_pairs1[key1]['A'].detach().clone().float() if isinstance(lora_pairs1[key1]['A'], torch.Tensor) else torch.tensor(lora_pairs1[key1]['A'], dtype=torch.float32)\n",
    "      lora_pairs1[key1]['B'] = lora_pairs1[key1]['B'].detach().clone().float() if isinstance(lora_pairs1[key1]['B'], torch.Tensor) else torch.tensor(lora_pairs1[key1]['B'], dtype=torch.float32)\n",
    "      lora_pairs2[key2]['A'] = lora_pairs2[key2]['A'].detach().clone().float() if isinstance(lora_pairs2[key2]['A'], torch.Tensor) else torch.tensor(lora_pairs2[key2]['A'], dtype=torch.float32)\n",
    "      lora_pairs2[key2]['B'] = lora_pairs2[key2]['B'].detach().clone().float() if isinstance(lora_pairs2[key2]['B'], torch.Tensor) else torch.tensor(lora_pairs2[key2]['B'], dtype=torch.float32)\n",
    "      with torch.amp.autocast('cuda'):\n",
    "        matrix1 = lora_pairs1[key1]['B'] @ lora_pairs1[key1]['A']\n",
    "        matrix2 = lora_pairs2[key2]['B'] @ lora_pairs2[key2]['A']\n",
    "\n",
    "      #ffn도 통합시켜야됨\n",
    "      if 'ffn' in key1:\n",
    "        matrix1_norm = torch.norm(matrix1, p='fro')\n",
    "        matrix2_norm = torch.norm(matrix2, p='fro')\n",
    "\n",
    "        alpha = (matrix1_norm.item() + matrix2_norm.item()) / 2\n",
    "        matrix1_lora_normed = (alpha / matrix1_norm.item()) * matrix1\n",
    "        matrix2_lora_normed = (alpha / matrix2_norm.item()) * matrix2\n",
    "        matrix1_lora_normed = matrix1_lora_normed.to(torch.float32)\n",
    "        matrix2_lora_normed = matrix2_lora_normed.to(torch.float32)\n",
    "        ffn_merge = matrix1_lora_normed + matrix2_lora_normed\n",
    "        merge_lora[key1] = ffn_merge\n",
    "\n",
    "    else:\n",
    "      print('key1, key2가 같지 않다')\n",
    "\n",
    "    # attention 통합과정#------------------------------------\n",
    "    for i in range(40):\n",
    "      target = f\"blocks.{i}.self_attn\"\n",
    "      result1 = {k: v for k, v in lora_pairs1.items() if target in k}\n",
    "      result2 = {\n",
    "        \".\".join(k.split(\".\")[2:]): v   # 앞의 'blocks.0.' 부분 제거\n",
    "        for k, v in result2.items()}\n",
    "      result3 = {k: v for k, v in lora_pairs2.items() if target in k}\n",
    "      result4 = {\n",
    "        \".\".join(k.split(\".\")[2:]): v   # 앞의 'blocks.0.' 부분 제거\n",
    "        for k, v in result3.items()}\n",
    "\n",
    "      setattr(model.block[i], \"self_attn\", videoattn(model.block[i].self_attn, result2, result4))\n",
    "    # blocks.1.cross_attn.k.lora_A.weight\n",
    "    for i in range(40):\n",
    "      target = f\"blocks.{i}.cross_attn\"\n",
    "      result1 = {k: v for k, v in lora_pairs1.items() if target in k}\n",
    "      result2 = {\n",
    "        \".\".join(k.split(\".\")[2:]): v   # 앞의 'blocks.0.' 부분 제거\n",
    "        for k, v in result2.items()}\n",
    "      result3 = {k: v for k, v in lora_pairs2.items() if target in k}\n",
    "      result4 = {\n",
    "        \".\".join(k.split(\".\")[2:]): v   # 앞의 'blocks.0.' 부분 제거\n",
    "        for k, v in result3.items()}\n",
    "\n",
    "      setattr(model.block[i], \"cross_attn\", videocrossattn(model.block[i].cross_attn, result2, result4))\n",
    "    # blocks.1.cross_attn.k.lora_A.weight\n",
    "    # merge_lora.keys는 blocks.0.self_attn.k.lora_A.weight 이 형태\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "  for key, value in model.named_parameters():\n",
    "\n",
    "    if \"self_attn\" in key or \"cross_attn\" in key:\n",
    "        continue\n",
    "    else:\n",
    "      base = key.rsplit('.', 1)[0] # 마지막 토큰 제거\n",
    "      if key in [k + \".weight\" for k in lora_pairs1.keys()]:\n",
    "          print(f'base_key가 존재:{key}') # blocks.0.self_attn.q.bias도 걸러지네 이게 blocks.0.self_attn.q.weight\n",
    "          param = reduce(getattr, key.split('.'), model)\n",
    "\n",
    "          print(key) #blocks.0.self_attn.q\n",
    "          print(param.data.shape) #torch.Size([5120, 5120])\n",
    "          print(f'base:{base}')\n",
    "          with torch.amp.autocast('cuda'):\n",
    "            matrix1 = lora_pairs1[base]['B'] @ lora_pairs1[base]['A'] #'blocks.0.self_attn'\n",
    "            matrix2 = lora_pairs2[base]['B'] @ lora_pairs2[base]['A']\n",
    "            param.data += matrix1.to('cuda')\n",
    "            param.data += matrix2.to('cuda')\n",
    "            print(f'{base} lora 추가')\n",
    "  return model\n",
    "\n",
    "\n",
    "class WanSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 window_size=(-1, -1),\n",
    "                 qk_norm=True,\n",
    "                 eps=1e-6):\n",
    "        assert dim % num_heads == 0\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.window_size = window_size\n",
    "        self.qk_norm = qk_norm\n",
    "        self.eps = eps\n",
    "        self.q = nn.Linear(dim, dim)\n",
    "        self.k = nn.Linear(dim, dim)\n",
    "        self.v = nn.Linear(dim, dim)\n",
    "        self.o = nn.Linear(dim, dim)\n",
    "        self.norm_q = WanRMSNorm(dim, eps=eps) if qk_norm else nn.Identity()\n",
    "        self.norm_k = WanRMSNorm(dim, eps=eps) if qk_norm else nn.Identity()\n",
    "\n",
    "    def forward(self, x, seq_lens, grid_sizes, freqs):\n",
    "        #dim 2048 num_heads 16 head_dim 128\n",
    "        b, s, n, d = *x.shape[:2], self.num_heads, self.head_dim\n",
    "\n",
    "        # query, key, value function\n",
    "        def qkv_fn(x):\n",
    "            q = self.norm_q(self.q(x)).view(b, s, n, d)\n",
    "            k = self.norm_k(self.k(x)).view(b, s, n, d)\n",
    "            v = self.v(x).view(b, s, n, d)\n",
    "            return q, k, v\n",
    "\n",
    "        q, k, v = qkv_fn(x)\n",
    "    \n",
    "        x = flash_attention(\n",
    "            q=rope_apply(q, grid_sizes, freqs),\n",
    "            k=rope_apply(k, grid_sizes, freqs),\n",
    "            v=v,\n",
    "            k_lens=seq_lens,\n",
    "            window_size=self.window_size)\n",
    "\n",
    "        x = x.flatten(2)\n",
    "        x = self.o(x)\n",
    "        return x\n",
    "\n",
    "class videoattn(nn.Module):\n",
    "    def __init__(self, wan_self_attention: WanSelfAttention, lora1. lora2):\n",
    "\n",
    "        super().__init__()\n",
    "        self.wan_self_attention = wan_self_attention\n",
    "        self.lora1 = lora1\n",
    "        self.lora2 = lora2\n",
    "        self.wan_self_attention\n",
    "\n",
    "    def forward(self, x, seq_lens, grid_sizes, freqs, additional_arg=None):\n",
    "        \"\"\"\n",
    "            x (Tensor): Shape [B, L, num_heads, C / num_heads]\n",
    "            seq_lens (Tensor): Shape [B]\n",
    "            grid_sizes (Tensor): Shape [B, 3], (F, H, W)\n",
    "            freqs (Tensor): Shape [1024, C / num_heads / 2]\n",
    "            additional_arg (optional): 추가 인자 (예시로 일부 수정 가능)\n",
    "        \"\"\"\n",
    "        if self.attn: # 기존의 LoRA 비교를 통해 이미 우위가 정해진거 계산되있다\n",
    "\n",
    "          b, s, n, d = *x.shape[:2], self.wan_self_attention.num_heads, self.wan_self_attention.head_dim\n",
    "          def qkv_fn(x):\n",
    "              q = self.wan_self_attention.norm_q(self.wan_self_attention.q(x)).view(b, s, n, d)\n",
    "              k = self.wan_self_attention.norm_k(self.wan_self_attention.k(x)).view(b, s, n, d)\n",
    "              v = self.wan_self_attention.v(x).view(b, s, n, d)\n",
    "              return q, k, v\n",
    "          q, k, v = qkv_fn(x)\n",
    "          q=rope_apply(q, grid_sizes, freqs)\n",
    "          k=rope_apply(k, grid_sizes, freqs)\n",
    "          x = flash_attention( q=q, k=k, v=v, k_lens=seq_lens, window_size=self.wan_self_attention.window_size)\n",
    "          x = x.flatten(2)\n",
    "          x = self.wan_self_attention.o(x)\n",
    "          return x\n",
    "        else:\n",
    "          b, s, n, d = *x.shape[:2], self.wan_self_attention.num_heads, self.wan_self_attention.head_dim\n",
    "\n",
    "          lora_query1 = self.lora1['self_attn.q.lora_B'] @ self.lora1['self_attn.q.lora_A']\n",
    "          lora_query2 = self.lora2['self_attn.q.lora_B'] @ self.lora2['self_attn.q.lora_A']\n",
    "\n",
    "          q1 = self.wan_self_attention.norm_q(self.wan_self_attention.q(x) + lora_query1(x)).view(b, s, n, d)\n",
    "          q2 = self.wan_self_attention.norm_q(self.wan_self_attention.q(x) + lora_query2(x)).view(b, s, n, d)\n",
    "\n",
    "          lora_key1 = self.lora1['self_attn.k.lora_B'] @ self.lora1['self_attn.k.lora_A']\n",
    "          lora_key2 = self.lora2['self_attn.k.lora_B'] @ self.lora2['self_attn.k.lora_A']\n",
    "\n",
    "          k1 = self.wan_self_attention.norm_k(self.wan_self_attention.k(x))\n",
    "          k2 = self.wan_self_attention.norm_k(self.wan_self_attention.k(x))\n",
    "\n",
    "          v = self.wan_self_attention.v(x).view(b, s, n, d)\n",
    "          q=rope_apply(q, grid_sizes, freqs)\n",
    "          k=rope_apply(k, grid_sizes, freqs)\n",
    "\n",
    "          #inverse\n",
    "          q1 = reverse_flatten(q1.float())\n",
    "          q2 = reverse_flatten(q2.float())\n",
    "\n",
    "          dim_splits = (20, 22, 22)  # (Time, Height, Width)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "          start_idx = 0\n",
    "          axis_names = ['Time', 'Height', 'Width']\n",
    "\n",
    "          for axis_idx, size in enumerate(dim_splits):\n",
    "              end_idx = start_idx + size\n",
    "              axis_name = axis_names[axis_idx]\n",
    "\n",
    "              q1_part = q1[..., start_idx:end_idx]\n",
    "              q2_part = q2[..., start_idx:end_idx]\n",
    "\n",
    "              val_c = torch.abs(q1_part).mean().item()\n",
    "              val_m = torch.abs(q2_part).mean().item()\n",
    "\n",
    "            data_records.append({'Block': i, 'Model': 'Content', 'Axis': axis_name, 'Value': val_c})\n",
    "            data_records.append({'Block': i, 'Model': 'Move',    'Axis': axis_name, 'Value': val_m})\n",
    "\n",
    "            start_idx = end_idx\n",
    "\n",
    "          # RoPE 적용후의 임베딩 2개 비교해서 self.wan_self_attention의 q, k로 넣어주고 self.attn True해줘\n",
    "          x = flash_attention(\n",
    "              q=q, k=k, v=v,\n",
    "              k_lens=seq_lens,\n",
    "              window_size=self.wan_self_attention.window_size)\n",
    "\n",
    "          x = x.flatten(2)\n",
    "          x = self.wan_self_attention.o(x)\n",
    "          return x\n",
    "def rope_apply(x, grid_sizes, freqs):\n",
    "    n, c = x.size(2), x.size(3) // 2\n",
    "\n",
    "    freqs = freqs.split([c - 2 * (c // 3), c // 3, c // 3], dim=1)\n",
    "\n",
    "    output = []\n",
    "    for i, (f, h, w) in enumerate(grid_sizes.tolist()):\n",
    "        seq_len = f * h * w\n",
    "        x_i = torch.view_as_complex(x[i, :seq_len].to(torch.float64).reshape(\n",
    "            seq_len, n, -1, 2))\n",
    "        freqs_i = torch.cat([\n",
    "            freqs[0][:f].view(f, 1, 1, -1).expand(f, h, w, -1),\n",
    "            freqs[1][:h].view(1, h, 1, -1).expand(f, h, w, -1),\n",
    "            freqs[2][:w].view(1, 1, w, -1).expand(f, h, w, -1)\n",
    "        ],\n",
    "                            dim=-1).reshape(seq_len, 1, -1)\n",
    "        x_i = torch.view_as_real(x_i * freqs_i).flatten(2)\n",
    "        x_i = torch.cat([x_i, x[i, seq_len:]])\n",
    "\n",
    "        output.append(x_i)\n",
    "    return torch.stack(output).float()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
